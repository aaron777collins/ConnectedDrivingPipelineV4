var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#connecteddrivingpipelinev4","title":"ConnectedDrivingPipelineV4","text":"<p>This pipeline was created to end the continuous cycle of making maching learning (ML) pipelines.</p> <p>This version of the pipeline was created for connected driving research. The goal is to create a better, more realistic dataset for training malicious bsm detection systems in the connected driving space. The current datasets, such as Veremi and Veremi Extension are too easy to train detection models for and don't model the world realistically.</p> <p>We took the Wyoming CV Pilot Basic Safety Message One Day Sample dataset from OpenDataNetwork as our original, real data.</p> <p>This pipeline creates malicious datasets based on customizable attacks and then tests them using machine learning models such as Random Forest, Decision Tree and K-Nearest-Neighbours.</p> <p>Head to Setup to get started.</p>"},{"location":"index.html#links","title":"Links","text":"<ul> <li>Setup to get started.</li> <li>Development for developing a pipeline user file.</li> <li>Repo for the project repository.</li> <li>Other Projects that I have created.</li> </ul>"},{"location":"API_Reference.html","title":"API Reference - Dask Components","text":"<p>Version: 1.0.0 Last Updated: 2026-01-18 Framework: ConnectedDrivingPipelineV4 (Dask Migration)</p>"},{"location":"API_Reference.html#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Core Infrastructure</li> <li>DaskSessionManager</li> <li>DaskParquetCache</li> <li>Pipeline Components</li> <li>DaskPipelineRunner</li> <li>DaskMClassifierPipeline</li> <li>Data Layer</li> <li>DaskDataGatherer</li> <li>DaskConnectedDrivingCleaner</li> <li>DaskCleanWithTimestamps</li> <li>DaskConnectedDrivingLargeDataCleaner</li> <li>Filtering Components</li> <li>DaskCleanerWithPassthroughFilter</li> <li>DaskCleanerWithFilterWithinRange</li> <li>DaskCleanerWithFilterWithinRangeXY</li> <li>DaskCleanerWithFilterWithinRangeXYAndDay</li> <li>DaskCleanerWithFilterWithinRangeXYAndDateRange</li> <li>ML Components</li> <li>DaskMConnectedDrivingDataCleaner</li> <li>DaskConnectedDrivingAttacker</li> <li>Utilities</li> <li>DaskUDFRegistry</li> <li>Testing</li> <li>DaskFixtures</li> </ol>"},{"location":"API_Reference.html#overview","title":"Overview","text":"<p>The Dask migration provides distributed, memory-efficient alternatives to the original pandas/PySpark components. All Dask components follow consistent patterns:</p> <ul> <li>Lazy evaluation: Operations build computation graphs, execute with <code>.compute()</code></li> <li>Parquet caching: Using <code>@DaskParquetCache</code> decorator for efficient persistence</li> <li>Memory safety: Optimized for 64GB systems with 15M+ row datasets</li> <li>Interface compatibility: Same method signatures as pandas versions where possible</li> </ul> <p>System Requirements: - 64GB RAM (recommended) - 6-12 CPU cores - 500GB+ SSD storage</p> <p>Dependencies: <pre><code>dask[complete]&gt;=2024.1.0\ndask-ml&gt;=2024.4.0\ndistributed&gt;=2024.1.0\npyarrow&gt;=14.0.0\n</code></pre></p> <p>For installation instructions, see the main README.md.</p> <p></p>"},{"location":"API_Reference.html#core-infrastructure","title":"Core Infrastructure","text":""},{"location":"API_Reference.html#dasksessionmanager","title":"DaskSessionManager","text":"<p>Module: <code>Helpers/DaskSessionManager.py</code></p> <p>Centralized singleton for managing Dask distributed cluster and client lifecycle.</p>"},{"location":"API_Reference.html#features","title":"Features","text":"<ul> <li>Singleton pattern - One cluster per application</li> <li>Auto-configuration - Loads settings from YAML config files</li> <li>Memory-safe defaults - Optimized for 64GB systems (6 workers \u00d7 8GB)</li> <li>Dashboard integration - Built-in monitoring interface</li> <li>Graceful shutdown - Proper resource cleanup</li> </ul>"},{"location":"API_Reference.html#usage-example","title":"Usage Example","text":"<pre><code>from Helpers.DaskSessionManager import DaskSessionManager\n\n# Get or create Dask client (singleton)\nclient = DaskSessionManager.get_client()\n\n# Access dashboard for monitoring\nprint(f\"Dashboard: {client.dashboard_link}\")\n\n# Check worker info\nworkers = DaskSessionManager.get_worker_info()\nprint(f\"Active workers: {len(workers)}\")\n\n# Check memory usage\nmemory = DaskSessionManager.get_memory_usage()\nprint(f\"Total memory: {memory['total_memory_gb']:.2f} GB\")\nprint(f\"Used memory: {memory['used_memory_gb']:.2f} GB\")\n\n# Restart cluster if needed\nDaskSessionManager.restart()\n\n# Shutdown at end of application\nDaskSessionManager.shutdown()\n</code></pre>"},{"location":"API_Reference.html#api-reference","title":"API Reference","text":""},{"location":"API_Reference.html#get_cluster-localcluster","title":"<code>get_cluster() -&gt; LocalCluster</code>","text":"<p>Creates or retrieves the singleton Dask LocalCluster.</p> <p>Returns: - <code>LocalCluster</code>: Configured Dask cluster instance</p> <p>Configuration: - Default: 6 workers, 8GB per worker (48GB total) - Configurable via <code>config/dask_config.yaml</code></p> <p>Example: <pre><code>cluster = DaskSessionManager.get_cluster()\nprint(f\"Scheduler address: {cluster.scheduler_address}\")\n</code></pre></p>"},{"location":"API_Reference.html#get_client-client","title":"<code>get_client() -&gt; Client</code>","text":"<p>Gets or creates the Dask Client connected to the cluster.</p> <p>Returns: - <code>Client</code>: Dask distributed client</p> <p>Notes: - Automatically creates cluster if not exists - Reuses existing client (singleton) - Safe to call multiple times</p> <p>Example: <pre><code>client = DaskSessionManager.get_client()\nfutures = client.map(lambda x: x**2, range(10))\nresults = client.gather(futures)\n</code></pre></p>"},{"location":"API_Reference.html#shutdown-none","title":"<code>shutdown() -&gt; None</code>","text":"<p>Shuts down the Dask cluster and client, releasing all resources.</p> <p>Side Effects: - Closes all active workers - Terminates scheduler - Clears singleton state</p> <p>Example: <pre><code># At end of application\nDaskSessionManager.shutdown()\n</code></pre></p>"},{"location":"API_Reference.html#restart-none","title":"<code>restart() -&gt; None</code>","text":"<p>Restarts the Dask cluster (shutdown + get_client).</p> <p>Use Cases: - Recovering from worker failures - Clearing memory leaks - Applying new configuration</p> <p>Example: <pre><code># After heavy computation\nDaskSessionManager.restart()\n</code></pre></p>"},{"location":"API_Reference.html#get_dashboard_link-str","title":"<code>get_dashboard_link() -&gt; str</code>","text":"<p>Returns the URL for the Dask diagnostic dashboard.</p> <p>Returns: - <code>str</code>: Dashboard URL (e.g., <code>http://127.0.0.1:8787/status</code>)</p> <p>Example: <pre><code>url = DaskSessionManager.get_dashboard_link()\nprint(f\"Open dashboard: {url}\")\n</code></pre></p>"},{"location":"API_Reference.html#get_worker_info-dict","title":"<code>get_worker_info() -&gt; dict</code>","text":"<p>Returns information about active workers.</p> <p>Returns: - <code>dict</code>: Worker information with keys:   - <code>num_workers</code> (int): Number of active workers   - <code>total_cores</code> (int): Total CPU cores   - <code>total_memory_gb</code> (float): Total worker memory in GB   - <code>workers</code> (list): List of worker details</p> <p>Example: <pre><code>info = DaskSessionManager.get_worker_info()\nprint(f\"Workers: {info['num_workers']}\")\nprint(f\"Cores: {info['total_cores']}\")\nprint(f\"Memory: {info['total_memory_gb']} GB\")\n</code></pre></p>"},{"location":"API_Reference.html#get_memory_usage-dict","title":"<code>get_memory_usage() -&gt; dict</code>","text":"<p>Returns current memory usage statistics.</p> <p>Returns: - <code>dict</code>: Memory stats with keys:   - <code>total_memory_gb</code> (float): Total available memory   - <code>used_memory_gb</code> (float): Currently used memory   - <code>available_memory_gb</code> (float): Available memory   - <code>percent_used</code> (float): Percentage used (0-100)</p> <p>Example: <pre><code>mem = DaskSessionManager.get_memory_usage()\nif mem['percent_used'] &gt; 80:\n    print(\"Warning: High memory usage!\")\n    DaskSessionManager.restart()\n</code></pre></p> <p></p>"},{"location":"API_Reference.html#daskparquetcache","title":"DaskParquetCache","text":"<p>Module: <code>Decorators/DaskParquetCache.py</code></p> <p>Decorator for caching Dask DataFrame function results as Parquet files.</p>"},{"location":"API_Reference.html#features_1","title":"Features","text":"<ul> <li>MD5 hash-based cache keys - Automatic cache invalidation</li> <li>Parquet columnar storage - Efficient compression (~70% size reduction)</li> <li>Lazy evaluation compatible - Handles Dask DataFrames correctly</li> <li>Selective caching - Choose which arguments affect cache key</li> <li>Custom cache paths - Override default location</li> </ul>"},{"location":"API_Reference.html#usage-example_1","title":"Usage Example","text":"<pre><code>from Decorators.DaskParquetCache import DaskParquetCache\nimport dask.dataframe as dd\n\n@DaskParquetCache\ndef process_large_dataset(self, file_path: str, year: int) -&gt; dd.DataFrame:\n    \"\"\"Process BSM data for specific year.\"\"\"\n    df = dd.read_csv(file_path, blocksize='128MB')\n    df = df[df['year'] == year]\n    # ... expensive processing ...\n    return df\n\n# First call: computes and caches\nresult = obj.process_large_dataset(\"data.csv\", 2023)\n\n# Second call: loads from cache (fast!)\nresult = obj.process_large_dataset(\"data.csv\", 2023)\n</code></pre>"},{"location":"API_Reference.html#advanced-usage","title":"Advanced Usage","text":"<pre><code># Cache only on specific variables\n@DaskParquetCache(cache_variables=['year', 'region'])\ndef filter_data(self, file_path, year, region, debug=False) -&gt; dd.DataFrame:\n    # 'debug' flag won't affect cache key\n    df = dd.read_csv(file_path)\n    return df[(df['year'] == year) &amp; (df['region'] == region)]\n\n# Override cache location\n@DaskParquetCache(full_file_cache_path='/custom/path/cache')\ndef custom_location(self, data_file) -&gt; dd.DataFrame:\n    return dd.read_csv(data_file)\n</code></pre>"},{"location":"API_Reference.html#api-reference_1","title":"API Reference","text":""},{"location":"API_Reference.html#decorator-parameters","title":"Decorator Parameters","text":"<p><code>cache_variables</code> (optional) - Type: <code>list[str]</code> - Default: All function arguments (excluding <code>self</code>, <code>**kwargs</code>) - Description: Specify which arguments affect the cache key</p> <p><code>full_file_cache_path</code> (optional) - Type: <code>str</code> - Default: Auto-generated from function name + args hash - Description: Override cache file path (DO NOT include <code>.parquet</code> extension)</p>"},{"location":"API_Reference.html#requirements","title":"Requirements","text":"<ol> <li> <p>Type annotation required: <pre><code># \u2705 Correct\n@DaskParquetCache\ndef process(self, data) -&gt; dd.DataFrame:\n    return dd.read_csv(data)\n\n# \u274c Wrong (missing return type)\n@DaskParquetCache\ndef process(self, data):\n    return dd.read_csv(data)\n</code></pre></p> </li> <li> <p>Return type must be <code>dask.dataframe.DataFrame</code></p> </li> <li> <p>Cache directory automatically created</p> </li> </ol>"},{"location":"API_Reference.html#cache-location","title":"Cache Location","text":"<p>Default cache directory: <code>.cache_files/parquet/</code></p> <p>Cache filename format: <code>{function_name}_{md5_hash}.parquet/</code></p> <p>Example cache path: <pre><code>.cache_files/parquet/process_large_dataset_a3f2e1b9d4c5.parquet/\n</code></pre></p>"},{"location":"API_Reference.html#cache-invalidation","title":"Cache Invalidation","text":"<p>Cache is invalidated when: - Function arguments change (based on <code>cache_variables</code>) - Function source code changes - Cached file is manually deleted</p> <p></p>"},{"location":"API_Reference.html#pipeline-components","title":"Pipeline Components","text":""},{"location":"API_Reference.html#daskpipelinerunner","title":"DaskPipelineRunner","text":"<p>Module: <code>MachineLearning/DaskPipelineRunner.py</code></p> <p>Unified, config-driven ML pipeline executor that replaces 55+ individual pipeline scripts.</p>"},{"location":"API_Reference.html#features_2","title":"Features","text":"<ul> <li>JSON configuration - Complete pipeline defined in config file</li> <li>7 attack types - Rand offset, const offset, position swap, etc.</li> <li>4 feature sets - From minimal (6 cols) to full (30+ cols)</li> <li>3 filtering modes - No filter, XY offset, bounding box</li> <li>Reproducible - Random seed support</li> <li>Parquet caching - Intermediate results cached</li> </ul>"},{"location":"API_Reference.html#usage-example_2","title":"Usage Example","text":"<pre><code>from MachineLearning.DaskPipelineRunner import DaskPipelineRunner\n\n# From JSON config file\nrunner = DaskPipelineRunner.from_config(\"configs/pipeline_2000m_rand_offset.json\")\nresults = runner.run()\n\nprint(f\"Train size: {results['train_size']}\")\nprint(f\"Test size: {results['test_size']}\")\nprint(f\"Accuracy: {results['accuracy']:.4f}\")\nprint(f\"Precision: {results['precision']:.4f}\")\nprint(f\"Recall: {results['recall']:.4f}\")\n</code></pre>"},{"location":"API_Reference.html#configuration-example","title":"Configuration Example","text":"<pre><code>{\n  \"pipeline_name\": \"2000m_rand_offset_30pct\",\n  \"data\": {\n    \"source_file\": \"data/BSM_2023.csv\",\n    \"filtering_type\": \"xy_offset_position\",\n    \"filtering_xy_offset_position_distance\": 2000,\n    \"partitions\": 60,\n    \"date_ranges\": [[\"2023-01-01\", \"2023-12-31\"]]\n  },\n  \"features\": {\n    \"column_set\": \"minimal_xy_elev\"\n  },\n  \"attacks\": {\n    \"attack_type\": \"rand_offset\",\n    \"attacker_percentage\": 0.30,\n    \"attack_min_distance\": 10,\n    \"attack_max_distance\": 20,\n    \"random_seed\": 42\n  },\n  \"ml\": {\n    \"split_type\": \"fixed_size\",\n    \"test_size\": 100000,\n    \"random_seed\": 42\n  },\n  \"cache\": {\n    \"enabled\": true\n  }\n}\n</code></pre> <p>For complete configuration reference, see DaskPipelineRunner_Configuration_Guide.md.</p>"},{"location":"API_Reference.html#api-reference_2","title":"API Reference","text":""},{"location":"API_Reference.html#__init__config-dict","title":"<code>__init__(config: dict)</code>","text":"<p>Initializes pipeline from configuration dictionary.</p> <p>Parameters: - <code>config</code> (dict): Pipeline configuration</p> <p>Raises: - <code>ValueError</code>: If config is invalid or missing required fields</p> <p>Example: <pre><code>config = {\n    \"pipeline_name\": \"baseline\",\n    \"data\": {\"source_file\": \"data.csv\", ...},\n    ...\n}\nrunner = DaskPipelineRunner(config)\n</code></pre></p>"},{"location":"API_Reference.html#from_configconfig_path-str-daskpipelinerunner","title":"<code>from_config(config_path: str) -&gt; DaskPipelineRunner</code>","text":"<p>Class method to create pipeline from JSON file.</p> <p>Parameters: - <code>config_path</code> (str): Path to JSON configuration file</p> <p>Returns: - <code>DaskPipelineRunner</code>: Configured pipeline instance</p> <p>Raises: - <code>FileNotFoundError</code>: If config file doesn't exist - <code>json.JSONDecodeError</code>: If config file is invalid JSON - <code>ValueError</code>: If config validation fails</p> <p>Example: <pre><code>runner = DaskPipelineRunner.from_config(\"configs/experiment_001.json\")\n</code></pre></p>"},{"location":"API_Reference.html#run-dict","title":"<code>run() -&gt; dict</code>","text":"<p>Executes the complete ML pipeline.</p> <p>Returns: - <code>dict</code>: Results dictionary with keys:   - <code>pipeline_name</code> (str): Pipeline identifier   - <code>train_size</code> (int): Training set size   - <code>test_size</code> (int): Test set size   - <code>attacker_count</code> (int): Number of attackers in dataset   - <code>accuracy</code> (float): Model accuracy (0-1)   - <code>precision</code> (float): Model precision (0-1)   - <code>recall</code> (float): Model recall (0-1)   - <code>f1_score</code> (float): F1 score (0-1)   - <code>confusion_matrix</code> (array): 2x2 confusion matrix   - <code>execution_time_seconds</code> (float): Total runtime   - <code>cache_hit_rate</code> (float): Parquet cache hit rate (0-1)</p> <p>Pipeline Steps: 1. Data gathering (CSV \u2192 Dask DataFrame) 2. Large data cleaning (spatial/temporal filtering) 3. Train/test split 4. Attack simulation (add attackers + position attacks) 5. ML feature preparation (hex conversion, column selection) 6. Classifier training (RandomForest) 7. Results collection</p> <p>Example: <pre><code>results = runner.run()\n\nprint(f\"Pipeline: {results['pipeline_name']}\")\nprint(f\"Accuracy: {results['accuracy']:.4f}\")\nprint(f\"Runtime: {results['execution_time_seconds']:.1f}s\")\nprint(f\"Cache hits: {results['cache_hit_rate']:.1%}\")\n</code></pre></p>"},{"location":"API_Reference.html#attack-types","title":"Attack Types","text":"<p>The pipeline supports 7 attack simulation types:</p> Attack Type Description Parameters <code>rand_offset</code> Random position offset per message <code>min_distance</code>, <code>max_distance</code> <code>const_offset_per_id</code> Fixed offset per vehicle ID <code>min_distance</code>, <code>max_distance</code> <code>rand_position</code> Completely random positions <code>min_distance</code>, <code>max_distance</code> <code>position_swap</code> Swap positions between vehicle pairs <code>swap_distance</code> (optional) <code>const_offset</code> Fixed offset for all attackers <code>offset_x</code>, <code>offset_y</code> <code>override_const</code> All attackers report same position <code>target_x</code>, <code>target_y</code> <code>override_rand</code> Each message reports random position <code>min_x</code>, <code>max_x</code>, <code>min_y</code>, <code>max_y</code> <p>See Configuration Guide for detailed parameter descriptions.</p>"},{"location":"API_Reference.html#feature-column-sets","title":"Feature Column Sets","text":"<p>Four predefined column sets for different analysis needs:</p> Column Set Columns Description <code>minimal_xy_elev</code> 6 Core positional: <code>latitude</code>, <code>longitude</code>, <code>elevation</code>, <code>x</code>, <code>y</code>, <code>TxRxTime</code> <code>standard</code> 15 Adds speed, heading, accuracy metrics <code>full_standard</code> 25+ Standard + acceleration, brake status, vehicle metadata <code>all_available</code> 30+ All BSM fields <p></p>"},{"location":"API_Reference.html#daskmclassifierpipeline","title":"DaskMClassifierPipeline","text":"<p>Module: <code>MachineLearning/DaskMClassifierPipeline.py</code></p> <p>ML classifier training pipeline using Dask DataFrames with sklearn integration.</p>"},{"location":"API_Reference.html#features_3","title":"Features","text":"<ul> <li>Lazy evaluation - Builds computation graph, executes on <code>.compute()</code></li> <li>sklearn compatibility - Auto-converts to pandas before training</li> <li>Multiple classifiers - RandomForest, DecisionTree, KNeighbors</li> <li>Attack labeling - Binary classification (normal vs attacker)</li> </ul>"},{"location":"API_Reference.html#usage-example_3","title":"Usage Example","text":"<pre><code>from MachineLearning.DaskMClassifierPipeline import DaskMClassifierPipeline\nimport dask.dataframe as dd\n\n# Load cleaned, attacked data\ndata = dd.read_parquet(\"cleaned_data.parquet\")\n\n# Train classifier\npipeline = DaskMClassifierPipeline()\nresults = pipeline.run_classifier(\n    data=data,\n    classifier_type='RandomForest',\n    test_size=0.2,\n    random_state=42\n)\n\nprint(f\"Accuracy: {results['accuracy']:.4f}\")\nprint(f\"F1 Score: {results['f1_score']:.4f}\")\n</code></pre>"},{"location":"API_Reference.html#api-reference_3","title":"API Reference","text":""},{"location":"API_Reference.html#run_classifierdata-classifier_type-test_size-random_state-dict","title":"<code>run_classifier(data, classifier_type, test_size, random_state) -&gt; dict</code>","text":"<p>Trains ML classifier and returns performance metrics.</p> <p>Parameters: - <code>data</code> (dd.DataFrame): Dask DataFrame with features and <code>isAttacker</code> label - <code>classifier_type</code> (str): One of <code>'RandomForest'</code>, <code>'DecisionTree'</code>, <code>'KNeighbors'</code> - <code>test_size</code> (float or int): Test set size (0-1 for fraction, &gt;1 for count) - <code>random_state</code> (int): Random seed for reproducibility</p> <p>Returns: - <code>dict</code>: Results with keys:   - <code>accuracy</code> (float)   - <code>precision</code> (float)   - <code>recall</code> (float)   - <code>f1_score</code> (float)   - <code>confusion_matrix</code> (array)   - <code>train_size</code> (int)   - <code>test_size</code> (int)</p> <p>Notes: - Automatically calls <code>.compute()</code> before sklearn training - Drops non-numeric columns automatically - Requires <code>isAttacker</code> column for labels</p> <p></p>"},{"location":"API_Reference.html#data-layer","title":"Data Layer","text":""},{"location":"API_Reference.html#daskdatagatherer","title":"DaskDataGatherer","text":"<p>Module: <code>Gatherer/DaskDataGatherer.py</code></p> <p>Dask-based data loading with Parquet caching.</p>"},{"location":"API_Reference.html#usage-example_4","title":"Usage Example","text":"<pre><code>from Gatherer.DaskDataGatherer import DaskDataGatherer\n\ngatherer = DaskDataGatherer()\ndata = gatherer.gather_data()  # Returns dd.DataFrame\n\n# Lazy evaluation - no computation yet\nfiltered = data[data['speed'] &gt; 0]\n\n# Trigger computation\nresult = filtered.compute()  # Returns pandas DataFrame\n</code></pre>"},{"location":"API_Reference.html#api-reference_4","title":"API Reference","text":""},{"location":"API_Reference.html#gather_data-dddataframe","title":"<code>gather_data() -&gt; dd.DataFrame</code>","text":"<p>Gathers BSM data from source file with automatic Parquet caching.</p> <p>Returns: - <code>dd.DataFrame</code>: Lazy Dask DataFrame</p> <p>Caching: - First call: Reads CSV, caches as Parquet - Subsequent calls: Loads from Parquet (10x faster)</p> <p>Configuration: - Source file: Set via dependency injection - Blocksize: 128MB default - Partitions: Auto-calculated</p> <p></p>"},{"location":"API_Reference.html#daskconnecteddrivingcleaner","title":"DaskConnectedDrivingCleaner","text":"<p>Module: <code>Cleaners/DaskConnectedDrivingCleaner.py</code></p> <p>Core data cleaner for BSM datasets.</p>"},{"location":"API_Reference.html#features_4","title":"Features","text":"<ul> <li>Drops invalid records (speed &lt; 0, missing coordinates)</li> <li>Converts hex IDs to integers</li> <li>Standardizes column types</li> <li>Handles timestamp parsing</li> </ul>"},{"location":"API_Reference.html#usage-example_5","title":"Usage Example","text":"<pre><code>from Cleaners.DaskConnectedDrivingCleaner import DaskConnectedDrivingCleaner\nimport dask.dataframe as dd\n\ndata = dd.read_csv(\"raw_data.csv\")\ncleaner = DaskConnectedDrivingCleaner()\ncleaned = cleaner.clean(data)\n\n# Count removed records\noriginal_count = data.shape[0].compute()\ncleaned_count = cleaned.shape[0].compute()\nprint(f\"Removed {original_count - cleaned_count} invalid records\")\n</code></pre>"},{"location":"API_Reference.html#api-reference_5","title":"API Reference","text":""},{"location":"API_Reference.html#cleandata-dddataframe-dddataframe","title":"<code>clean(data: dd.DataFrame) -&gt; dd.DataFrame</code>","text":"<p>Cleans raw BSM data.</p> <p>Parameters: - <code>data</code> (dd.DataFrame): Raw Dask DataFrame</p> <p>Returns: - <code>dd.DataFrame</code>: Cleaned DataFrame</p> <p>Cleaning Operations: 1. Drop records with <code>speed &lt; 0</code> 2. Drop records with missing <code>latitude</code>/<code>longitude</code> 3. Convert hex <code>deviceId</code> to integer 4. Parse timestamp fields 5. Standardize column types</p> <p></p>"},{"location":"API_Reference.html#daskcleanwithtimestamps","title":"DaskCleanWithTimestamps","text":"<p>Module: <code>Cleaners/DaskCleanWithTimestamps.py</code></p> <p>Cleaner with timestamp-based filtering.</p>"},{"location":"API_Reference.html#usage-example_6","title":"Usage Example","text":"<pre><code>from Cleaners.DaskCleanWithTimestamps import DaskCleanWithTimestamps\n\ncleaner = DaskCleanWithTimestamps(\n    start_time=\"2023-01-01 00:00:00\",\n    end_time=\"2023-12-31 23:59:59\"\n)\ncleaned = cleaner.clean(data)\n</code></pre>"},{"location":"API_Reference.html#api-reference_6","title":"API Reference","text":""},{"location":"API_Reference.html#__init__start_time-str-end_time-str","title":"<code>__init__(start_time: str, end_time: str)</code>","text":"<p>Parameters: - <code>start_time</code> (str): ISO format timestamp - <code>end_time</code> (str): ISO format timestamp</p>"},{"location":"API_Reference.html#cleandata-dddataframe-dddataframe_1","title":"<code>clean(data: dd.DataFrame) -&gt; dd.DataFrame</code>","text":"<p>Filters data to timestamp range.</p> <p></p>"},{"location":"API_Reference.html#daskconnecteddrivinglargedatacleaner","title":"DaskConnectedDrivingLargeDataCleaner","text":"<p>Module: <code>Cleaners/DaskConnectedDrivingLargeDataCleaner.py</code></p> <p>Wrapper for applying spatial/temporal filtering to large datasets.</p>"},{"location":"API_Reference.html#usage-example_7","title":"Usage Example","text":"<pre><code>from Cleaners.DaskConnectedDrivingLargeDataCleaner import DaskConnectedDrivingLargeDataCleaner\n\ncleaner = DaskConnectedDrivingLargeDataCleaner(\n    filter_type=\"xy_offset_position\",\n    distance=2000,\n    center_x=500000,\n    center_y=4500000\n)\ncleaned = cleaner.clean(data)\n</code></pre>"},{"location":"API_Reference.html#api-reference_7","title":"API Reference","text":""},{"location":"API_Reference.html#__init__filter_type-str-filter_params","title":"<code>__init__(filter_type: str, **filter_params)</code>","text":"<p>Parameters: - <code>filter_type</code> (str): One of <code>'passthrough'</code>, <code>'xy_offset_position'</code>, <code>'bounding_box'</code> - <code>**filter_params</code>: Filter-specific parameters</p>"},{"location":"API_Reference.html#cleandata-dddataframe-dddataframe_2","title":"<code>clean(data: dd.DataFrame) -&gt; dd.DataFrame</code>","text":"<p>Applies configured filtering.</p> <p></p>"},{"location":"API_Reference.html#filtering-components","title":"Filtering Components","text":""},{"location":"API_Reference.html#daskcleanerwithpassthroughfilter","title":"DaskCleanerWithPassthroughFilter","text":"<p>Module: <code>Cleaners/DaskCleanerWithPassthroughFilter.py</code></p> <p>No-op filter (returns data unchanged).</p>"},{"location":"API_Reference.html#usage","title":"Usage","text":"<pre><code>from Cleaners.DaskCleanerWithPassthroughFilter import DaskCleanerWithPassthroughFilter\n\ncleaner = DaskCleanerWithPassthroughFilter()\nresult = cleaner.clean(data)  # Returns data as-is\n</code></pre>"},{"location":"API_Reference.html#daskcleanerwithfilterwithinrange","title":"DaskCleanerWithFilterWithinRange","text":"<p>Module: <code>Cleaners/DaskCleanerWithFilterWithinRange.py</code></p> <p>Filters records within distance from center point.</p>"},{"location":"API_Reference.html#usage_1","title":"Usage","text":"<pre><code>from Cleaners.DaskCleanerWithFilterWithinRange import DaskCleanerWithFilterWithinRange\n\ncleaner = DaskCleanerWithFilterWithinRange(\n    distance=2000,  # meters\n    center_x=500000,\n    center_y=4500000\n)\nfiltered = cleaner.clean(data)\n</code></pre>"},{"location":"API_Reference.html#daskcleanerwithfilterwithinrangexy","title":"DaskCleanerWithFilterWithinRangeXY","text":"<p>Module: <code>Cleaners/DaskCleanerWithFilterWithinRangeXY.py</code></p> <p>Filters using XY offset (rectangular bounding box).</p>"},{"location":"API_Reference.html#usage_2","title":"Usage","text":"<pre><code>from Cleaners.DaskCleanerWithFilterWithinRangeXY import DaskCleanerWithFilterWithinRangeXY\n\ncleaner = DaskCleanerWithFilterWithinRangeXY(\n    x_offset=2000,\n    y_offset=2000,\n    center_x=500000,\n    center_y=4500000\n)\nfiltered = cleaner.clean(data)\n</code></pre>"},{"location":"API_Reference.html#daskcleanerwithfilterwithinrangexyandday","title":"DaskCleanerWithFilterWithinRangeXYAndDay","text":"<p>Module: <code>Cleaners/DaskCleanerWithFilterWithinRangeXYAndDay.py</code></p> <p>Filters by XY offset and specific day.</p>"},{"location":"API_Reference.html#usage_3","title":"Usage","text":"<pre><code>from Cleaners.DaskCleanerWithFilterWithinRangeXYAndDay import DaskCleanerWithFilterWithinRangeXYAndDay\n\ncleaner = DaskCleanerWithFilterWithinRangeXYAndDay(\n    x_offset=2000,\n    y_offset=2000,\n    center_x=500000,\n    center_y=4500000,\n    day=15  # Day of month\n)\nfiltered = cleaner.clean(data)\n</code></pre>"},{"location":"API_Reference.html#daskcleanerwithfilterwithinrangexyanddaterange","title":"DaskCleanerWithFilterWithinRangeXYAndDateRange","text":"<p>Module: <code>Cleaners/DaskCleanerWithFilterWithinRangeXYAndDateRange.py</code></p> <p>Filters by XY offset and date range.</p>"},{"location":"API_Reference.html#usage_4","title":"Usage","text":"<pre><code>from Cleaners.DaskCleanerWithFilterWithinRangeXYAndDateRange import DaskCleanerWithFilterWithinRangeXYAndDateRange\n\ncleaner = DaskCleanerWithFilterWithinRangeXYAndDateRange(\n    x_offset=2000,\n    y_offset=2000,\n    center_x=500000,\n    center_y=4500000,\n    start_date=\"2023-01-01\",\n    end_date=\"2023-12-31\"\n)\nfiltered = cleaner.clean(data)\n</code></pre>"},{"location":"API_Reference.html#ml-components","title":"ML Components","text":""},{"location":"API_Reference.html#daskmconnecteddrivingdatacleaner","title":"DaskMConnectedDrivingDataCleaner","text":"<p>Module: <code>MachineLearning/DaskMConnectedDrivingDataCleaner.py</code></p> <p>Prepares data for ML training (feature engineering, column selection).</p>"},{"location":"API_Reference.html#features_5","title":"Features","text":"<ul> <li>Hex ID conversion</li> <li>Column selection (4 feature sets)</li> <li>Missing value handling</li> <li>Type standardization</li> </ul>"},{"location":"API_Reference.html#usage-example_8","title":"Usage Example","text":"<pre><code>from MachineLearning.DaskMConnectedDrivingDataCleaner import DaskMConnectedDrivingDataCleaner\n\ncleaner = DaskMConnectedDrivingDataCleaner(\n    column_set='minimal_xy_elev'\n)\nml_ready = cleaner.clean(attacked_data)\n</code></pre>"},{"location":"API_Reference.html#api-reference_8","title":"API Reference","text":""},{"location":"API_Reference.html#__init__column_set-str-minimal_xy_elev","title":"<code>__init__(column_set: str = 'minimal_xy_elev')</code>","text":"<p>Parameters: - <code>column_set</code> (str): One of <code>'minimal_xy_elev'</code>, <code>'standard'</code>, <code>'full_standard'</code>, <code>'all_available'</code></p>"},{"location":"API_Reference.html#cleandata-dddataframe-dddataframe_3","title":"<code>clean(data: dd.DataFrame) -&gt; dd.DataFrame</code>","text":"<p>Prepares data for ML training.</p> <p>Returns: - <code>dd.DataFrame</code>: ML-ready features + <code>isAttacker</code> label</p> <p></p>"},{"location":"API_Reference.html#daskconnecteddrivingattacker","title":"DaskConnectedDrivingAttacker","text":"<p>Module: <code>Attacks/DaskConnectedDrivingAttacker.py</code></p> <p>Simulates position falsification attacks on BSM data.</p>"},{"location":"API_Reference.html#features_6","title":"Features","text":"<ul> <li>7 attack types (rand_offset, const_offset, position_swap, etc.)</li> <li>Configurable attacker percentage</li> <li>Random seed support</li> <li>Adds <code>isAttacker</code> label column</li> </ul>"},{"location":"API_Reference.html#usage-example_9","title":"Usage Example","text":"<pre><code>from Attacks.DaskConnectedDrivingAttacker import DaskConnectedDrivingAttacker\n\nattacker = DaskConnectedDrivingAttacker(\n    attack_type='rand_offset',\n    attacker_percentage=0.30,\n    min_distance=10,\n    max_distance=20,\n    random_seed=42\n)\nattacked_data = attacker.add_attackers(clean_data)\n\n# Check attack statistics\ntotal = attacked_data.shape[0].compute()\nattackers = attacked_data[attacked_data['isAttacker'] == 1].shape[0].compute()\nprint(f\"Attackers: {attackers}/{total} ({attackers/total:.1%})\")\n</code></pre>"},{"location":"API_Reference.html#api-reference_9","title":"API Reference","text":""},{"location":"API_Reference.html#__init__attack_type-str-attacker_percentage-float-attack_params","title":"<code>__init__(attack_type: str, attacker_percentage: float, **attack_params)</code>","text":"<p>Parameters: - <code>attack_type</code> (str): Attack type (see table below) - <code>attacker_percentage</code> (float): Fraction of attackers (0.0-1.0) - <code>random_seed</code> (int, optional): For reproducibility - <code>**attack_params</code>: Attack-specific parameters</p>"},{"location":"API_Reference.html#add_attackersdata-dddataframe-dddataframe","title":"<code>add_attackers(data: dd.DataFrame) -&gt; dd.DataFrame</code>","text":"<p>Applies attack simulation.</p> <p>Returns: - <code>dd.DataFrame</code>: Data with <code>isAttacker</code> column (0=normal, 1=attacker)</p>"},{"location":"API_Reference.html#attack-parameters-by-type","title":"Attack Parameters by Type","text":"Attack Type Required Parameters <code>rand_offset</code> <code>min_distance</code>, <code>max_distance</code> <code>const_offset_per_id</code> <code>min_distance</code>, <code>max_distance</code> <code>rand_position</code> <code>min_distance</code>, <code>max_distance</code> <code>position_swap</code> None (optional: <code>swap_distance</code>) <code>const_offset</code> <code>offset_x</code>, <code>offset_y</code> <code>override_const</code> <code>target_x</code>, <code>target_y</code> <code>override_rand</code> <code>min_x</code>, <code>max_x</code>, <code>min_y</code>, <code>max_y</code>"},{"location":"API_Reference.html#utilities","title":"Utilities","text":""},{"location":"API_Reference.html#daskudfregistry","title":"DaskUDFRegistry","text":"<p>Module: <code>Helpers/DaskUDFs/DaskUDFRegistry.py</code></p> <p>Registry of Dask user-defined functions for common operations.</p>"},{"location":"API_Reference.html#available-functions","title":"Available Functions","text":"<ul> <li><code>haversine_distance(lat1, lon1, lat2, lon2)</code> - Great circle distance</li> <li><code>euclidean_distance_2d(x1, y1, x2, y2)</code> - Planar distance</li> <li><code>bearing(lat1, lon1, lat2, lon2)</code> - Heading between points</li> <li><code>hex_to_int(hex_str)</code> - Hex string to integer</li> <li><code>timestamp_to_unix(timestamp)</code> - Timestamp to epoch</li> </ul>"},{"location":"API_Reference.html#usage-example_10","title":"Usage Example","text":"<pre><code>from Helpers.DaskUDFs.DaskUDFRegistry import DaskUDFRegistry\nimport dask.dataframe as dd\n\n# Register functions\nregistry = DaskUDFRegistry()\nregistry.register_all()\n\n# Use in Dask operations\ndata['distance'] = registry.haversine_distance(\n    data['lat1'], data['lon1'],\n    data['lat2'], data['lon2']\n)\n</code></pre> <p>For complete UDF documentation, see Helpers/DaskUDFs/README.md.</p> <p></p>"},{"location":"API_Reference.html#testing","title":"Testing","text":""},{"location":"API_Reference.html#daskfixtures","title":"DaskFixtures","text":"<p>Module: <code>Test/DaskFixtures.py</code></p> <p>Pytest fixtures for Dask testing.</p>"},{"location":"API_Reference.html#available-fixtures","title":"Available Fixtures","text":"<ul> <li><code>dask_client</code> - Configured Dask client</li> <li><code>sample_dask_dataframe</code> - Small test DataFrame</li> <li><code>large_dask_dataframe</code> - 100k row test DataFrame</li> </ul>"},{"location":"API_Reference.html#usage-example_11","title":"Usage Example","text":"<pre><code>import pytest\nfrom Test.DaskFixtures import *\n\ndef test_pipeline(dask_client, sample_dask_dataframe):\n    # Test with pre-configured client and data\n    result = my_pipeline.run(sample_dask_dataframe)\n    assert result.shape[0].compute() &gt; 0\n</code></pre>"},{"location":"API_Reference.html#performance-guidelines","title":"Performance Guidelines","text":""},{"location":"API_Reference.html#memory-management","title":"Memory Management","text":"<p>Rule of thumb: Each worker needs ~8GB for 15M row datasets</p> <pre><code># Recommended configuration\nn_workers = 6\nmemory_per_worker = '8GB'\ntotal_memory_required = 64  # GB\n</code></pre> <p>Monitor memory: <pre><code>mem = DaskSessionManager.get_memory_usage()\nif mem['percent_used'] &gt; 80:\n    # Reduce partitions or restart cluster\n    DaskSessionManager.restart()\n</code></pre></p>"},{"location":"API_Reference.html#caching-best-practices","title":"Caching Best Practices","text":"<p>When to use <code>@DaskParquetCache</code>: - \u2705 Expensive operations (file reads, complex joins) - \u2705 Repeated operations with same inputs - \u2705 Intermediate pipeline stages</p> <p>When NOT to cache: - \u274c Cheap operations (simple filters, column selection) - \u274c One-time operations - \u274c Operations on small DataFrames (&lt;1000 rows)</p> <p>Cache hit rate target: \u226585% for iterative workflows</p>"},{"location":"API_Reference.html#partitioning-guidelines","title":"Partitioning Guidelines","text":"<p>Partition size recommendations:</p> Dataset Size Recommended Partitions Blocksize 1M rows 20-30 64MB 5M rows 40-50 128MB 15M rows 60-80 128MB 50M+ rows 100-120 256MB <p>Configure partitions: <pre><code># In DaskPipelineRunner config\n{\n  \"data\": {\n    \"partitions\": 60,  # For 15M rows\n    ...\n  }\n}\n</code></pre></p>"},{"location":"API_Reference.html#computation-best-practices","title":"Computation Best Practices","text":"<p>Avoid eager evaluation: <pre><code># \u274c Bad: Forces computation multiple times\ncount = data.shape[0].compute()\nmean = data['speed'].mean().compute()\nmax_val = data['speed'].max().compute()\n\n# \u2705 Good: Build graph, compute once\nresult = data.agg({\n    'count': 'size',\n    'speed_mean': ('speed', 'mean'),\n    'speed_max': ('speed', 'max')\n}).compute()\n</code></pre></p> <p>Use persist() for reused DataFrames: <pre><code># If data will be used multiple times\ndata = data.persist()\n\n# Now multiple operations are fast\nfiltered1 = data[data['speed'] &gt; 0]\nfiltered2 = data[data['heading'] &lt; 180]\n</code></pre></p>"},{"location":"API_Reference.html#migration-from-pandas","title":"Migration from Pandas","text":""},{"location":"API_Reference.html#common-patterns","title":"Common Patterns","text":"<p>Pandas: <pre><code>df = pd.read_csv(\"data.csv\")\nresult = df[df['speed'] &gt; 0].groupby('deviceId').mean()\n</code></pre></p> <p>Dask equivalent: <pre><code>df = dd.read_csv(\"data.csv\", blocksize='128MB')\nresult = df[df['speed'] &gt; 0].groupby('deviceId').mean().compute()\n#                                                        ^^^^^^^^^ Add this\n</code></pre></p>"},{"location":"API_Reference.html#key-differences","title":"Key Differences","text":"Operation Pandas Dask Read CSV <code>pd.read_csv()</code> <code>dd.read_csv(..., blocksize='128MB')</code> Get result Direct Append <code>.compute()</code> Length <code>len(df)</code> <code>df.shape[0].compute()</code> Unique values <code>df['col'].unique()</code> <code>df['col'].unique().compute()</code> Iteration <code>for row in df.iterrows()</code> Not recommended (use map_partitions)"},{"location":"API_Reference.html#sklearn-integration","title":"sklearn Integration","text":"<p>Always compute before sklearn: <pre><code># \u274c Wrong\nmodel.fit(dask_df[features], dask_df['label'])\n\n# \u2705 Correct\nX = dask_df[features].compute()\ny = dask_df['label'].compute()\nmodel.fit(X, y)\n</code></pre></p>"},{"location":"API_Reference.html#error-handling","title":"Error Handling","text":""},{"location":"API_Reference.html#common-errors","title":"Common Errors","text":"<p>1. <code>TypeError: expected DataFrame, got DataFrame</code> - Cause: Passing Dask DataFrame to pandas/sklearn function - Solution: Call <code>.compute()</code> first</p> <p>2. <code>MemoryError</code> or <code>KilledWorker</code> - Cause: Partition too large, exceeded worker memory - Solution: Increase partitions or reduce blocksize</p> <p>3. <code>KeyError: 'column_name'</code> - Cause: Column doesn't exist (check case sensitivity) - Solution: Use <code>data.columns.tolist()</code> to verify</p> <p>4. Low cache hit rate (&lt;50%) - Cause: Cache keys changing (arguments or code modified) - Solution: Check <code>cache_variables</code> parameter</p> <p>For comprehensive troubleshooting, see Troubleshooting_Guide.md.</p>"},{"location":"API_Reference.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Configuration Guide: DaskPipelineRunner_Configuration_Guide.md</li> <li>Troubleshooting: Troubleshooting_Guide.md</li> <li>UDF Documentation: Helpers/DaskUDFs/README.md</li> <li>Main README: README.md</li> <li>Example Configs: MClassifierPipelines/configs/</li> </ul> <p>Document Version: 1.0.0 Last Updated: 2026-01-18 Maintained by: ConnectedDrivingPipelineV4 Development Team</p>"},{"location":"CACHE_KEY_SPECIFICATION.html","title":"Cache Key Specification","text":""},{"location":"CACHE_KEY_SPECIFICATION.html#preventing-data-confusion-in-multi-source-cv-pilot-data","title":"Preventing Data Confusion in Multi-Source CV Pilot Data","text":"<p>Version: 1.1 Date: 2026-01-28 Status: Reviewed &amp; Audited Related: WYDOT Data Infrastructure Plan </p>"},{"location":"CACHE_KEY_SPECIFICATION.html#the-problem","title":"The Problem","text":"<p>When caching data from multiple sources (wydot, nycdot, thea), we MUST ensure:</p> <ol> <li>Source Isolation: WYDOT data never gets mixed with NYCDOT data</li> <li>Date Range Handling: Partial overlaps are handled correctly</li> <li>Message Type Isolation: BSM never gets confused with TIM or EVENT</li> <li>Integrity Verification: Corrupted/incomplete cache is detected</li> <li>Schema Evolution: Old cache is invalidated when schema changes</li> </ol>"},{"location":"CACHE_KEY_SPECIFICATION.html#cache-key-design","title":"Cache Key Design","text":""},{"location":"CACHE_KEY_SPECIFICATION.html#primary-key-structure","title":"Primary Key Structure","text":"<pre><code>{source}/{message_type}/{year}/{month:02d}/{day:02d}\n</code></pre> <p>Examples: <pre><code>wydot/BSM/2021/04/01      \u2192 wydot BSM data for April 1, 2021\nnycdot/EVENT/2021/04/01   \u2192 nycdot EVENT data for April 1, 2021\nthea/SPAT/2021/04/01      \u2192 thea SPAT data for April 1, 2021\n</code></pre></p> <p>Critical Design Decision: Source is the FIRST component, making it impossible to accidentally mix sources.</p>"},{"location":"CACHE_KEY_SPECIFICATION.html#file-system-layout","title":"File System Layout","text":"<pre><code>cache/\n\u251c\u2500\u2500 manifest.json                      # Master index\n\u251c\u2500\u2500 wydot/\n\u2502   \u251c\u2500\u2500 BSM/\n\u2502   \u2502   \u2514\u2500\u2500 2021/\n\u2502   \u2502       \u2514\u2500\u2500 04/\n\u2502   \u2502           \u251c\u2500\u2500 01.parquet        # April 1\n\u2502   \u2502           \u251c\u2500\u2500 02.parquet        # April 2\n\u2502   \u2502           \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 TIM/\n\u2502       \u2514\u2500\u2500 2021/\n\u2502           \u2514\u2500\u2500 04/\n\u2502               \u2514\u2500\u2500 01.parquet\n\u251c\u2500\u2500 thea/\n\u2502   \u251c\u2500\u2500 BSM/\n\u2502   \u2514\u2500\u2500 SPAT/\n\u2514\u2500\u2500 nycdot/\n    \u2514\u2500\u2500 EVENT/\n</code></pre>"},{"location":"CACHE_KEY_SPECIFICATION.html#manifest-schema","title":"Manifest Schema","text":"<p>The manifest (<code>manifest.json</code>) tracks all cached data and its integrity:</p> <pre><code>{\n  \"version\": 2,\n  \"created_at\": \"2026-01-28T23:00:00Z\",\n  \"last_modified\": \"2026-01-28T23:30:00Z\",\n\n  \"entries\": {\n    \"wydot/BSM/2021/04/01\": {\n      \"status\": \"complete\",\n      \"file_path\": \"wydot/BSM/2021/04/01.parquet\",\n      \"row_count\": 125432,\n      \"file_size_bytes\": 45123456,\n      \"source_file_count\": 24,\n      \"checksum_sha256\": \"a1b2c3d4e5f6...\",\n      \"fetched_at\": \"2026-01-28T22:00:00Z\",\n      \"schema_version\": \"6\",\n      \"s3_etags\": [\"etag1\", \"etag2\", \"...\"],\n      \"config_hash\": \"cfg_abc123\"\n    },\n\n    \"nycdot/EVENT/2021/04/01\": {\n      \"status\": \"complete\",\n      \"file_path\": \"nycdot/EVENT/2021/04/01.parquet\",\n      \"row_count\": 8923,\n      \"file_size_bytes\": 2345678,\n      \"source_file_count\": 8,\n      \"checksum_sha256\": \"x1y2z3...\",\n      \"fetched_at\": \"2026-01-28T22:15:00Z\",\n      \"schema_version\": \"1\",\n      \"s3_etags\": [\"etag1\", \"...\"],\n      \"config_hash\": \"cfg_abc123\"\n    }\n  },\n\n  \"in_progress\": {\n    \"wydot/BSM/2021/04/03\": {\n      \"status\": \"downloading\",\n      \"started_at\": \"2026-01-28T23:00:00Z\",\n      \"files_completed\": 12,\n      \"files_total\": 24,\n      \"bytes_downloaded\": 23456789\n    }\n  }\n}\n</code></pre>"},{"location":"CACHE_KEY_SPECIFICATION.html#entry-status-values","title":"Entry Status Values","text":"Status Meaning <code>complete</code> Fully downloaded and verified <code>downloading</code> Currently being fetched <code>failed</code> Download failed (will retry) <code>corrupted</code> Checksum mismatch (will re-download) <code>expired</code> TTL exceeded (will re-download)"},{"location":"CACHE_KEY_SPECIFICATION.html#cache-lookup-algorithm","title":"Cache Lookup Algorithm","text":"<pre><code>class CacheManager:\n    def get_data(\n        self,\n        source: str,          # \"wydot\", \"nycdot\", \"thea\"\n        message_type: str,    # \"BSM\", \"TIM\", \"SPAT\", \"EVENT\"\n        start_date: date,\n        end_date: date\n    ) -&gt; DataFrame:\n        \"\"\"\n        Retrieve data for the specified source, type, and date range.\n\n        CRITICAL: source and message_type are ALWAYS explicit.\n        We NEVER infer them from context or defaults.\n        \"\"\"\n\n        # 1. VALIDATE INPUTS (fail fast on invalid source)\n        if source not in VALID_SOURCES:\n            raise ValueError(f\"Invalid source: {source}. Valid: {VALID_SOURCES}\")\n        if message_type not in VALID_MESSAGE_TYPES[source]:\n            raise ValueError(f\"Invalid message_type {message_type} for source {source}\")\n\n        # 2. Build cache keys for each date in range\n        dates_needed = list(date_range(start_date, end_date))\n        cache_keys = [\n            f\"{source}/{message_type}/{d.year}/{d.month:02d}/{d.day:02d}\"\n            for d in dates_needed\n        ]\n\n        # 3. Check manifest for each key\n        cached_keys = []\n        missing_keys = []\n\n        for key, date in zip(cache_keys, dates_needed):\n            entry = self.manifest.get(key)\n\n            if entry is None:\n                missing_keys.append((key, date))\n            elif entry.status != \"complete\":\n                missing_keys.append((key, date))\n            elif not self._verify_integrity(entry):\n                self._mark_corrupted(key)\n                missing_keys.append((key, date))\n            elif self._is_expired(entry):\n                missing_keys.append((key, date))\n            else:\n                cached_keys.append(key)\n\n        # 4. Fetch missing data (source and type are EXPLICIT)\n        if missing_keys:\n            self._fetch_from_s3(\n                source=source,           # Explicit!\n                message_type=message_type, # Explicit!\n                dates=[d for _, d in missing_keys]\n            )\n            cached_keys.extend([k for k, _ in missing_keys])\n\n        # 5. Load and combine parquet files\n        dfs = []\n        for key in cached_keys:\n            path = self.cache_dir / f\"{key}.parquet\"\n            dfs.append(pd.read_parquet(path))\n\n        return pd.concat(dfs, ignore_index=True)\n\n    def _verify_integrity(self, entry: CacheEntry) -&gt; bool:\n        \"\"\"Verify file exists and checksum matches.\"\"\"\n        path = self.cache_dir / entry.file_path\n        if not path.exists():\n            return False\n\n        actual_checksum = compute_sha256(path)\n        return actual_checksum == entry.checksum_sha256\n</code></pre>"},{"location":"CACHE_KEY_SPECIFICATION.html#edge-cases-safeguards","title":"Edge Cases &amp; Safeguards","text":""},{"location":"CACHE_KEY_SPECIFICATION.html#case-1-source-confusion-prevention","title":"Case 1: Source Confusion Prevention","text":"<p>Scenario: User fetches WYDOT data, then later fetches NYCDOT data. Could they get mixed?</p> <p>Answer: NO. The source is the first path component.</p> <pre><code># These produce COMPLETELY DIFFERENT cache keys:\nget_data(source=\"wydot\", message_type=\"BSM\", ...)\n# Cache key: wydot/BSM/2021/04/01\n\nget_data(source=\"nycdot\", message_type=\"EVENT\", ...)  \n# Cache key: nycdot/EVENT/2021/04/01\n\n# They are in DIFFERENT directories. Impossible to mix.\n</code></pre>"},{"location":"CACHE_KEY_SPECIFICATION.html#case-2-partial-date-range-overlap","title":"Case 2: Partial Date Range Overlap","text":"<p>Scenario:  1. User caches April 1-10 2. User queries April 5-15</p> <p>Handling: <pre><code># Query: April 5-15\ncache_keys = [\n    \"wydot/BSM/2021/04/05\",  # \u2705 Cached (from first query)\n    \"wydot/BSM/2021/04/06\",  # \u2705 Cached\n    ...\n    \"wydot/BSM/2021/04/10\",  # \u2705 Cached\n    \"wydot/BSM/2021/04/11\",  # \u274c Missing - will fetch\n    ...\n    \"wydot/BSM/2021/04/15\",  # \u274c Missing - will fetch\n]\n# Only April 11-15 is fetched. April 5-10 comes from cache.\n</code></pre></p>"},{"location":"CACHE_KEY_SPECIFICATION.html#case-3-message-type-confusion","title":"Case 3: Message Type Confusion","text":"<p>Scenario: User fetches BSM, then fetches TIM. Could they get mixed?</p> <p>Answer: NO. Message type is the second path component.</p> <pre><code>get_data(source=\"wydot\", message_type=\"BSM\", ...)\n# Cache key: wydot/BSM/2021/04/01\n\nget_data(source=\"wydot\", message_type=\"TIM\", ...)\n# Cache key: wydot/TIM/2021/04/01\n\n# Different directories. Impossible to mix.\n</code></pre>"},{"location":"CACHE_KEY_SPECIFICATION.html#case-4-incomplete-download","title":"Case 4: Incomplete Download","text":"<p>Scenario: Download was interrupted at 50%.</p> <p>Handling: <pre><code># Before download completes, manifest shows:\n{\n  \"wydot/BSM/2021/04/01\": {\n    \"status\": \"downloading\",  # NOT \"complete\"\n    \"files_completed\": 12,\n    \"files_total\": 24\n  }\n}\n\n# On next query, we see status != \"complete\", so we re-fetch.\n# The incomplete parquet file is overwritten.\n</code></pre></p>"},{"location":"CACHE_KEY_SPECIFICATION.html#case-5-corrupted-cache-file","title":"Case 5: Corrupted Cache File","text":"<p>Scenario: Parquet file is corrupted (disk error, etc.)</p> <p>Handling: <pre><code>def _verify_integrity(self, entry):\n    path = self.cache_dir / entry.file_path\n\n    # Check 1: File exists\n    if not path.exists():\n        return False\n\n    # Check 2: File size matches\n    if path.stat().st_size != entry.file_size_bytes:\n        return False\n\n    # Check 3: Checksum matches (optional, slower)\n    if self.verify_checksums:\n        actual = compute_sha256(path)\n        if actual != entry.checksum_sha256:\n            return False\n\n    return True\n</code></pre></p>"},{"location":"CACHE_KEY_SPECIFICATION.html#case-6-schema-version-change","title":"Case 6: Schema Version Change","text":"<p>Scenario: S3 data schema changes from version 5 to version 6. Old cache is incompatible.</p> <p>Handling: <pre><code>CURRENT_SCHEMA_VERSION = \"6\"\n\ndef _is_compatible(self, entry):\n    return entry.schema_version == CURRENT_SCHEMA_VERSION\n\n# On query, if schema version mismatches, treat as cache miss.\n# Re-fetch and re-process with new schema.\n</code></pre></p>"},{"location":"CACHE_KEY_SPECIFICATION.html#case-7-configuration-change","title":"Case 7: Configuration Change","text":"<p>Scenario: User changes processing config (e.g., adds coordinate filtering). Old cache used different config.</p> <p>Handling: <pre><code>def _compute_config_hash(self, config: DataSourceConfig) -&gt; str:\n    \"\"\"Hash the config settings that affect cached data.\"\"\"\n    relevant = {\n        \"validate_schema\": config.validate_schema,\n        \"drop_invalid\": config.drop_invalid,\n        \"coordinate_bounds\": config.coordinate_bounds,\n        # ... other relevant settings\n    }\n    return hashlib.sha256(json.dumps(relevant, sort_keys=True).encode()).hexdigest()[:12]\n\n# Cache entries include config_hash.\n# On query, if config_hash doesn't match, re-process.\n</code></pre></p>"},{"location":"CACHE_KEY_SPECIFICATION.html#validation-rules","title":"Validation Rules","text":""},{"location":"CACHE_KEY_SPECIFICATION.html#source-validation","title":"Source Validation","text":"<pre><code>VALID_SOURCES = {\"wydot\", \"wydot_backup\", \"thea\", \"nycdot\"}\n\nVALID_MESSAGE_TYPES = {\n    \"wydot\": {\"BSM\", \"TIM\"},\n    \"wydot_backup\": {\"BSM\", \"TIM\"},\n    \"thea\": {\"BSM\", \"TIM\", \"SPAT\"},\n    \"nycdot\": {\"EVENT\"},\n}\n\ndef validate_source_and_type(source: str, message_type: str):\n    if source not in VALID_SOURCES:\n        raise ValueError(f\"Invalid source '{source}'. Valid: {VALID_SOURCES}\")\n\n    valid_types = VALID_MESSAGE_TYPES[source]\n    if message_type not in valid_types:\n        raise ValueError(\n            f\"Invalid message_type '{message_type}' for source '{source}'. \"\n            f\"Valid types for {source}: {valid_types}\"\n        )\n</code></pre>"},{"location":"CACHE_KEY_SPECIFICATION.html#never-infer-always-explicit","title":"Never Infer, Always Explicit","text":"<pre><code># \u274c BAD: Inferring source from somewhere\ndef get_data(start_date, end_date):\n    source = self.default_source  # DANGEROUS!\n    ...\n\n# \u2705 GOOD: Always require explicit source\ndef get_data(source: str, message_type: str, start_date, end_date):\n    validate_source_and_type(source, message_type)  # Fail fast\n    ...\n</code></pre>"},{"location":"CACHE_KEY_SPECIFICATION.html#testing-scenarios","title":"Testing Scenarios","text":""},{"location":"CACHE_KEY_SPECIFICATION.html#test-1-source-isolation","title":"Test 1: Source Isolation","text":"<pre><code>def test_source_isolation():\n    cache = CacheManager(\"test_cache\")\n\n    # Fetch WYDOT data\n    wydot_df = cache.get_data(\n        source=\"wydot\",\n        message_type=\"BSM\",\n        start_date=date(2021, 4, 1),\n        end_date=date(2021, 4, 1)\n    )\n\n    # Fetch NYCDOT data\n    nycdot_df = cache.get_data(\n        source=\"nycdot\",\n        message_type=\"EVENT\",\n        start_date=date(2021, 4, 1),\n        end_date=date(2021, 4, 1)\n    )\n\n    # Verify completely different data\n    assert \"coreData_position_lat\" in wydot_df.columns  # BSM field\n    assert \"eventHeader\" in nycdot_df.columns  # EVENT field\n\n    # Verify separate cache files\n    assert (cache.cache_dir / \"wydot/BSM/2021/04/01.parquet\").exists()\n    assert (cache.cache_dir / \"nycdot/EVENT/2021/04/01.parquet\").exists()\n</code></pre>"},{"location":"CACHE_KEY_SPECIFICATION.html#test-2-message-type-isolation","title":"Test 2: Message Type Isolation","text":"<pre><code>def test_message_type_isolation():\n    cache = CacheManager(\"test_cache\")\n\n    # Fetch BSM\n    bsm_df = cache.get_data(\"wydot\", \"BSM\", date(2021, 4, 1), date(2021, 4, 1))\n\n    # Fetch TIM\n    tim_df = cache.get_data(\"wydot\", \"TIM\", date(2021, 4, 1), date(2021, 4, 1))\n\n    # Verify different structures\n    assert \"coreData\" in str(bsm_df.columns)\n    assert \"travelerInformation\" in str(tim_df.columns)  # TIM-specific\n</code></pre>"},{"location":"CACHE_KEY_SPECIFICATION.html#test-3-invalid-source-rejection","title":"Test 3: Invalid Source Rejection","text":"<pre><code>def test_invalid_source_rejected():\n    cache = CacheManager(\"test_cache\")\n\n    with pytest.raises(ValueError, match=\"Invalid source\"):\n        cache.get_data(\n            source=\"invalid_source\",\n            message_type=\"BSM\",\n            start_date=date(2021, 4, 1),\n            end_date=date(2021, 4, 1)\n        )\n\ndef test_invalid_message_type_for_source():\n    cache = CacheManager(\"test_cache\")\n\n    # NYCDOT only has EVENT, not BSM\n    with pytest.raises(ValueError, match=\"Invalid message_type\"):\n        cache.get_data(\n            source=\"nycdot\",\n            message_type=\"BSM\",  # Invalid for nycdot!\n            start_date=date(2021, 4, 1),\n            end_date=date(2021, 4, 1)\n        )\n</code></pre>"},{"location":"CACHE_KEY_SPECIFICATION.html#summary","title":"Summary","text":"Concern Solution Source confusion Source is FIRST path component Message type confusion Message type is SECOND path component Partial downloads Status field in manifest (<code>downloading</code> vs <code>complete</code>) Corrupted files SHA256 checksum verification Schema changes Schema version in manifest Config changes Config hash in manifest Invalid sources Strict validation on every call Inference errors NEVER infer, ALWAYS require explicit params <p>Key Principle: The cache key <code>{source}/{message_type}/{year}/{month}/{day}</code> makes it structurally impossible to confuse data from different sources or message types.</p>"},{"location":"CACHE_KEY_SPECIFICATION.html#additional-safeguards-added-in-audit","title":"Additional Safeguards (Added in Audit)","text":""},{"location":"CACHE_KEY_SPECIFICATION.html#case-8-empty-s3-prefix-no-data-for-date","title":"Case 8: Empty S3 Prefix (No Data for Date)","text":"<p>Scenario: April 15, 2021 has no BSM data in S3 (system was down).</p> <p>Problem: Without tracking, we'd re-fetch every time.</p> <p>Solution: <pre><code># Manifest tracks \"no data\" explicitly:\n{\n  \"wydot/BSM/2021/04/15\": {\n    \"status\": \"no_data\",  # Not \"complete\", not missing\n    \"checked_at\": \"2026-01-28T22:00:00Z\",\n    \"s3_files_found\": 0\n  }\n}\n\n# On cache check:\nif entry.status == \"no_data\":\n    # Return empty DataFrame, don't re-fetch\n    return pd.DataFrame()\n</code></pre></p>"},{"location":"CACHE_KEY_SPECIFICATION.html#case-9-concurrent-access","title":"Case 9: Concurrent Access","text":"<p>Scenario: Two processes try to fetch the same date simultaneously.</p> <p>Solution: File locking per cache key. <pre><code>from filelock import FileLock\n\ndef save_processed(self, key, df):\n    lock = FileLock(f\".locks/{key.replace('/', '_')}.lock\", timeout=60)\n    with lock:\n        # Only one process writes at a time\n        temp = f\".tmp_{uuid4()}.parquet\"\n        df.to_parquet(temp)\n        os.rename(temp, f\"{key}.parquet\")  # Atomic\n</code></pre></p>"},{"location":"CACHE_KEY_SPECIFICATION.html#case-10-disk-full-during-write","title":"Case 10: Disk Full During Write","text":"<p>Solution: Atomic writes prevent partial files. <pre><code>def save_processed(self, key, df):\n    temp_path = self.cache_dir / f\".tmp_{uuid.uuid4()}.parquet\"\n    final_path = self.cache_dir / f\"{key}.parquet\"\n\n    try:\n        df.to_parquet(temp_path)\n        temp_path.rename(final_path)\n    except OSError as e:\n        temp_path.unlink(missing_ok=True)  # Clean up temp file\n        if \"No space left\" in str(e):\n            self.evict_lru(bytes_needed=temp_path.stat().st_size)\n            raise  # Let caller retry\n        raise\n</code></pre></p>"},{"location":"CACHE_KEY_SPECIFICATION.html#case-11-time-zone-confusion","title":"Case 11: Time Zone Confusion","text":"<p>Scenario: User specifies April 1 in local time (America/Denver), but S3 uses UTC.</p> <p>Solution: Always normalize to UTC before cache key generation. <pre><code>import pytz\nfrom datetime import datetime\n\ndef normalize_date(dt: date, tz: str) -&gt; date:\n    \"\"\"Convert local date to UTC date.\"\"\"\n    if tz == \"UTC\":\n        return dt\n    local_tz = pytz.timezone(tz)\n    local_dt = local_tz.localize(datetime.combine(dt, datetime.min.time()))\n    return local_dt.astimezone(pytz.UTC).date()\n\n# Cache key always uses UTC date:\ncache_key = f\"{source}/{msg_type}/{utc_date.year}/...\"\n</code></pre></p>"},{"location":"CACHE_KEY_SPECIFICATION.html#audit-checklist","title":"Audit Checklist","text":"<ul> <li>[x] Source isolation (FIRST path component)</li> <li>[x] Message type isolation (SECOND path component)</li> <li>[x] Partial downloads (status tracking)</li> <li>[x] Corrupted files (SHA256 checksums)</li> <li>[x] Schema changes (version in manifest)</li> <li>[x] Config changes (config hash)</li> <li>[x] Invalid sources (strict validation)</li> <li>[x] Never infer (always explicit params)</li> <li>[x] Empty data (no_data status)</li> <li>[x] Concurrent access (file locking)</li> <li>[x] Disk full (atomic writes)</li> <li>[x] Time zones (UTC normalization)</li> </ul>"},{"location":"DaskPipelineRunner_Configuration_Guide.html","title":"DaskPipelineRunner Configuration Guide","text":"<p>This guide provides comprehensive documentation for configuring <code>DaskPipelineRunner</code> pipelines using JSON configuration files.</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#table-of-contents","title":"Table of Contents","text":"<ol> <li>Quick Start</li> <li>Configuration Schema</li> <li>Section Reference</li> <li>pipeline_name</li> <li>data</li> <li>features</li> <li>attacks</li> <li>ml</li> <li>cache</li> <li>Attack Types Reference</li> <li>Feature Column Sets</li> <li>Filtering Types</li> <li>Complete Examples</li> <li>Configuration Validation</li> <li>Best Practices</li> </ol>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#quick-start","title":"Quick Start","text":""},{"location":"DaskPipelineRunner_Configuration_Guide.html#minimal-working-configuration","title":"Minimal Working Configuration","text":"<p>The simplest configuration requires only the essential fields:</p> <pre><code>{\n  \"pipeline_name\": \"my_first_pipeline\",\n  \"data\": {\n    \"source_file\": \"data/bsm_data.csv\",\n    \"filtering\": {\n      \"type\": \"none\"\n    }\n  },\n  \"features\": {\n    \"columns\": \"minimal_xy_elev\"\n  },\n  \"attacks\": {\n    \"enabled\": false\n  },\n  \"ml\": {\n    \"train_test_split\": {\n      \"type\": \"random\",\n      \"train_ratio\": 0.8,\n      \"test_ratio\": 0.2,\n      \"random_seed\": 42\n    }\n  }\n}\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#usage","title":"Usage","text":"<pre><code>from MachineLearning.DaskPipelineRunner import DaskPipelineRunner\n\n# Load from JSON file\npipeline = DaskPipelineRunner.from_config(\"config.json\")\n\n# Or create from dict\npipeline = DaskPipelineRunner({\n    \"pipeline_name\": \"my_pipeline\",\n    # ... config dict ...\n})\n\n# Run the pipeline\nresults = pipeline.run()\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#configuration-schema","title":"Configuration Schema","text":"<p>A complete DaskPipelineRunner configuration is a JSON object with six top-level sections:</p> <pre><code>{\n  \"pipeline_name\": \"string (required)\",\n  \"data\": { /* object (required) */ },\n  \"features\": { /* object (required) */ },\n  \"attacks\": { /* object (required) */ },\n  \"ml\": { /* object (required) */ },\n  \"cache\": { /* object (optional) */ }\n}\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#section-reference","title":"Section Reference","text":""},{"location":"DaskPipelineRunner_Configuration_Guide.html#pipeline_name","title":"<code>pipeline_name</code>","text":"<p>Type: <code>string</code> (required)</p> <p>Description: Unique identifier for this pipeline run. Used for: - Log file naming - Cache directory naming - Output file prefixes - Tracking in Dask dashboard</p> <p>Example: <pre><code>\"pipeline_name\": \"xy_offset_2000m_rand_offset_30attackers\"\n</code></pre></p> <p>Best Practices: - Use descriptive names that indicate key parameters - Include attack type, ratio, and filtering in the name - Avoid special characters (use underscores instead of spaces) - Keep under 100 characters for filesystem compatibility</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#data","title":"<code>data</code>","text":"<p>Type: <code>object</code> (required)</p> <p>Description: Configures data loading, spatial filtering, and temporal filtering.</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#required-fields","title":"Required Fields","text":""},{"location":"DaskPipelineRunner_Configuration_Guide.html#source_file","title":"<code>source_file</code>","text":"<p>Type: <code>string</code> (required)</p> <p>Path to the source CSV file containing BSM data.</p> <pre><code>\"source_file\": \"data/bsm_data.csv\"\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#filtering","title":"<code>filtering</code>","text":"<p>Type: <code>object</code> (required)</p> <p>Spatial filtering configuration. See Filtering Types for details.</p> <pre><code>\"filtering\": {\n  \"type\": \"xy_offset_position\",\n  \"distance_meters\": 2000,\n  \"center_x\": -106.0831353,\n  \"center_y\": 41.5430216\n}\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#optional-fields","title":"Optional Fields","text":""},{"location":"DaskPipelineRunner_Configuration_Guide.html#num_subsection_rows","title":"<code>num_subsection_rows</code>","text":"<p>Type: <code>integer</code> (optional, default: 100000)</p> <p>Number of rows per Dask partition. Controls parallelism and memory usage.</p> <pre><code>\"num_subsection_rows\": 100000\n</code></pre> <p>Tuning Guide: - Small datasets (&lt;1M rows): 50000-100000 - Medium datasets (1-10M rows): 100000-500000 - Large datasets (&gt;10M rows): 500000-1000000 - 64GB RAM systems: Max 1000000 (6 workers \u00d7 8GB each)</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#date_range","title":"<code>date_range</code>","text":"<p>Type: <code>object</code> (optional)</p> <p>Temporal filtering to restrict data to specific date ranges.</p> <pre><code>\"date_range\": {\n  \"start_day\": 1,\n  \"end_day\": 30,\n  \"start_month\": 4,\n  \"end_month\": 4,\n  \"start_year\": 2021,\n  \"end_year\": 2021\n}\n</code></pre> <p>All fields required if <code>date_range</code> is specified: - <code>start_day</code>, <code>end_day</code>: Day of month (1-31) - <code>start_month</code>, <code>end_month</code>: Month (1-12) - <code>start_year</code>, <code>end_year</code>: Year (2000-2099)</p> <p>Example Use Cases: - Single month: <code>start_month = end_month = 4</code> - Multi-month: <code>start_month = 4, end_month = 6</code> (April to June) - Single day: All start/end values identical</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#features","title":"<code>features</code>","text":"<p>Type: <code>object</code> (required)</p> <p>Description: Specifies which BSM columns to include in the feature set.</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#columns","title":"<code>columns</code>","text":"<p>Type: <code>string</code> (required)</p> <p>Predefined column set name. See Feature Column Sets for available options.</p> <pre><code>\"features\": {\n  \"columns\": \"minimal_xy_elev\"\n}\n</code></pre> <p>Available Values: - <code>\"minimal_xy_elev\"</code> - 5 columns (recommended for quick experiments) - <code>\"extended_timestamps\"</code> or <code>\"extended_with_timestamps\"</code> - 13 columns - <code>\"minimal_xy_elev_heading_speed\"</code> - 7 columns - <code>\"all\"</code> - ~50 columns (all BSM fields)</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#attacks","title":"<code>attacks</code>","text":"<p>Type: <code>object</code> (required)</p> <p>Description: Configures attack simulation parameters.</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#required-fields_1","title":"Required Fields","text":""},{"location":"DaskPipelineRunner_Configuration_Guide.html#enabled","title":"<code>enabled</code>","text":"<p>Type: <code>boolean</code> (required)</p> <p>Enable or disable attack simulation.</p> <pre><code>\"enabled\": true\n</code></pre> <p>When <code>enabled: false</code>: - No attack simulation is performed - All vehicles labeled as legitimate (<code>is_attacker = 0</code>) - Other attack fields are ignored - Use for baseline/control experiments</p> <p>When <code>enabled: true</code>: - Attack simulation is performed according to other attack parameters - Must specify <code>attack_ratio</code>, <code>type</code>, and type-specific parameters</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#optional-fields-required-if-enabled-true","title":"Optional Fields (required if <code>enabled: true</code>)","text":""},{"location":"DaskPipelineRunner_Configuration_Guide.html#attack_ratio","title":"<code>attack_ratio</code>","text":"<p>Type: <code>number</code> (required if enabled, range: 0.0-1.0)</p> <p>Fraction of vehicles to compromise.</p> <pre><code>\"attack_ratio\": 0.3\n</code></pre> <p>Examples: - <code>0.1</code> = 10% of vehicles are attackers - <code>0.3</code> = 30% of vehicles are attackers (typical) - <code>0.5</code> = 50% of vehicles are attackers</p> <p>Best Practices: - Use 0.1-0.3 for realistic scenarios - Use 0.5+ for stress testing classifiers - Always include <code>random_seed</code> for reproducibility</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#type","title":"<code>type</code>","text":"<p>Type: <code>string</code> (required if enabled)</p> <p>Attack type identifier. See Attack Types Reference for all types.</p> <pre><code>\"type\": \"rand_offset\"\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#random_seed","title":"<code>random_seed</code>","text":"<p>Type: <code>integer</code> (optional, recommended)</p> <p>Random seed for reproducible attack selection.</p> <pre><code>\"random_seed\": 42\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#attack-type-specific-parameters","title":"Attack Type-Specific Parameters","text":"<p>Each attack type requires additional parameters. See Attack Types Reference.</p> <p>Example: <code>rand_offset</code> Attack <pre><code>\"attacks\": {\n  \"enabled\": true,\n  \"attack_ratio\": 0.3,\n  \"type\": \"rand_offset\",\n  \"min_distance\": 10,\n  \"max_distance\": 20,\n  \"random_seed\": 42\n}\n</code></pre></p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#ml","title":"<code>ml</code>","text":"<p>Type: <code>object</code> (required)</p> <p>Description: Machine learning configuration including train/test split strategy.</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#train_test_split","title":"<code>train_test_split</code>","text":"<p>Type: <code>object</code> (required)</p> <p>Configures how to split data into training and test sets.</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#random-split-default","title":"Random Split (Default)","text":"<pre><code>\"ml\": {\n  \"train_test_split\": {\n    \"type\": \"random\",\n    \"train_ratio\": 0.8,\n    \"test_ratio\": 0.2,\n    \"random_seed\": 42\n  }\n}\n</code></pre> <p>Fields: - <code>type</code>: Must be <code>\"random\"</code> - <code>train_ratio</code>: Fraction for training (0.0-1.0) - <code>test_ratio</code>: Fraction for testing (0.0-1.0) - <code>random_seed</code>: Seed for reproducibility</p> <p>Constraints: - <code>train_ratio + test_ratio</code> must equal 1.0 - Common splits: 80/20, 70/30, 90/10</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#fixed-size-split","title":"Fixed-Size Split","text":"<pre><code>\"ml\": {\n  \"train_test_split\": {\n    \"type\": \"fixed_size\",\n    \"train_size\": 80000,\n    \"test_size\": 20000,\n    \"random_seed\": 42\n  }\n}\n</code></pre> <p>Fields: - <code>type</code>: Must be <code>\"fixed_size\"</code> - <code>train_size</code>: Exact number of rows for training - <code>test_size</code>: Exact number of rows for testing - <code>random_seed</code>: Seed for reproducibility</p> <p>Use Cases: - Consistent dataset sizes across experiments - Ensuring minimum test set size - Comparing models with identical data sizes</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#cache","title":"<code>cache</code>","text":"<p>Type: <code>object</code> (optional)</p> <p>Description: Controls Parquet caching for intermediate results.</p> <pre><code>\"cache\": {\n  \"enabled\": true\n}\n</code></pre> <p>Default Behavior (if omitted): Caching is disabled</p> <p>When Enabled: - Intermediate results cached to <code>cache/</code> directory - Uses Parquet format with LZ4 compression - Automatic cache key generation based on config - Typical cache hit rates: \u226585% after first run - Reduces re-computation time by 60-80%</p> <p>When to Enable: - Iterating on ML models with same data - Running multiple experiments with same base config - Working with large datasets (&gt;5M rows)</p> <p>When to Disable: - One-off experiments - Testing caching infrastructure itself - Disk space constraints - Ensuring fresh computation</p> <p>Cache Directory Structure: <pre><code>cache/\n\u251c\u2500\u2500 data_gatherer/\n\u2502   \u2514\u2500\u2500 {config_hash}_raw.parquet\n\u251c\u2500\u2500 large_cleaner/\n\u2502   \u2514\u2500\u2500 {config_hash}_cleaned.parquet\n\u251c\u2500\u2500 train_test_split/\n\u2502   \u251c\u2500\u2500 {config_hash}_train.parquet\n\u2502   \u2514\u2500\u2500 {config_hash}_test.parquet\n\u2514\u2500\u2500 attacks/\n    \u251c\u2500\u2500 {config_hash}_train_attacked.parquet\n    \u2514\u2500\u2500 {config_hash}_test_attacked.parquet\n</code></pre></p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#attack-types-reference","title":"Attack Types Reference","text":""},{"location":"DaskPipelineRunner_Configuration_Guide.html#1-rand_offset-random-position-offset","title":"1. <code>rand_offset</code> - Random Position Offset","text":"<p>Description: Each message from attacker vehicles has a random position offset applied. Different offset for each message.</p> <p>Use Case: Simulates GPS jamming or sensor noise attacks.</p> <p>Required Parameters: - <code>min_distance</code> (number): Minimum offset in meters - <code>max_distance</code> (number): Maximum offset in meters</p> <p>Example: <pre><code>\"attacks\": {\n  \"enabled\": true,\n  \"attack_ratio\": 0.3,\n  \"type\": \"rand_offset\",\n  \"min_distance\": 10,\n  \"max_distance\": 20,\n  \"random_seed\": 42\n}\n</code></pre></p> <p>Typical Values: - Light attack: <code>min: 10, max: 20</code> (10-20m offset) - Moderate attack: <code>min: 50, max: 100</code> (50-100m offset) - Severe attack: <code>min: 100, max: 200</code> (100-200m offset)</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#2-const_offset_per_id-constant-offset-per-vehicle","title":"2. <code>const_offset_per_id</code> - Constant Offset Per Vehicle","text":"<p>Description: Each attacker vehicle has a fixed offset applied to all messages. Different vehicles get different offsets.</p> <p>Use Case: Simulates miscalibrated sensors or systematic position bias.</p> <p>Required Parameters: - <code>min_distance</code> (number): Minimum offset in meters - <code>max_distance</code> (number): Maximum offset in meters</p> <p>Example: <pre><code>\"attacks\": {\n  \"enabled\": true,\n  \"attack_ratio\": 0.3,\n  \"type\": \"const_offset_per_id\",\n  \"min_distance\": 100,\n  \"max_distance\": 200,\n  \"random_seed\": 42\n}\n</code></pre></p> <p>Typical Values: - Moderate attack: <code>min: 50, max: 100</code> (50-100m offset) - Severe attack: <code>min: 100, max: 200</code> (100-200m offset) - Extreme attack: <code>min: 200, max: 500</code> (200-500m offset)</p> <p>Offset Selection: Each vehicle's offset is randomly chosen once from <code>[min_distance, max_distance]</code> range and applied consistently to all messages from that vehicle.</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#3-rand_position-random-position-within-area","title":"3. <code>rand_position</code> - Random Position Within Area","text":"<p>Description: Attacker positions are completely randomized within the filtered area, ignoring actual vehicle positions.</p> <p>Use Case: Simulates Sybil attacks or complete position spoofing.</p> <p>Required Parameters: - <code>min_distance</code> (number): Minimum radius from center (typically 0) - <code>max_distance</code> (number): Maximum radius from center (matches filtering distance)</p> <p>Example: <pre><code>\"attacks\": {\n  \"enabled\": true,\n  \"attack_ratio\": 0.3,\n  \"type\": \"rand_position\",\n  \"min_distance\": 0,\n  \"max_distance\": 2000,\n  \"random_seed\": 42\n}\n</code></pre></p> <p>Typical Configuration: - Set <code>max_distance</code> to match <code>data.filtering.distance_meters</code> - Results in uniformly distributed fake positions - Most aggressive attack type</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#4-position_swap-swap-positions-between-vehicles","title":"4. <code>position_swap</code> - Swap Positions Between Vehicles","text":"<p>Description: Attacker vehicles are paired up and swap positions with each other.</p> <p>Use Case: Simulates coordinated position exchange attacks.</p> <p>Required Parameters: None (only <code>enabled</code>, <code>attack_ratio</code>, <code>type</code>)</p> <p>Example: <pre><code>\"attacks\": {\n  \"enabled\": true,\n  \"attack_ratio\": 0.4,\n  \"type\": \"position_swap\",\n  \"random_seed\": 42\n}\n</code></pre></p> <p>Behavior: - Requires even number of attackers (pairs) - If odd number, one attacker remains unswapped - Positions swapped for all messages - Maintains realistic position values (from other real vehicles)</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#5-const_offset-fixed-offset-all-vehicles","title":"5. <code>const_offset</code> - Fixed Offset (All Vehicles)","text":"<p>Description: All attacker vehicles receive the same offset in the same direction.</p> <p>Use Case: Simulates coordinated attacks or regional GPS interference.</p> <p>Required Parameters: - <code>direction_angle</code> (number): Offset direction in degrees (0-360) - <code>distance_meters</code> (number): Offset distance in meters</p> <p>Example: <pre><code>\"attacks\": {\n  \"enabled\": true,\n  \"attack_ratio\": 0.3,\n  \"type\": \"const_offset\",\n  \"direction_angle\": 90,\n  \"distance_meters\": 100,\n  \"random_seed\": 42\n}\n</code></pre></p> <p>Direction Convention: - <code>0\u00b0</code> = North - <code>90\u00b0</code> = East - <code>180\u00b0</code> = South - <code>270\u00b0</code> = West</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#6-override_const-override-with-fixed-position","title":"6. <code>override_const</code> - Override with Fixed Position","text":"<p>Description: All attacker messages report the same fixed position (calculated from center + offset).</p> <p>Use Case: Simulates stationary fake vehicle broadcasts.</p> <p>Required Parameters: - <code>direction_angle</code> (number): Direction from center (0-360) - <code>distance_meters</code> (number): Distance from center</p> <p>Example: <pre><code>\"attacks\": {\n  \"enabled\": true,\n  \"attack_ratio\": 0.3,\n  \"type\": \"override_const\",\n  \"direction_angle\": 45,\n  \"distance_meters\": 500,\n  \"random_seed\": 42\n}\n</code></pre></p> <p>Behavior: - All attacker messages report identical position - Position calculated as: <code>center + (distance * direction)</code> - Easily detectable by most classifiers</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#7-override_rand-override-with-random-position","title":"7. <code>override_rand</code> - Override with Random Position","text":"<p>Description: Each attacker message reports a random position within specified range from center.</p> <p>Use Case: Simulates random fake position broadcasts.</p> <p>Required Parameters: - <code>min_distance</code> (number): Minimum distance from center - <code>max_distance</code> (number): Maximum distance from center</p> <p>Example: <pre><code>\"attacks\": {\n  \"enabled\": true,\n  \"attack_ratio\": 0.3,\n  \"type\": \"override_rand\",\n  \"min_distance\": 100,\n  \"max_distance\": 500,\n  \"random_seed\": 42\n}\n</code></pre></p> <p>Behavior: - New random position for each message - Positions uniformly distributed in annulus - Ignores actual vehicle trajectory</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#feature-column-sets","title":"Feature Column Sets","text":""},{"location":"DaskPipelineRunner_Configuration_Guide.html#minimal_xy_elev","title":"<code>minimal_xy_elev</code>","text":"<p>Column Count: 5</p> <p>Columns: - <code>latitude</code> - <code>longitude</code> - <code>elevation</code> - <code>speed</code> - <code>heading</code></p> <p>Use Case: - Quick experiments and prototyping - Minimal memory footprint - Baseline classifier performance - Development and debugging</p> <p>Example: <pre><code>\"features\": {\n  \"columns\": \"minimal_xy_elev\"\n}\n</code></pre></p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#extended_timestamps-extended_with_timestamps","title":"<code>extended_timestamps</code> / <code>extended_with_timestamps</code>","text":"<p>Column Count: 13</p> <p>Columns: - All columns from <code>minimal_xy_elev</code> (5) - <code>secMark</code> - Milliseconds within current minute - <code>elevation_change</code> - Rate of altitude change - Plus additional timestamp-derived features</p> <p>Use Case: - Temporal pattern analysis - Time-series classification - Attack detection based on timing - Production classifiers</p> <p>Example: <pre><code>\"features\": {\n  \"columns\": \"extended_timestamps\"\n}\n</code></pre></p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#minimal_xy_elev_heading_speed","title":"<code>minimal_xy_elev_heading_speed</code>","text":"<p>Column Count: 7</p> <p>Columns: - All columns from <code>minimal_xy_elev</code> (5) - Additional heading-derived features - Additional speed-derived features</p> <p>Use Case: - Movement pattern analysis - Velocity-based attack detection</p> <p>Example: <pre><code>\"features\": {\n  \"columns\": \"minimal_xy_elev_heading_speed\"\n}\n</code></pre></p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#all","title":"<code>all</code>","text":"<p>Column Count: ~50</p> <p>Columns: All available BSM data fields including: - Position (lat, lon, elev) - Motion (speed, heading, acceleration) - Timestamps (secMark, time-of-day, day-of-week) - Vehicle metadata - Message metadata</p> <p>Use Case: - Feature engineering exploration - Maximum classifier information - Research experiments</p> <p>Warning: - High memory usage (2-3x more than minimal) - Longer computation time - May cause overfitting with small datasets</p> <p>Example: <pre><code>\"features\": {\n  \"columns\": \"all\"\n}\n</code></pre></p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#filtering-types","title":"Filtering Types","text":""},{"location":"DaskPipelineRunner_Configuration_Guide.html#1-none-no-filtering-passthrough","title":"1. <code>none</code> - No Filtering (Passthrough)","text":"<p>Description: Load all data from source file without spatial filtering.</p> <p>Example: <pre><code>\"filtering\": {\n  \"type\": \"none\"\n}\n</code></pre></p> <p>Use Case: - Small datasets that fit in memory - No spatial constraints - Testing data loading pipeline</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#2-xy_offset_position-radial-distance-filter","title":"2. <code>xy_offset_position</code> - Radial Distance Filter","text":"<p>Description: Keep only records within specified distance from a center point.</p> <p>Required Parameters: - <code>distance_meters</code> (number): Radius in meters - <code>center_x</code> (number): Center longitude - <code>center_y</code> (number): Center latitude</p> <p>Example: <pre><code>\"filtering\": {\n  \"type\": \"xy_offset_position\",\n  \"distance_meters\": 2000,\n  \"center_x\": -106.0831353,\n  \"center_y\": 41.5430216\n}\n</code></pre></p> <p>Use Case: - Focus on specific intersection or area - Reduce dataset size - Geographic region analysis</p> <p>Distance Recommendations: - Small intersection: 500-1000m - Large intersection: 1000-2000m - Regional area: 2000-5000m</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#3-bounding_box-rectangular-area-filter","title":"3. <code>bounding_box</code> - Rectangular Area Filter","text":"<p>Description: Keep only records within specified lat/lon bounding box.</p> <p>Required Parameters: - <code>min_latitude</code> (number) - <code>max_latitude</code> (number) - <code>min_longitude</code> (number) - <code>max_longitude</code> (number)</p> <p>Example: <pre><code>\"filtering\": {\n  \"type\": \"bounding_box\",\n  \"min_latitude\": 41.0,\n  \"max_latitude\": 42.0,\n  \"min_longitude\": -107.0,\n  \"max_longitude\": -106.0\n}\n</code></pre></p> <p>Use Case: - Rectangular study areas - City/county boundaries - Grid-based analysis</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#complete-examples","title":"Complete Examples","text":""},{"location":"DaskPipelineRunner_Configuration_Guide.html#example-1-basic-experiment-no-attacks","title":"Example 1: Basic Experiment (No Attacks)","text":"<pre><code>{\n  \"pipeline_name\": \"baseline_no_attacks\",\n  \"data\": {\n    \"source_file\": \"data/bsm_data.csv\",\n    \"filtering\": {\n      \"type\": \"xy_offset_position\",\n      \"distance_meters\": 1000,\n      \"center_x\": -106.0831353,\n      \"center_y\": 41.5430216\n    }\n  },\n  \"features\": {\n    \"columns\": \"minimal_xy_elev\"\n  },\n  \"attacks\": {\n    \"enabled\": false\n  },\n  \"ml\": {\n    \"train_test_split\": {\n      \"type\": \"random\",\n      \"train_ratio\": 0.8,\n      \"test_ratio\": 0.2,\n      \"random_seed\": 42\n    }\n  }\n}\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#example-2-random-offset-attack-30-attackers","title":"Example 2: Random Offset Attack (30% Attackers)","text":"<pre><code>{\n  \"pipeline_name\": \"rand_offset_10_20m_30pct_attackers\",\n  \"data\": {\n    \"source_file\": \"data/bsm_data.csv\",\n    \"num_subsection_rows\": 100000,\n    \"filtering\": {\n      \"type\": \"xy_offset_position\",\n      \"distance_meters\": 2000,\n      \"center_x\": -106.0831353,\n      \"center_y\": 41.5430216\n    },\n    \"date_range\": {\n      \"start_day\": 1,\n      \"end_day\": 30,\n      \"start_month\": 4,\n      \"end_month\": 4,\n      \"start_year\": 2021,\n      \"end_year\": 2021\n    }\n  },\n  \"features\": {\n    \"columns\": \"extended_timestamps\"\n  },\n  \"attacks\": {\n    \"enabled\": true,\n    \"attack_ratio\": 0.3,\n    \"type\": \"rand_offset\",\n    \"min_distance\": 10,\n    \"max_distance\": 20,\n    \"random_seed\": 42\n  },\n  \"ml\": {\n    \"train_test_split\": {\n      \"type\": \"random\",\n      \"train_ratio\": 0.8,\n      \"test_ratio\": 0.2,\n      \"random_seed\": 42\n    }\n  },\n  \"cache\": {\n    \"enabled\": true\n  }\n}\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#example-3-constant-offset-per-vehicle-severe-attack","title":"Example 3: Constant Offset Per Vehicle (Severe Attack)","text":"<pre><code>{\n  \"pipeline_name\": \"const_offset_per_id_100_200m_40pct\",\n  \"data\": {\n    \"source_file\": \"data/bsm_data.csv\",\n    \"num_subsection_rows\": 500000,\n    \"filtering\": {\n      \"type\": \"xy_offset_position\",\n      \"distance_meters\": 2000,\n      \"center_x\": -106.0831353,\n      \"center_y\": 41.5430216\n    }\n  },\n  \"features\": {\n    \"columns\": \"all\"\n  },\n  \"attacks\": {\n    \"enabled\": true,\n    \"attack_ratio\": 0.4,\n    \"type\": \"const_offset_per_id\",\n    \"min_distance\": 100,\n    \"max_distance\": 200,\n    \"random_seed\": 42\n  },\n  \"ml\": {\n    \"train_test_split\": {\n      \"type\": \"random\",\n      \"train_ratio\": 0.8,\n      \"test_ratio\": 0.2,\n      \"random_seed\": 42\n    }\n  },\n  \"cache\": {\n    \"enabled\": true\n  }\n}\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#example-4-fixed-size-split-with-position-swap","title":"Example 4: Fixed-Size Split with Position Swap","text":"<pre><code>{\n  \"pipeline_name\": \"position_swap_fixed_size_80k_20k\",\n  \"data\": {\n    \"source_file\": \"data/bsm_data.csv\",\n    \"filtering\": {\n      \"type\": \"none\"\n    }\n  },\n  \"features\": {\n    \"columns\": \"minimal_xy_elev_heading_speed\"\n  },\n  \"attacks\": {\n    \"enabled\": true,\n    \"attack_ratio\": 0.2,\n    \"type\": \"position_swap\",\n    \"random_seed\": 42\n  },\n  \"ml\": {\n    \"train_test_split\": {\n      \"type\": \"fixed_size\",\n      \"train_size\": 80000,\n      \"test_size\": 20000,\n      \"random_seed\": 42\n    }\n  },\n  \"cache\": {\n    \"enabled\": true\n  }\n}\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#example-5-multiple-date-ranges-april-2021","title":"Example 5: Multiple Date Ranges (April 2021)","text":"<pre><code>{\n  \"pipeline_name\": \"april_2021_rand_position_attack\",\n  \"data\": {\n    \"source_file\": \"data/bsm_data.csv\",\n    \"filtering\": {\n      \"type\": \"xy_offset_position\",\n      \"distance_meters\": 2000,\n      \"center_x\": -106.0831353,\n      \"center_y\": 41.5430216\n    },\n    \"date_range\": {\n      \"start_day\": 1,\n      \"end_day\": 30,\n      \"start_month\": 4,\n      \"end_month\": 4,\n      \"start_year\": 2021,\n      \"end_year\": 2021\n    }\n  },\n  \"features\": {\n    \"columns\": \"extended_timestamps\"\n  },\n  \"attacks\": {\n    \"enabled\": true,\n    \"attack_ratio\": 0.3,\n    \"type\": \"rand_position\",\n    \"min_distance\": 0,\n    \"max_distance\": 2000,\n    \"random_seed\": 42\n  },\n  \"ml\": {\n    \"train_test_split\": {\n      \"type\": \"random\",\n      \"train_ratio\": 0.8,\n      \"test_ratio\": 0.2,\n      \"random_seed\": 42\n    }\n  },\n  \"cache\": {\n    \"enabled\": true\n  }\n}\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#configuration-validation","title":"Configuration Validation","text":""},{"location":"DaskPipelineRunner_Configuration_Guide.html#using-the-validation-script","title":"Using the Validation Script","text":"<p>The codebase includes a validation script to check configuration files before running pipelines:</p> <pre><code>python scripts/validate_pipeline_configs.py MClassifierPipelines/configs/my_config.json\n</code></pre> <p>What It Checks: - JSON syntax validity - Required fields present - Field types correct - Value ranges valid - Attack parameters match attack type - File paths exist</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#common-validation-errors","title":"Common Validation Errors","text":""},{"location":"DaskPipelineRunner_Configuration_Guide.html#missing-required-field","title":"Missing Required Field","text":"<p><pre><code>Error: Missing required field 'pipeline_name'\n</code></pre> Fix: Add the missing field to your config.</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#invalid-attack-ratio","title":"Invalid Attack Ratio","text":"<p><pre><code>Error: attack_ratio must be between 0.0 and 1.0, got 1.5\n</code></pre> Fix: Set <code>attack_ratio</code> to a value in range [0.0, 1.0].</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#traintest-ratios-dont-sum-to-10","title":"Train/Test Ratios Don't Sum to 1.0","text":"<p><pre><code>Error: train_ratio (0.7) + test_ratio (0.2) must equal 1.0\n</code></pre> Fix: Adjust ratios to sum to 1.0: <code>\"train_ratio\": 0.8, \"test_ratio\": 0.2</code></p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#missing-attack-parameters","title":"Missing Attack Parameters","text":"<p><pre><code>Error: rand_offset attack requires min_distance and max_distance\n</code></pre> Fix: Add required parameters for the attack type.</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#best-practices","title":"Best Practices","text":""},{"location":"DaskPipelineRunner_Configuration_Guide.html#1-always-use-random_seed","title":"1. Always Use <code>random_seed</code>","text":"<p>Why: Ensures reproducible results across runs.</p> <pre><code>\"attacks\": {\n  \"random_seed\": 42\n},\n\"ml\": {\n  \"train_test_split\": {\n    \"random_seed\": 42\n  }\n}\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#2-enable-caching-for-iterative-work","title":"2. Enable Caching for Iterative Work","text":"<p>Why: Reduces re-computation time by 60-80% when running multiple experiments with the same base data.</p> <pre><code>\"cache\": {\n  \"enabled\": true\n}\n</code></pre> <p>When to Disable: - One-off experiments - Testing caching system itself - Debugging data loading issues</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#3-use-descriptive-pipeline_name","title":"3. Use Descriptive <code>pipeline_name</code>","text":"<p>Good: <pre><code>\"pipeline_name\": \"xy2000m_rand_offset_10_20m_30pct_minimal_cols\"\n</code></pre></p> <p>Bad: <pre><code>\"pipeline_name\": \"test1\"\n</code></pre></p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#4-match-attack-distance-to-filtering-distance","title":"4. Match Attack Distance to Filtering Distance","text":"<p>For <code>rand_position</code> attacks, set <code>max_distance</code> to match the filtering radius:</p> <pre><code>\"data\": {\n  \"filtering\": {\n    \"distance_meters\": 2000\n  }\n},\n\"attacks\": {\n  \"type\": \"rand_position\",\n  \"max_distance\": 2000  // Match filtering distance\n}\n</code></pre>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#5-start-with-minimal_xy_elev-columns","title":"5. Start with <code>minimal_xy_elev</code> Columns","text":"<p>Why: Faster computation, easier debugging, lower memory usage.</p> <p>Once the pipeline works, scale up to <code>extended_timestamps</code> or <code>all</code>.</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#6-tune-num_subsection_rows-for-your-system","title":"6. Tune <code>num_subsection_rows</code> for Your System","text":"<p>64GB RAM System: - Safe: 100000-500000 rows/partition - Aggressive: 500000-1000000 rows/partition</p> <p>32GB RAM System: - Safe: 50000-100000 rows/partition</p> <p>Monitor: Check Dask dashboard at <code>http://localhost:8787</code> for memory usage.</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#7-use-fixed-size-splits-for-comparisons","title":"7. Use Fixed-Size Splits for Comparisons","text":"<p>When comparing different models or attacks:</p> <pre><code>\"ml\": {\n  \"train_test_split\": {\n    \"type\": \"fixed_size\",\n    \"train_size\": 80000,\n    \"test_size\": 20000,\n    \"random_seed\": 42\n  }\n}\n</code></pre> <p>Ensures identical dataset sizes across all experiments.</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#8-validate-before-running","title":"8. Validate Before Running","text":"<pre><code>python scripts/validate_pipeline_configs.py config.json\n</code></pre> <p>Catches errors before starting expensive computations.</p>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Main README: /README.md - Installation, quick start, architecture</li> <li>Example Configs: /MClassifierPipelines/configs/ - 55+ real-world examples</li> <li>API Documentation: /docs/API/ - Class and method references</li> <li>Performance Guide: /docs/performance/ - Benchmarks and optimization</li> <li>Testing Guide: /Test/README.md - Running tests</li> </ul>"},{"location":"DaskPipelineRunner_Configuration_Guide.html#support","title":"Support","text":"<p>If you encounter issues:</p> <ol> <li>Check the Troubleshooting section in README</li> <li>Validate your config: <code>python scripts/validate_pipeline_configs.py config.json</code></li> <li>Check Dask dashboard: <code>http://localhost:8787</code></li> <li>Review logs in <code>logs/</code> directory</li> <li>Open an issue on GitHub with:</li> <li>Your config file</li> <li>Error message</li> <li>System specs (RAM, CPU, OS)</li> </ol> <p>Last Updated: 2026-01-18 Version: 1.0 Dask Migration: Complete (51/58 tasks, Phase 8 in progress)</p>"},{"location":"Troubleshooting_Guide.html","title":"Dask Pipeline Troubleshooting Guide","text":"<p>This comprehensive guide covers common issues, error messages, and solutions for the Dask-based Connected Driving data pipeline.</p>"},{"location":"Troubleshooting_Guide.html#table-of-contents","title":"Table of Contents","text":"<ol> <li>Common Error Messages</li> <li>Configuration Issues</li> <li>Memory Management</li> <li>Cache Problems</li> <li>Performance Issues</li> <li>Integration Issues</li> <li>Validation Scripts</li> <li>Debugging Tools</li> <li>Recovery Procedures</li> <li>Quick Reference</li> </ol>"},{"location":"Troubleshooting_Guide.html#1-common-error-messages","title":"1. Common Error Messages","text":""},{"location":"Troubleshooting_Guide.html#type-validation-errors","title":"Type Validation Errors","text":""},{"location":"Troubleshooting_Guide.html#typeerror-expected-dask-dataframe-got-type","title":"<code>TypeError: Expected Dask DataFrame, got &lt;type&gt;</code>","text":"<p>Location: DaskConnectedDrivingAttacker, DaskConnectedDrivingCleaner initialization</p> <p>Cause: Passing non-Dask DataFrame to attacker or cleaner classes</p> <p>Solution: <pre><code># Incorrect\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\")\nattacker = DaskConnectedDrivingAttacker(df)  # Error!\n\n# Correct\nfrom Gatherer.DaskDataGatherer import DaskDataGatherer\ngatherer = DaskDataGatherer(...)\ndf = gatherer.gather_data()  # Returns Dask DataFrame\nattacker = DaskConnectedDrivingAttacker(df)  # Works!\n</code></pre></p> <p>Prevention: Always use <code>isinstance(data, dd.DataFrame)</code> checks before passing data</p>"},{"location":"Troubleshooting_Guide.html#configuration-errors","title":"Configuration Errors","text":""},{"location":"Troubleshooting_Guide.html#valueerror-no-data-specified","title":"<code>ValueError: No data specified</code>","text":"<p>Location: DaskConnectedDrivingCleaner initialization</p> <p>Cause: Missing data parameter when <code>shouldGatherAutomatically=False</code></p> <p>Solution: <pre><code># Option 1: Pass data explicitly\ncleaner = DaskConnectedDrivingCleaner(data=dask_df, ...)\n\n# Option 2: Enable automatic gathering\nConnectedDrivingCleaner.shouldGatherAutomatically = True\ncleaner = DaskConnectedDrivingCleaner(...)  # Will gather automatically\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#keyerror-function-not-registered","title":"<code>KeyError: Function not registered</code>","text":"<p>Location: DaskUDFRegistry.get_function()</p> <p>Cause: Trying to access UDF not in registry</p> <p>Solution: <pre><code>from Helpers.DaskUDFRegistry import DaskUDFRegistry\n\n# Register UDF before use\n@DaskUDFRegistry.register(\"my_function\")\ndef my_function(x):\n    return x * 2\n\n# Now it's available\nfunc = DaskUDFRegistry.get_function(\"my_function\")\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#data-gathering-errors","title":"Data Gathering Errors","text":""},{"location":"Troubleshooting_Guide.html#valueerror-no-data-gathered-yet","title":"<code>ValueError: No data gathered yet</code>","text":"<p>Location: DaskDataGatherer.compute_data() and persist_data()</p> <p>Cause: Calling compute/persist before gather_data()</p> <p>Solution: <pre><code># Incorrect\ngatherer = DaskDataGatherer(...)\ndf = gatherer.compute_data()  # Error!\n\n# Correct\ngatherer = DaskDataGatherer(...)\ndf = gatherer.gather_data()  # Must call this first\ncomputed_df = gatherer.compute_data()  # Now works\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#filenotfounderror-no-such-file-or-directory","title":"<code>FileNotFoundError: No such file or directory</code>","text":"<p>Location: DaskDataGatherer when loading CSV</p> <p>Cause: Invalid or missing source_file path in config</p> <p>Solution: <pre><code># Verify file exists\nls -lh /path/to/data.csv\n\n# Check config has correct path\ncat config.json | grep source_file\n\n# Use absolute paths in config\n{\n  \"data\": {\n    \"source_file\": \"/absolute/path/to/data.csv\"\n  }\n}\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#2-configuration-issues","title":"2. Configuration Issues","text":""},{"location":"Troubleshooting_Guide.html#required-configuration-fields","title":"Required Configuration Fields","text":"<p>All pipeline configs MUST include these sections:</p> <pre><code>{\n  \"pipeline_name\": \"unique_identifier\",\n  \"data\": {\n    \"source_file\": \"path/to/data.csv\",\n    \"filtering\": { \"type\": \"xy_offset_position\" }\n  },\n  \"features\": {\n    \"column_set\": \"minimal_xy_elev\"\n  },\n  \"attacks\": {\n    \"type\": \"none\",\n    \"attack_ratio\": 0.0\n  },\n  \"ml\": {\n    \"split_method\": \"ratio\",\n    \"train_ratio\": 0.8\n  },\n  \"cache\": {\n    \"enabled\": true\n  }\n}\n</code></pre>"},{"location":"Troubleshooting_Guide.html#common-config-issues","title":"Common Config Issues","text":"Issue Root Cause Solution Invalid JSON Syntax errors Run <code>python -m json.tool config.json</code> to validate Missing filtering.type Defaults applied Explicitly set to: <code>none</code>, <code>xy_offset_position</code>, or <code>bounding_box</code> Invalid date format Wrong format Use \"YYYY-MM-DD\" format in date_range Missing source_file CSV path not provided Add <code>\"source_file\": \"path/to/data.csv\"</code> Invalid attack.type Unsupported type Use: <code>none</code>, <code>rand_offset</code>, <code>const_offset</code>, <code>const_offset_per_id</code>, <code>swap_rand</code>, <code>override_const</code>, <code>override_rand</code> attack_ratio out of range Value not in [0.0, 1.0) Set between 0.0 and 0.99"},{"location":"Troubleshooting_Guide.html#validate-configuration","title":"Validate Configuration","text":"<pre><code># Run validation script\npython scripts/validate_pipeline_configs.py --verbose\n\n# Validate specific config\npython -m json.tool config.json\n\n# Check config in Python\nfrom MachineLearning.DaskPipelineRunner import DaskPipelineRunner\nrunner = DaskPipelineRunner.from_config(\"config.json\")  # Will raise if invalid\n</code></pre>"},{"location":"Troubleshooting_Guide.html#3-memory-management","title":"3. Memory Management","text":""},{"location":"Troubleshooting_Guide.html#peak-memory-usage-profile","title":"Peak Memory Usage Profile","text":"<ul> <li>Baseline: 48GB (Dask cluster: 6 workers \u00d7 8GB each)</li> <li>Data gathering: 15M rows at 64MB blocksize = ~67 partitions</li> <li>Attack operations: ~3x data size (compute + copy + result)</li> <li>Target: Keep peak &lt;52GB to avoid worker crashes</li> </ul>"},{"location":"Troubleshooting_Guide.html#memory-configuration","title":"Memory Configuration","text":"<p>DaskDataGatherer blocksize tuning: <pre><code># Default: 64MB (optimized in Task 49)\ngatherer = DaskDataGatherer(\n    source_file=\"data.csv\",\n    blocksize=\"64MB\"  # Produces ~225K rows/partition\n)\n\n# For limited memory systems\ngatherer = DaskDataGatherer(\n    source_file=\"data.csv\",\n    blocksize=\"32MB\"  # Smaller partitions, more memory-friendly\n)\n</code></pre></p> <p>Worker memory limits: <pre><code>from Helpers.DaskSessionManager import DaskSessionManager\n\n# Default: 8GB per worker (for 64GB system)\nclient = DaskSessionManager.get_client()\n\n# For smaller systems\nclient = DaskSessionManager.get_cluster(memory_limit='4GB')\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#memory-monitoring","title":"Memory Monitoring","text":"<pre><code>from Gatherer.DaskDataGatherer import DaskDataGatherer\n\ngatherer = DaskDataGatherer(...)\ngatherer.gather_data()\n\n# Log memory usage\ngatherer.log_memory_usage()\n# Output: \"Cluster memory: Worker 0: 6.2GB / 8GB, Worker 1: 5.8GB / 8GB, ...\"\n</code></pre>"},{"location":"Troubleshooting_Guide.html#common-oom-errors","title":"Common OOM Errors","text":""},{"location":"Troubleshooting_Guide.html#1-compute-on-15m-rows-without-caution","title":"1. Compute on 15M rows without caution","text":"<p>Symptom: Workers repeatedly spill to disk, performance degrades</p> <p>Solution: <pre><code># Bad: Computes entire 15M row dataset\ndf_computed = df.compute()\n\n# Good: Use .head() for testing\ndf_sample = df.head(1000)\n\n# Good: Keep in Dask format and use lazy evaluation\nresult = df.groupby(\"column\").mean()  # Not computed yet\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#2-large-position-swap-operations","title":"2. Large position swap operations","text":"<p>Symptom: Memory usage spikes to &gt;52GB, worker crashes</p> <p>Solution: <pre><code># Memory peaks at ~3x data size during swap\n# Ensure 52GB+ available before running position_swap attack\n\n# Monitor during operation\ngatherer.log_memory_usage()\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#3-per-partition-memory-exceeded","title":"3. Per-partition memory exceeded","text":"<p>Symptom: \"Worker exceeded memory limit\" error</p> <p>Solution: <pre><code># Reduce blocksize to create smaller partitions\ngatherer = DaskDataGatherer(\n    source_file=\"data.csv\",\n    blocksize=\"32MB\"  # Down from default 64MB\n)\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#4-cache-problems","title":"4. Cache Problems","text":""},{"location":"Troubleshooting_Guide.html#target-85-cache-hit-rate","title":"Target: \u226585% Cache Hit Rate","text":"<p>Check cache health: <pre><code>python scripts/monitor_cache_health.py\n\n# Output:\n# Cache Statistics:\n#   Total entries: 47\n#   Hit rate: 87.3%  \u2713 (target: \u226585%)\n#   Total size: 23.4 GB\n#   Status: EXCELLENT\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#low-hit-rate-70","title":"Low Hit Rate (&lt;70%)","text":"<p>Symptom: Cache misses exceed 30%</p> <p>Causes: 1. Non-deterministic cache key generation 2. Cache variables don't include all relevant parameters 3. Configuration changes invalidate old entries</p> <p>Solution: <pre><code># 1. Run health check\npython scripts/monitor_cache_health.py\n\n# 2. Check hit rate\n# - \u226585%: Excellent (goal achieved)\n# - 70-85%: Acceptable but optimize\n# - &lt;70%: Investigate cache key generation\n\n# 3. Run cleanup to remove stale entries\npython scripts/monitor_cache_health.py --cleanup\n\n# 4. Exit codes for automation\n# 0 = success (\u226585%)\n# 1 = error (&lt;70%)\n# 2 = warning (70-85%)\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#cache-key-issues","title":"Cache Key Issues","text":"<p>Non-deterministic keys: Cache keys must be deterministic for reproducibility</p> <pre><code># Bad: Uses timestamp (non-deterministic)\n@FileCache(cache_variables=[\"timestamp\"])\ndef process_data(timestamp):\n    return data\n\n# Good: Uses all relevant config parameters\n@FileCache(cache_variables=[\"source_file\", \"random_seed\", \"attack_type\"])\ndef process_data(source_file, random_seed, attack_type):\n    return data\n</code></pre>"},{"location":"Troubleshooting_Guide.html#parquet-cache-errors","title":"Parquet Cache Errors","text":""},{"location":"Troubleshooting_Guide.html#pyarrowlibarrowexception-when-reading-cache","title":"<code>pyarrow.lib.ArrowException</code> when reading cache","text":"<p>Cause: Corrupted or incomplete parquet write</p> <p>Solution: <pre><code># Delete corrupted cache file\nrm -rf cache/path/to/entry.parquet\n\n# Re-run pipeline to regenerate\npython scripts/run_pipeline.py config.json\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#filenotfounderror-for-cache-directory","title":"<code>FileNotFoundError</code> for cache directory","text":"<p>Cause: Cache directory doesn't exist or was deleted</p> <p>Solution: <pre><code># Ensure cache directories exist\nimport os\nos.makedirs(\"cache\", exist_ok=True)\n\n# Or set in config\n{\n  \"cache\": {\n    \"enabled\": true,\n    \"cache_dir\": \"cache\"  # Will be created if missing\n  }\n}\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#cache-manager-operations","title":"Cache Manager Operations","text":"<pre><code>from Decorators.CacheManager import CacheManager\n\n# Reset singleton (for testing)\nCacheManager.reset_instance()\n\n# Get statistics\nmanager = CacheManager.get_instance()\nstats = manager.get_statistics()\nprint(f\"Hit rate: {stats['hit_rate_percent']:.1f}%\")\n\n# Cleanup old entries (LRU policy)\nmanager.cleanup_cache(max_size_gb=100)\n</code></pre>"},{"location":"Troubleshooting_Guide.html#5-performance-issues","title":"5. Performance Issues","text":""},{"location":"Troubleshooting_Guide.html#slow-operations-common-bottlenecks","title":"Slow Operations - Common Bottlenecks","text":"Operation Typical Time (15M rows) Troubleshooting CSV\u2192Dask read 30-60s Use blocksize=64MB; ensure adequate memory Position attacks 2-5min Use compute-then-daskify strategy (automatic) ML training 5-15min Use sample data for testing; increase to full for final Parquet write 1-3min Use snappy compression; enable overwrite"},{"location":"Troubleshooting_Guide.html#performance-targets","title":"Performance Targets","text":"<ul> <li>Baseline: Original pandas/Spark implementation</li> <li>Target: Dask implementation \u22652x faster (achieved in Task 48)</li> <li>Key: Distributed processing + lazy evaluation</li> </ul>"},{"location":"Troubleshooting_Guide.html#optimization-checklist","title":"Optimization Checklist","text":"<p>\u2713 Partition count: Match worker count (6 workers = 6+ partitions) <pre><code># Check partition count\ndf = gatherer.gather_data()\nprint(f\"Partitions: {df.npartitions}\")  # Should be 6+\n</code></pre></p> <p>\u2713 Blocksize: Use 64MB default; adjust based on row size <pre><code>gatherer = DaskDataGatherer(blocksize=\"64MB\")  # Default\n</code></pre></p> <p>\u2713 Caching: Ensure &gt;85% cache hit rate <pre><code>python scripts/monitor_cache_health.py\n</code></pre></p> <p>\u2713 Persistence: Use <code>.persist()</code> for data accessed multiple times <pre><code># If accessing data multiple times\ndf = gatherer.gather_data()\ndf = df.persist()  # Keep in worker memory\n</code></pre></p> <p>\u2713 Compute: Only call <code>.compute()</code> when absolutely needed <pre><code># Bad: Computes unnecessarily\ndf_computed = df.compute()\nresult = df_computed.mean()\n\n# Good: Lazy evaluation\nresult = df.mean().compute()  # Only computes final result\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#slow-pipeline-diagnosis","title":"Slow Pipeline Diagnosis","text":"<p>Symptom: Pipeline takes &gt;10 minutes for 15M rows</p> <p>Diagnosis steps: <pre><code># 1. Check Dask dashboard for bottlenecks\n# Open dashboard URL from pipeline logs\n\n# 2. Enable profiling\npython scripts/profile_pipeline.py config.json\n\n# 3. Common causes:\n# - Too many small partitions \u2192 increase blocksize\n# - Disk I/O bottleneck \u2192 use faster SSD\n# - Insufficient workers \u2192 increase n_workers (if RAM allows)\n# - Cache misses \u2192 check cache hit rate\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#6-integration-issues","title":"6. Integration Issues","text":""},{"location":"Troubleshooting_Guide.html#sklearn-integration","title":"sklearn Integration","text":"<p>Issue: Dask DataFrame not compatible with sklearn</p> <p>Solution: Call <code>.compute()</code> BEFORE passing to sklearn</p> <pre><code># Incorrect\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier()\nclassifier.fit(train_X, train_Y)  # Error if Dask DataFrames!\n\n# Correct\nclassifier = RandomForestClassifier()\nX_train = train_X.compute()  # Convert to pandas\nY_train = train_Y.compute()\nclassifier.fit(X_train, Y_train)  # Works!\n</code></pre>"},{"location":"Troubleshooting_Guide.html#pandasdask-interoperability","title":"pandas/Dask Interoperability","text":"<p>Converting between formats: <pre><code>import pandas as pd\nimport dask.dataframe as dd\n\n# pandas \u2192 Dask\npandas_df = pd.read_csv(\"data.csv\")\ndask_df = dd.from_pandas(pandas_df, npartitions=6)\n\n# Dask \u2192 pandas\ndask_df = dd.read_csv(\"data.csv\", blocksize=\"64MB\")\npandas_df = dask_df.compute()\n</code></pre></p> <p>Common mistake: Using pandas methods on Dask DataFrame <pre><code># Bad: .map() doesn't exist in Dask\ndf['new_col'] = df['old_col'].map(lambda x: x * 2)\n\n# Good: Use .map_partitions()\ndf['new_col'] = df['old_col'].map_partitions(lambda x: x * 2)\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#supported-sklearn-classifiers","title":"Supported sklearn Classifiers","text":"<p>The pipeline supports these classifiers: - <code>RandomForestClassifier</code> - <code>DecisionTreeClassifier</code> - <code>KNeighborsClassifier</code></p> <p>Metrics available: accuracy, precision, recall, f1, specificity</p> <p>Note: All classifiers require computed (pandas) data for training</p>"},{"location":"Troubleshooting_Guide.html#7-validation-scripts","title":"7. Validation Scripts","text":""},{"location":"Troubleshooting_Guide.html#available-scripts","title":"Available Scripts","text":"Script Purpose Usage validate_pipeline_configs.py Validates JSON configs <code>python scripts/validate_pipeline_configs.py --verbose</code> monitor_cache_health.py Cache hit rate &amp; size <code>python scripts/monitor_cache_health.py --report</code> validate_dask_data_loading.py Tests data loading <code>python scripts/validate_dask_data_loading.py</code> validate_profiling_script.py Validates profiling <code>python scripts/validate_profiling_script.py</code>"},{"location":"Troubleshooting_Guide.html#health-check-routine","title":"Health Check Routine","text":"<pre><code># 1. Validate pipeline configuration\npython scripts/validate_pipeline_configs.py --fail-fast\n\n# 2. Monitor cache health\npython scripts/monitor_cache_health.py\n# Exit codes: 0=success (\u226585%), 1=error (&lt;70%), 2=warning (70-85%)\n\n# 3. Run data loading validation\npython scripts/validate_dask_data_loading.py\n\n# 4. Run full test suite\npytest Test/test_dask*.py -v\n</code></pre>"},{"location":"Troubleshooting_Guide.html#programmatic-health-check","title":"Programmatic Health Check","text":"<pre><code>from MachineLearning.DaskPipelineRunner import DaskPipelineRunner\n\n# Check pipeline initialization\nrunner = DaskPipelineRunner.from_config(\"config.json\")\nrunner.logger.log(\"Pipeline initialized successfully\")\n\n# Run pipeline with validation\nresult = runner.run()\nif result is not None:\n    print(\"Pipeline completed successfully\")\n</code></pre>"},{"location":"Troubleshooting_Guide.html#8-debugging-tools","title":"8. Debugging Tools","text":""},{"location":"Troubleshooting_Guide.html#logging","title":"Logging","text":"<p>Enable logging: <pre><code>from Logger.Logger import Logger\n\nlogger = Logger(\"MyComponent\")\nlogger.log(\"Info message\")\nlogger.log(f\"Debug: variable={variable}\")\n</code></pre></p> <p>Important log messages to watch for: - <code>DaskPipelineRunner.run()</code> - Pipeline start - <code>Step X: ...</code> - Each pipeline stage - <code>Dask client created</code> - Client initialization - <code>Cache HIT/MISS</code> - Cache operations - <code>Cluster memory</code> - Memory monitoring</p>"},{"location":"Troubleshooting_Guide.html#dask-dashboard","title":"Dask Dashboard","text":"<p>The Dask dashboard provides real-time visualization of: - Task graph and execution - Worker status and resource usage - Memory usage per worker - Task progress and bottlenecks</p> <p>Access dashboard: <pre><code>from Helpers.DaskSessionManager import DaskSessionManager\n\nclient = DaskSessionManager.get_client()\nprint(f\"Dashboard: {client.dashboard_link}\")\n# Open URL in browser: http://localhost:8787\n</code></pre></p>"},{"location":"Troubleshooting_Guide.html#memory-monitoring_1","title":"Memory Monitoring","text":"<pre><code>from Helpers.DaskSessionManager import DaskSessionManager\n\n# Get memory usage per worker\nmemory_usage = DaskSessionManager.get_memory_usage()\nprint(memory_usage)\n# Output: {'worker-0': '6.2GB / 8GB', 'worker-1': '5.8GB / 8GB', ...}\n</code></pre>"},{"location":"Troubleshooting_Guide.html#9-recovery-procedures","title":"9. Recovery Procedures","text":""},{"location":"Troubleshooting_Guide.html#pipeline-fails-during-data-gathering","title":"Pipeline Fails During Data Gathering","text":"<pre><code># 1. Check error message for file path issues\ncat logs/pipeline.log | grep \"FileNotFoundError\"\n\n# 2. Verify source file exists and is readable\nls -lh /path/to/data.csv\n\n# 3. Run validation\npython scripts/validate_dask_data_loading.py\n\n# 4. Check memory\npython -c \"from Gatherer.DaskDataGatherer import DaskDataGatherer; \\\n           g = DaskDataGatherer('data.csv'); \\\n           g.log_memory_usage()\"\n\n# 5. If OOM: Reduce blocksize or increase workers\n# Edit config: blocksize=\"32MB\"\n</code></pre>"},{"location":"Troubleshooting_Guide.html#pipeline-fails-during-ml-training","title":"Pipeline Fails During ML Training","text":"<pre><code># 1. Verify data has correct columns\npython -c \"import dask.dataframe as dd; \\\n           df = dd.read_csv('data.csv'); \\\n           print(df.columns.tolist())\"\n\n# 2. Check for NaN values\npython -c \"import dask.dataframe as dd; \\\n           df = dd.read_csv('data.csv'); \\\n           print(df.isna().sum().compute())\"\n\n# 3. Try with smaller dataset first\n# Edit config: \"numrows\": 10000\n\n# 4. Verify feature columns match config\ncat config.json | grep -A 3 \"features\"\n</code></pre>"},{"location":"Troubleshooting_Guide.html#cache-corruption-recovery","title":"Cache Corruption Recovery","text":"<pre><code># 1. Delete cache directory\nrm -rf cache/\n\n# 2. Clear metadata\nrm cache_metadata.json\n\n# 3. Re-run pipeline (will regenerate cache)\npython scripts/run_pipeline.py config.json\n\n# 4. Verify cache health\npython scripts/monitor_cache_health.py\n</code></pre>"},{"location":"Troubleshooting_Guide.html#memory-pressure-recovery","title":"Memory Pressure Recovery","text":"<pre><code>from Gatherer.DaskDataGatherer import DaskDataGatherer\nfrom Helpers.DaskSessionManager import DaskSessionManager\n\n# 1. Monitor memory\ngatherer = DaskDataGatherer(...)\ngatherer.log_memory_usage()\n\n# 2. If &gt;80% used: Reduce blocksize or numrows\ngatherer = DaskDataGatherer(source_file=\"data.csv\", blocksize=\"32MB\")\n\n# 3. If &gt;90% used: Restart Dask cluster\nDaskSessionManager.restart()\n\n# 4. Verify: Check dashboard for task distribution\nclient = DaskSessionManager.get_client()\nprint(client.dashboard_link)\n</code></pre>"},{"location":"Troubleshooting_Guide.html#10-quick-reference","title":"10. Quick Reference","text":""},{"location":"Troubleshooting_Guide.html#file-locations","title":"File Locations","text":"<pre><code>/tmp/original-repo/\n\u251c\u2500\u2500 MachineLearning/\n\u2502   \u251c\u2500\u2500 DaskPipelineRunner.py          # Main pipeline\n\u2502   \u2514\u2500\u2500 DaskMClassifierPipeline.py     # ML classifier wrapper\n\u251c\u2500\u2500 Gatherer/\n\u2502   \u2514\u2500\u2500 DaskDataGatherer.py            # Data loading\n\u251c\u2500\u2500 Generator/Attackers/\n\u2502   \u2514\u2500\u2500 DaskConnectedDrivingAttacker.py # Attack simulations\n\u251c\u2500\u2500 Cleaners/\n\u2502   \u251c\u2500\u2500 DaskCleanerWithPassthroughFilter.py\n\u2502   \u251c\u2500\u2500 DaskCleanerWithFilterWithinRange.py\n\u2502   \u2514\u2500\u2500 DaskCleanerWithFilterWithinRangeXY.py\n\u251c\u2500\u2500 Decorators/\n\u2502   \u251c\u2500\u2500 CacheManager.py                # Cache statistics\n\u2502   \u251c\u2500\u2500 FileCache.py                   # File caching\n\u2502   \u2514\u2500\u2500 DaskParquetCache.py            # Parquet caching\n\u251c\u2500\u2500 Helpers/\n\u2502   \u251c\u2500\u2500 DaskSessionManager.py          # Dask cluster management\n\u2502   \u2514\u2500\u2500 DaskUDFRegistry.py             # UDF registration\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 validate_pipeline_configs.py   # Config validation\n\u2502   \u251c\u2500\u2500 monitor_cache_health.py        # Cache monitoring\n\u2502   \u2514\u2500\u2500 validate_dask_data_loading.py  # Data loading tests\n\u251c\u2500\u2500 Test/\n\u2502   \u251c\u2500\u2500 test_dask_cleaners.py\n\u2502   \u251c\u2500\u2500 test_dask_attackers.py\n\u2502   \u2514\u2500\u2500 test_dask_pipeline_runner.py\n\u2514\u2500\u2500 MClassifierPipelines/configs/      # 55+ example configs\n</code></pre>"},{"location":"Troubleshooting_Guide.html#key-metrics-and-thresholds","title":"Key Metrics and Thresholds","text":"Metric Target Warning Critical Cache hit rate \u226585% 70-84% &lt;70% Memory usage &lt;48GB 48-52GB &gt;52GB Pipeline runtime 5-20min 20-40min &gt;40min Partition count 6+ 4-5 &lt;4 Cache size &lt;50GB 50-100GB &gt;100GB"},{"location":"Troubleshooting_Guide.html#common-commands","title":"Common Commands","text":"<pre><code># Validate configuration\npython scripts/validate_pipeline_configs.py --verbose\n\n# Check cache health\npython scripts/monitor_cache_health.py\n\n# Run pipeline\npython scripts/run_pipeline.py config.json\n\n# Run tests\npytest Test/test_dask*.py -v\n\n# Monitor memory\npython -c \"from Helpers.DaskSessionManager import DaskSessionManager; \\\n           print(DaskSessionManager.get_memory_usage())\"\n\n# Clean cache\npython scripts/monitor_cache_health.py --cleanup\n</code></pre>"},{"location":"Troubleshooting_Guide.html#emergency-contacts","title":"Emergency Contacts","text":"<ul> <li>GitHub Issues: Report bugs and issues at the repository</li> <li>Documentation: See <code>README.md</code> and <code>docs/DaskPipelineRunner_Configuration_Guide.md</code></li> <li>Examples: Check <code>MClassifierPipelines/configs/</code> for 55+ working examples</li> </ul>"},{"location":"Troubleshooting_Guide.html#summary","title":"Summary","text":"<p>This troubleshooting guide covers the most common issues encountered when using the Dask-based Connected Driving pipeline. For additional help:</p> <ol> <li>Check the README.md for quick start and basic usage</li> <li>Review the Configuration Guide for detailed config docs</li> <li>Examine example configs in <code>MClassifierPipelines/configs/</code></li> <li>Run validation scripts in <code>scripts/</code></li> <li>Check test files in <code>Test/</code> for usage examples</li> </ol> <p>If you encounter an issue not covered here, please check the logs and Dask dashboard first, then consult the documentation or file an issue.</p>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html","title":"WYDOT Data Infrastructure Plan","text":""},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#comprehensive-architecture-for-flexible-cv-pilot-data-access","title":"Comprehensive Architecture for Flexible CV Pilot Data Access","text":"<p>Version: 1.1 Date: 2026-01-28 Status: Reviewed &amp; Audited  </p>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#executive-summary","title":"Executive Summary","text":"<p>This document outlines a comprehensive infrastructure for accessing Wyoming Connected Vehicle (CV) Pilot data from the USDOT ITS Data Sandbox. The architecture supports:</p> <ul> <li>Flexible date-range queries - Fetch any historical data on demand</li> <li>Multiple data sources - WYDOT, THEA, NYCDOT (extensible)</li> <li>Configurable pipelines - YAML-based configuration for experiments</li> <li>Memory-efficient processing - Optimized for 32GB-128GB systems</li> <li>Robust error handling - Resume-capable downloads with integrity checks</li> </ul> <p>Related Documents: - Cache Key Specification - Detailed cache design</p>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#table-of-contents","title":"Table of Contents","text":"<ol> <li>Data Source Overview</li> <li>Architecture Design</li> <li>Component Specifications</li> <li>Configuration System</li> <li>Implementation Plan</li> <li>Dependencies &amp; Prerequisites</li> <li>Error Handling &amp; Recovery</li> <li>Concurrency &amp; Safety</li> <li>Testing Strategy</li> <li>Appendices</li> </ol>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#1-data-source-overview","title":"1. Data Source Overview","text":""},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#11-primary-data-source-aws-s3-bucket","title":"1.1 Primary Data Source: AWS S3 Bucket","text":"Property Value Bucket Name <code>usdot-its-cvpilot-publicdata</code> Region <code>us-east-1</code> Access Public (no auth required for read) Web Interface http://usdot-its-cvpilot-publicdata.s3.amazonaws.com/index.html"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#12-folder-hierarchy","title":"1.2 Folder Hierarchy","text":"<pre><code>{Source_Name}/{Data_Type}/{Year}/{Month}/{Day}/{Hour}/\n</code></pre> Component Values Description Source_Name <code>wydot</code>, <code>wydot_backup</code>, <code>thea</code>, <code>nycdot</code> Data producer Data_Type <code>BSM</code>, <code>TIM</code>, <code>SPAT</code>, <code>EVENT</code> Message type Year <code>2017</code>-<code>2026</code> 4-digit year (UTC) Month <code>01</code>-<code>12</code> 2-digit month (UTC) Day <code>01</code>-<code>31</code> 2-digit day (UTC) Hour <code>00</code>-<code>23</code> 2-digit hour (UTC) <p>\u26a0\ufe0f CRITICAL: All dates in S3 are in UTC. See Time Zone Handling.</p>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#13-data-format","title":"1.3 Data Format","text":"<ul> <li>Pre-2018-01-18: One JSON message per file</li> <li>Post-2018-01-18: Newline-delimited JSON (NDJSON) with multiple messages per file</li> </ul>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#14-available-message-types-by-source","title":"1.4 Available Message Types by Source","text":"Source BSM TIM SPAT EVENT wydot \u2705 \u2705 \u274c \u274c wydot_backup \u2705 \u2705 \u274c \u274c thea \u2705 \u2705 \u2705 \u274c nycdot \u274c \u274c \u274c \u2705"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#15-known-data-gaps","title":"1.5 Known Data Gaps","text":"<p>Some dates may have no data due to: - System maintenance - Pilot downtime - Network issues</p> <p>See: ITS CV Pilot Known Data Gaps</p>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#2-architecture-design","title":"2. Architecture Design","text":""},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#21-high-level-architecture","title":"2.1 High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Configuration Layer                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 data_source  \u2502  \u2502 date_range   \u2502  \u2502 processing_config    \u2502  \u2502\n\u2502  \u2502 .yml         \u2502  \u2502 .yml         \u2502  \u2502 .yml                 \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                    \u2193 Pydantic Validation                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Data Fetching Layer                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                   S3DataFetcher                           \u2502  \u2502\n\u2502  \u2502  - List objects by date range                             \u2502  \u2502\n\u2502  \u2502  - Parallel download with resume                          \u2502  \u2502\n\u2502  \u2502  - Rate limiting &amp; backoff                                \u2502  \u2502\n\u2502  \u2502  - Integrity verification                                 \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                              \u2502                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                    Local Cache                            \u2502  \u2502\n\u2502  \u2502  - Hierarchical: {source}/{type}/{year}/{month}/{day}    \u2502  \u2502\n\u2502  \u2502  - Manifest with file locking                             \u2502  \u2502\n\u2502  \u2502  - LRU eviction when disk full                            \u2502  \u2502\n\u2502  \u2502  - Atomic writes (temp file + rename)                     \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Data Processing Layer                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502   JSON     \u2502  \u2502  Schema    \u2502  \u2502   DataFrame            \u2502    \u2502\n\u2502  \u2502   Parser   \u2502\u2500\u2500\u25b6  Validator \u2502\u2500\u2500\u25b6  Converter (Dask/Spark)\u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                              \u2502                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Parquet Cache (Processed)                    \u2502  \u2502\n\u2502  \u2502  - Consistent schema (merged columns)                     \u2502  \u2502\n\u2502  \u2502  - Partitioned by date                                    \u2502  \u2502\n\u2502  \u2502  - Lazy loading for large ranges                          \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Existing Pipeline Integration                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502         ConnectedDrivingPipelineV4                      \u2502    \u2502\n\u2502  \u2502  - DaskDataGatherer / SparkDataGatherer                 \u2502    \u2502\n\u2502  \u2502  - ML Classifier Pipelines                              \u2502    \u2502\n\u2502  \u2502  - Attack Simulation                                    \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#22-data-flow","title":"2.2 Data Flow","text":"<pre><code>User Request (config.yml)\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Validate Config \u2502 \u2500\u2500\u2500 Pydantic validation, fail fast on errors\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Normalize Dates \u2502 \u2500\u2500\u2500 Convert to UTC if local time specified\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Check Cache     \u2502 \u2500\u2500\u2500 Acquire read lock, check manifest\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u251c\u2500\u2500\u2500 Cache Hit \u2500\u2500\u2500\u25b6 Verify integrity \u2500\u2500\u2500\u25b6 Load Parquet (lazy)\n    \u2502\n    \u251c\u2500\u2500\u2500 Partial Hit \u2500\u2500\u25b6 Fetch missing dates only\n    \u2502\n    \u2514\u2500\u2500\u2500 Cache Miss \u2500\u2500\u25b6\n                       \u2502\n                       \u25bc\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502 List S3 Objects \u2502 \u2500\u2500\u2500 Handle empty results gracefully\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u25bc\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502 Download Files  \u2502 \u2500\u2500\u2500 Parallel, with rate limiting\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u25bc\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502 Parse &amp; Validate\u2502 \u2500\u2500\u2500 Streaming, memory-bounded\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u25bc\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502 Write Parquet   \u2502 \u2500\u2500\u2500 Atomic: temp file \u2192 rename\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u25bc\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502 Update Manifest \u2502 \u2500\u2500\u2500 Acquire write lock\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u25bc\n               Return DataFrame (lazy for Dask)\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#3-component-specifications","title":"3. Component Specifications","text":""},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#31-s3datafetcher-class","title":"3.1 S3DataFetcher Class","text":"<p>Location: <code>DataSources/S3DataFetcher.py</code></p> <pre><code>class S3DataFetcher:\n    \"\"\"\n    Fetches CV Pilot data from USDOT ITS Sandbox S3 bucket.\n\n    Features:\n    - Date-range based queries\n    - Parallel downloads with configurable concurrency\n    - Resume support for interrupted downloads\n    - Rate limiting with exponential backoff\n    - Integrity verification via ETag/MD5\n    \"\"\"\n\n    def __init__(self, config: DataSourceConfig):\n        self.bucket = config.bucket\n        self.source = config.source\n        self.message_type = config.message_type\n        self.cache_dir = config.cache_dir\n        self.max_workers = config.max_workers\n        self._rate_limiter = RateLimiter(max_requests_per_second=10)\n\n    def list_files(self, start_date: date, end_date: date) -&gt; List[S3Object]:\n        \"\"\"\n        List all files in date range.\n\n        Returns empty list if no data exists (not an error).\n        \"\"\"\n\n    def download_files(\n        self, \n        files: List[S3Object], \n        progress_callback: Callable = None\n    ) -&gt; List[Path]:\n        \"\"\"\n        Download files with parallel execution and resume support.\n\n        - Uses atomic writes (temp file + rename)\n        - Respects rate limits\n        - Retries with exponential backoff\n        \"\"\"\n\n    def get_data(\n        self, \n        start_date: date, \n        end_date: date,\n        force_refresh: bool = False  # Bypass cache\n    ) -&gt; dd.DataFrame:  # Returns Dask DataFrame for lazy loading\n        \"\"\"Main entry point: fetch, parse, and return data.\"\"\"\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#32-cachemanager-class","title":"3.2 CacheManager Class","text":"<p>Location: <code>DataSources/CacheManager.py</code></p> <pre><code>class CacheManager:\n    \"\"\"\n    Manages local cache of downloaded and processed data.\n\n    Features:\n    - File locking for concurrent access\n    - Atomic manifest updates\n    - LRU eviction when disk full\n    - Tracks \"no data\" vs \"not fetched\"\n\n    Cache Structure:\n    cache/\n    \u251c\u2500\u2500 manifest.json           # Tracking metadata (with lock)\n    \u251c\u2500\u2500 wydot/\n    \u2502   \u2514\u2500\u2500 BSM/\n    \u2502       \u2514\u2500\u2500 2021/04/01.parquet\n    \u2514\u2500\u2500 .locks/                 # Lock files for concurrent access\n    \"\"\"\n\n    def __init__(self, cache_dir: Path, max_size_gb: float = 50.0):\n        self.cache_dir = cache_dir\n        self.max_size_gb = max_size_gb\n        self.lock_dir = cache_dir / \".locks\"\n        self._manifest_lock = FileLock(self.lock_dir / \"manifest.lock\")\n\n    def get_cached_dates(self, source: str, msg_type: str) -&gt; Set[date]:\n        \"\"\"Return set of dates already in cache (including 'no_data' entries).\"\"\"\n\n    def get_missing_dates(self, source: str, msg_type: str,\n                          start: date, end: date) -&gt; List[date]:\n        \"\"\"Return dates not yet cached.\"\"\"\n\n    def save_processed(\n        self, \n        df: pd.DataFrame, \n        source: str, \n        msg_type: str, \n        date: date\n    ) -&gt; Path:\n        \"\"\"\n        Save processed DataFrame to Parquet cache.\n\n        Uses atomic write: temp file \u2192 rename.\n        \"\"\"\n\n    def mark_no_data(self, source: str, msg_type: str, date: date):\n        \"\"\"\n        Mark a date as having no data in S3.\n\n        Prevents re-fetching empty dates.\n        \"\"\"\n\n    def evict_lru(self, bytes_needed: int):\n        \"\"\"Evict least-recently-used entries to free space.\"\"\"\n\n    def clear_cache(self, source: str = None, msg_type: str = None):\n        \"\"\"Clear cache entries (all, or filtered by source/type).\"\"\"\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#33-schemavalidator-class","title":"3.3 SchemaValidator Class","text":"<p>Location: <code>DataSources/SchemaValidator.py</code></p> <pre><code>class BSMSchemaValidator:\n    \"\"\"\n    Validates BSM records against J2735 schema.\n    Handles schema evolution across different time periods.\n    \"\"\"\n\n    REQUIRED_FIELDS = [\n        'metadata.recordGeneratedAt',\n        'payload.data.coreData.position.latitude',\n        'payload.data.coreData.position.longitude',\n        'payload.data.coreData.elevation',\n        'payload.data.coreData.speed',\n        'payload.data.coreData.heading',\n    ]\n\n    # Schema versions by date range\n    SCHEMA_VERSIONS = {\n        (date(2017, 1, 1), date(2018, 1, 17)): 2,\n        (date(2018, 1, 18), date(2020, 6, 30)): 3,\n        (date(2020, 7, 1), date(2099, 12, 31)): 6,\n    }\n\n    def validate(self, record: dict) -&gt; Tuple[bool, List[str]]:\n        \"\"\"Validate single record, return (valid, errors).\"\"\"\n\n    def batch_validate(self, records: List[dict]) -&gt; ValidationReport:\n        \"\"\"Validate batch with statistics.\"\"\"\n\n    def get_canonical_schema(self) -&gt; pa.Schema:\n        \"\"\"\n        Return the canonical PyArrow schema.\n\n        Used to ensure all parquet files have consistent columns.\n        \"\"\"\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#34-datasourceconfig-dataclass","title":"3.4 DataSourceConfig Dataclass","text":"<p>Location: <code>DataSources/config.py</code></p> <pre><code>from pydantic import BaseModel, validator, Field\nfrom datetime import date\nfrom pathlib import Path\nfrom typing import Optional, Literal\n\nclass DateRangeConfig(BaseModel):\n    \"\"\"Date range configuration with validation.\"\"\"\n\n    start_date: Optional[date] = None\n    end_date: Optional[date] = None\n    days_back: Optional[int] = None  # Relative to today\n    timezone: str = \"UTC\"  # Input timezone\n\n    @validator('end_date')\n    def end_after_start(cls, v, values):\n        if v and values.get('start_date') and v &lt; values['start_date']:\n            raise ValueError('end_date must be &gt;= start_date')\n        return v\n\n    @validator('days_back')\n    def days_back_positive(cls, v):\n        if v is not None and v &lt;= 0:\n            raise ValueError('days_back must be positive')\n        return v\n\nclass DataSourceConfig(BaseModel):\n    \"\"\"Configuration for data source access with full validation.\"\"\"\n\n    # Source identification\n    bucket: str = \"usdot-its-cvpilot-publicdata\"\n    source: Literal[\"wydot\", \"wydot_backup\", \"thea\", \"nycdot\"]\n    message_type: str\n\n    # Date range\n    date_range: DateRangeConfig\n\n    # Cache settings\n    cache_dir: Path = Path(\"data/cache\")\n    cache_ttl_days: int = Field(default=30, ge=1)\n    cache_max_size_gb: float = Field(default=50.0, ge=1.0)\n\n    # Download settings\n    max_workers: int = Field(default=4, ge=1, le=16)\n    retry_attempts: int = Field(default=3, ge=1)\n    retry_delay: float = Field(default=1.0, ge=0.1)\n    timeout_seconds: int = Field(default=300, ge=30)\n\n    # Memory settings\n    max_memory_gb: float = Field(default=24.0, ge=4.0)\n    partition_size_mb: int = Field(default=128, ge=16)\n\n    # Processing settings\n    validate_schema: bool = True\n    drop_invalid: bool = True\n\n    @validator('message_type')\n    def valid_message_type_for_source(cls, v, values):\n        source = values.get('source')\n        valid_types = {\n            'wydot': {'BSM', 'TIM'},\n            'wydot_backup': {'BSM', 'TIM'},\n            'thea': {'BSM', 'TIM', 'SPAT'},\n            'nycdot': {'EVENT'},\n        }\n        if source and v not in valid_types.get(source, set()):\n            raise ValueError(\n                f\"Invalid message_type '{v}' for source '{source}'. \"\n                f\"Valid: {valid_types[source]}\"\n            )\n        return v\n\n    @classmethod\n    def from_yaml(cls, path: Path) -&gt; 'DataSourceConfig':\n        \"\"\"Load and validate configuration from YAML file.\"\"\"\n        import yaml\n\n        with open(path) as f:\n            raw = yaml.safe_load(f)\n\n        # Pydantic validates on construction\n        return cls(**raw)\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#4-configuration-system","title":"4. Configuration System","text":""},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#41-data-source-configuration","title":"4.1 Data Source Configuration","text":"<p>File: <code>configs/data_sources/wydot_bsm.yml</code></p> <pre><code># WYDOT BSM Data Source Configuration\n# All fields are validated by Pydantic on load\n\nsource: \"wydot\"\nmessage_type: \"BSM\"\nbucket: \"usdot-its-cvpilot-publicdata\"\n\ndate_range:\n  # Option 1: Relative dates (for testing)\n  days_back: 30\n  timezone: \"America/Denver\"  # Converted to UTC internally\n\n  # Option 2: Absolute dates (for production)\n  # start_date: \"2021-04-01\"\n  # end_date: \"2021-04-30\"\n  # timezone: \"UTC\"\n\ncache:\n  directory: \"data/cache\"\n  ttl_days: 30\n  max_size_gb: 50\n\ndownload:\n  max_workers: 4\n  retry_attempts: 3\n  retry_delay_seconds: 1.0\n  timeout_seconds: 300\n\nmemory:\n  max_usage_gb: 24\n  partition_size_mb: 128\n\nvalidation:\n  enabled: true\n  drop_invalid_records: true\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#42-experiment-configuration","title":"4.2 Experiment Configuration","text":"<p>File: <code>configs/experiments/test_32gb_april_2021.yml</code></p> <pre><code># Experiment: Test pipeline with April 2021 WYDOT data on 32GB system\n\nexperiment:\n  name: \"test_32gb_april_2021\"\n  description: \"Validate 32GB config with one month of WYDOT BSM data\"\n\ndata:\n  source: \"wydot\"\n  message_type: \"BSM\"\n  date_range:\n    start_date: \"2021-04-01\"\n    end_date: \"2021-04-30\"\n    timezone: \"UTC\"\n\n  # Limit for testing (optional)\n  max_rows: 1000000  # 1M rows for quick test\n\nprocessing:\n  engine: \"dask\"\n  config: \"configs/dask/32gb-production.yml\"\n\n  cleaning:\n    enabled: true\n    remove_duplicates: true\n    validate_coordinates: true\n    coordinate_bounds:\n      lat_min: 40.0\n      lat_max: 45.0\n      lon_min: -112.0\n      lon_max: -104.0\n\noutput:\n  results_dir: \"results/test_32gb_april_2021\"\n  save_model: true\n  save_predictions: true\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#5-implementation-plan","title":"5. Implementation Plan","text":""},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#51-phase-1-core-infrastructure-week-1","title":"5.1 Phase 1: Core Infrastructure (Week 1)","text":"Task Priority Effort Dependencies Create <code>DataSources/</code> module structure P0 2h None Implement Pydantic config models P0 4h pydantic Implement <code>S3DataFetcher</code> with rate limiting P0 8h boto3 Implement <code>CacheManager</code> with file locking P0 8h filelock Implement atomic file writes P0 2h None Write unit tests for core classes P0 6h pytest, moto"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#52-phase-2-data-processing-week-2","title":"5.2 Phase 2: Data Processing (Week 2)","text":"Task Priority Effort Dependencies Implement streaming JSON parser P0 6h Phase 1 Implement <code>SchemaValidator</code> with versioning P0 6h Phase 1 Implement schema merging for Parquet P0 4h pyarrow Create Dask lazy loading integration P0 8h dask Create Spark integration (optional) P1 6h pyspark"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#53-phase-3-pipeline-integration-week-3","title":"5.3 Phase 3: Pipeline Integration (Week 3)","text":"Task Priority Effort Dependencies Create adapter for <code>DataGatherer</code> interface P0 4h Phase 2 Update existing gatherers P0 6h Phase 2 Handle time zone conversion P0 4h pytz Integration testing P0 8h All above"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#54-phase-4-cli-polish-week-4","title":"5.4 Phase 4: CLI &amp; Polish (Week 4)","text":"Task Priority Effort Dependencies Create CLI for data fetching P1 6h click, rich Create CLI for cache management P1 4h Phase 3 Add progress bars and logging P0 4h rich Write user documentation P0 6h All above Performance benchmarking P1 4h Phase 3"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#6-dependencies-prerequisites","title":"6. Dependencies &amp; Prerequisites","text":""},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#61-python-dependencies","title":"6.1 Python Dependencies","text":"<pre><code># AWS Access\nboto3&gt;=1.26.0\nbotocore&gt;=1.29.0\n\n# Data Processing\npandas&gt;=1.5.0\npyarrow&gt;=11.0.0\ndask[complete]&gt;=2024.1.0\n\n# Configuration &amp; Validation\npyyaml&gt;=6.0\npydantic&gt;=2.0.0\n\n# Concurrency &amp; Safety\nfilelock&gt;=3.12.0\n\n# Time Zones\npytz&gt;=2023.3\n\n# CLI\nclick&gt;=8.0.0\nrich&gt;=13.0.0\n\n# Testing\npytest&gt;=7.0.0\npytest-asyncio&gt;=0.21.0\nmoto&gt;=4.0.0\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#62-system-requirements","title":"6.2 System Requirements","text":"Requirement Minimum Recommended RAM 8 GB 32+ GB Storage 50 GB SSD 200+ GB SSD Network 10 Mbps 100+ Mbps Python 3.10+ 3.11+"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#7-error-handling-recovery","title":"7. Error Handling &amp; Recovery","text":""},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#71-network-errors","title":"7.1 Network Errors","text":"Error Type Handling Strategy Connection timeout Retry with exponential backoff (1s, 2s, 4s, max 3 attempts) Rate limiting (429/503) Pause based on Retry-After header, reduce concurrency S3 access denied Log error, fail explicitly (don't silently skip) Incomplete download Track bytes downloaded, resume from last position DNS failure Retry after 30s, max 3 attempts"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#72-data-errors","title":"7.2 Data Errors","text":"Error Type Handling Strategy Malformed JSON Log record to invalid.jsonl, skip, continue Missing required field Apply null/default if safe, else skip record Out-of-range values Include with <code>_warning</code> flag column Unexpected schema Log warning, use flexible parsing Empty S3 prefix Mark date as <code>no_data</code> in manifest (not an error)"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#73-disk-errors","title":"7.3 Disk Errors","text":"Error Type Handling Strategy Disk full during write Evict LRU cache entries, retry Permission denied Fail explicitly with clear error message Corrupted file Detect via checksum, remove and re-fetch"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#8-concurrency-safety","title":"8. Concurrency &amp; Safety","text":""},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#81-file-locking-strategy","title":"8.1 File Locking Strategy","text":"<pre><code>from filelock import FileLock\n\nclass CacheManager:\n    def __init__(self, cache_dir: Path):\n        self.lock_dir = cache_dir / \".locks\"\n        self.lock_dir.mkdir(exist_ok=True)\n\n    def _get_lock(self, key: str) -&gt; FileLock:\n        \"\"\"Get a lock for a specific cache key.\"\"\"\n        lock_file = self.lock_dir / f\"{key.replace('/', '_')}.lock\"\n        return FileLock(lock_file, timeout=60)\n\n    def save_processed(self, df, source, msg_type, date_val):\n        key = f\"{source}/{msg_type}/{date_val.year}/{date_val.month:02d}/{date_val.day:02d}\"\n\n        with self._get_lock(key):\n            # Atomic write: temp file \u2192 rename\n            temp_path = self.cache_dir / f\".tmp_{uuid.uuid4()}.parquet\"\n            final_path = self.cache_dir / f\"{key}.parquet\"\n\n            try:\n                df.to_parquet(temp_path, compression='snappy')\n                temp_path.rename(final_path)  # Atomic on POSIX\n            finally:\n                temp_path.unlink(missing_ok=True)\n\n            # Update manifest atomically\n            self._update_manifest(key, final_path)\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#82-manifest-updates","title":"8.2 Manifest Updates","text":"<pre><code>def _update_manifest(self, key: str, path: Path):\n    \"\"\"Update manifest with file locking.\"\"\"\n    manifest_lock = FileLock(self.lock_dir / \"manifest.lock\", timeout=60)\n\n    with manifest_lock:\n        manifest = self._load_manifest()\n        manifest['entries'][key] = {\n            'status': 'complete',\n            'file_path': str(path.relative_to(self.cache_dir)),\n            'checksum_sha256': compute_sha256(path),\n            'updated_at': datetime.utcnow().isoformat(),\n            # ... other metadata\n        }\n        self._save_manifest(manifest)\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#83-rate-limiting","title":"8.3 Rate Limiting","text":"<pre><code>import time\nfrom threading import Lock\n\nclass RateLimiter:\n    \"\"\"Token bucket rate limiter for S3 requests.\"\"\"\n\n    def __init__(self, max_requests_per_second: float = 10.0):\n        self.rate = max_requests_per_second\n        self.tokens = max_requests_per_second\n        self.last_update = time.monotonic()\n        self.lock = Lock()\n\n    def acquire(self):\n        with self.lock:\n            now = time.monotonic()\n            elapsed = now - self.last_update\n            self.tokens = min(self.rate, self.tokens + elapsed * self.rate)\n            self.last_update = now\n\n            if self.tokens &lt; 1:\n                sleep_time = (1 - self.tokens) / self.rate\n                time.sleep(sleep_time)\n                self.tokens = 0\n            else:\n                self.tokens -= 1\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#84-time-zone-handling","title":"8.4 Time Zone Handling","text":"<pre><code>import pytz\nfrom datetime import date, datetime\n\ndef normalize_to_utc(dt: date, timezone: str) -&gt; date:\n    \"\"\"\n    Convert a date from local timezone to UTC.\n\n    S3 data is stored in UTC, so all queries must use UTC dates.\n    \"\"\"\n    if timezone == \"UTC\":\n        return dt\n\n    local_tz = pytz.timezone(timezone)\n    # Treat date as start of day in local timezone\n    local_dt = local_tz.localize(datetime.combine(dt, datetime.min.time()))\n    utc_dt = local_dt.astimezone(pytz.UTC)\n    return utc_dt.date()\n\n# In config loading:\nconfig.start_date = normalize_to_utc(config.start_date, config.timezone)\nconfig.end_date = normalize_to_utc(config.end_date, config.timezone)\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#9-testing-strategy","title":"9. Testing Strategy","text":""},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#91-unit-tests","title":"9.1 Unit Tests","text":"<pre><code>class TestConfigValidation:\n    def test_invalid_source_rejected(self):\n        with pytest.raises(ValueError, match=\"Invalid source\"):\n            DataSourceConfig(source=\"invalid\", message_type=\"BSM\", ...)\n\n    def test_invalid_message_type_for_source(self):\n        with pytest.raises(ValueError, match=\"Invalid message_type\"):\n            DataSourceConfig(source=\"nycdot\", message_type=\"BSM\", ...)\n\n    def test_end_date_before_start_rejected(self):\n        with pytest.raises(ValueError, match=\"end_date must be &gt;= start_date\"):\n            DateRangeConfig(start_date=date(2021, 4, 30), end_date=date(2021, 4, 1))\n\nclass TestCacheManager:\n    def test_concurrent_writes_safe(self):\n        \"\"\"Test that concurrent writes don't corrupt cache.\"\"\"\n\n    def test_atomic_write_on_failure(self):\n        \"\"\"Test that failed writes don't leave partial files.\"\"\"\n\n    def test_no_data_marked_correctly(self):\n        \"\"\"Test that empty S3 prefixes are cached as 'no_data'.\"\"\"\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#92-integration-tests","title":"9.2 Integration Tests","text":"<pre><code>@pytest.mark.integration\nclass TestS3Integration:\n    def test_fetch_one_hour_real_data(self):\n        \"\"\"Test fetching real WYDOT data.\"\"\"\n\n    def test_rate_limiting_respected(self):\n        \"\"\"Test that rate limits are honored.\"\"\"\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#10-cli-commands","title":"10. CLI Commands","text":"<pre><code># Fetch data\npython -m datasources fetch \\\n  --source wydot \\\n  --type BSM \\\n  --start 2021-04-01 \\\n  --end 2021-04-07 \\\n  --timezone America/Denver\n\n# Show cache status\npython -m datasources cache status\n\n# Clear cache\npython -m datasources cache clear --source wydot\n\n# Force refresh (bypass cache)\npython -m datasources fetch \\\n  --source wydot \\\n  --type BSM \\\n  --start 2021-04-01 \\\n  --end 2021-04-01 \\\n  --force-refresh\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#appendices","title":"Appendices","text":""},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#a-s3-bucket-contents-summary","title":"A. S3 Bucket Contents Summary","text":"Source Start Date Data Types Est. Total Size wydot 2017-11 BSM, TIM ~500 GB wydot_backup 2018-01 BSM, TIM ~300 GB thea 2018-06 BSM, TIM, SPAT ~200 GB nycdot 2020-01 EVENT ~50 GB"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#b-related-documents","title":"B. Related Documents","text":"<ul> <li>Cache Key Specification</li> <li>32GB Config</li> <li>Dask 32GB Config</li> </ul>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#c-useful-aws-cli-commands","title":"C. Useful AWS CLI Commands","text":"<pre><code># List all sources\naws s3 ls s3://usdot-its-cvpilot-publicdata/ --no-sign-request\n\n# Check if date has data\naws s3 ls s3://usdot-its-cvpilot-publicdata/wydot/BSM/2021/04/01/ --no-sign-request\n\n# Download one day\naws s3 cp s3://usdot-its-cvpilot-publicdata/wydot/BSM/2021/04/01/ \\\n  ./data/wydot/BSM/2021/04/01/ --recursive --no-sign-request\n</code></pre>"},{"location":"WYDOT_DATA_INFRASTRUCTURE_PLAN.html#document-history","title":"Document History","text":"Version Date Author Changes 1.0 2026-01-28 Sophie (AI) Initial draft 1.1 2026-01-28 Sophie (AI) Audit: Added concurrency, time zones, disk management, validation <p>Audit Checklist: - [x] Concurrency handling (file locking) - [x] Disk space management (LRU eviction) - [x] Time zone handling (UTC normalization) - [x] Rate limiting (token bucket) - [x] Atomic file writes (temp + rename) - [x] Empty data handling (no_data status) - [x] Config validation (Pydantic) - [x] Schema consistency (canonical schema) - [x] Error propagation (fail fast) - [x] Cache invalidation (force_refresh, clear)</p>"},{"location":"about.html","title":"Author","text":"<p>This pipeline was created  me, Aaron Collins. You can learn more about me at https://aaroncollins.info.</p>"},{"location":"ci_cd_fixes.html","title":"CI/CD Fixes Documentation","text":"<p>This document details all fixes applied to resolve CI/CD test suite failures in ConnectedDrivingPipelineV4.</p>"},{"location":"ci_cd_fixes.html#overview","title":"Overview","text":"<p>The CI/CD pipeline was failing with 25 out of 27 tests failing. Through systematic analysis and fixes, all test failures were resolved. This document provides a complete record of all changes with before/after code examples and validation commands.</p>"},{"location":"ci_cd_fixes.html#summary-of-fixes","title":"Summary of Fixes","text":"Task Issue Impact Status 1.1 Python Version Requirements CI configuration \u2705 Complete 1.2 CacheManager Recursive Bug 25/27 test failures \u2705 Complete 1.3 Missing <code>__init__.py</code> Files Import errors \u2705 Complete 2.1 <code>.dockerignore</code> Exclusion Docker build failure \u2705 Complete 2.2 Validation Script Indentation Syntax errors \u2705 Complete 3.1 Missing Test Fixture 1 test failure \u2705 Complete"},{"location":"ci_cd_fixes.html#detailed-fixes","title":"Detailed Fixes","text":""},{"location":"ci_cd_fixes.html#task-11-python-version-requirements","title":"Task 1.1: Python Version Requirements","text":"<p>Issue: CI was testing against Python 3.8 and 3.9, which are incompatible with the codebase that requires Python 3.10+ features.</p> <p>Impact: Test failures due to syntax errors on older Python versions</p> <p>Fix: Updated <code>.github/workflows/test.yml</code> to test only Python 3.10, 3.11, and 3.12</p> <p>Before: <pre><code>strategy:\n  fail-fast: false\n  matrix:\n    python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\"]\n</code></pre></p> <p>After: <pre><code>strategy:\n  fail-fast: false\n  matrix:\n    python-version: [\"3.10\", \"3.11\", \"3.12\"]\n</code></pre></p> <p>Files Changed: - <code>.github/workflows/test.yml:27</code></p> <p>Validation: <pre><code># Verify Python version requirements\ngrep -A 3 \"python-version:\" .github/workflows/test.yml\n\n# Expected output should show only [\"3.10\", \"3.11\", \"3.12\"]\n</code></pre></p>"},{"location":"ci_cd_fixes.html#task-12-cachemanager-recursive-bug","title":"Task 1.2: CacheManager Recursive Bug","text":"<p>Issue: The <code>CacheManager</code> class had an infinite recursion bug where <code>_log()</code> was calling <code>self._log()</code> through <code>__getattribute__</code>, creating a stack overflow.</p> <p>Impact: 25 out of 27 test failures - this was the critical bug causing the majority of failures</p> <p>Root Cause: In <code>Decorators/CacheManager.py</code>, the internal <code>_log()</code> method was calling <code>self._log()</code>, which triggered <code>__getattribute__</code>, which then called <code>_log()</code> again, creating infinite recursion.</p> <p>Fix: Changed the logger call in <code>_log()</code> method to use <code>self.logger.log()</code> directly instead of <code>self._log()</code></p> <p>Before (Decorators/CacheManager.py:112): <pre><code>def _log(self, message: str, level: int = logging.INFO) -&gt; None:\n    \"\"\"Internal logging method.\"\"\"\n    if self.logger:\n        self._log(message, level)  # \u274c Infinite recursion!\n</code></pre></p> <p>After (Decorators/CacheManager.py:112): <pre><code>def _log(self, message: str, level: int = logging.INFO) -&gt; None:\n    \"\"\"Internal logging method.\"\"\"\n    if self.logger:\n        self.logger.log(level, message)  # \u2705 Direct logger call\n</code></pre></p> <p>Files Changed: - <code>Decorators/CacheManager.py:112</code></p> <p>Commit: <code>6338bac</code></p> <p>Validation: <pre><code># Run tests to verify recursion is fixed\npytest Test/Tests/TestCacheManager.py -v\n\n# Run full test suite\npytest -v\n\n# Expected: 27/27 tests passing (was 2/27 before fix)\n</code></pre></p> <p>Technical Details:</p> <p>The bug manifested because: 1. Any attribute access on <code>CacheManager</code> goes through <code>__getattribute__</code> 2. <code>__getattribute__</code> calls <code>_log()</code> for logging 3. <code>_log()</code> was trying to call <code>self._log()</code> again 4. This triggered <code>__getattribute__</code> again, creating infinite recursion</p> <p>The fix bypasses this by calling <code>self.logger.log()</code> directly, which doesn't trigger the recursive loop since <code>logger</code> is a standard Python object.</p>"},{"location":"ci_cd_fixes.html#task-13-add-missing-__init__py-files","title":"Task 1.3: Add Missing <code>__init__.py</code> Files","text":"<p>Issue: The <code>Helpers/</code> and <code>Decorators/</code> directories were missing <code>__init__.py</code> files, causing Python to not recognize them as packages.</p> <p>Impact: Import errors in tests: <code>ModuleNotFoundError: No module named 'Helpers'</code> or <code>'Decorators'</code></p> <p>Fix: Created empty <code>__init__.py</code> files in both directories</p> <p>Files Created: - <code>Helpers/__init__.py</code> (empty file) - <code>Decorators/__init__.py</code> (empty file)</p> <p>Commits: <code>465e016</code>, <code>919d8ad</code></p> <p>Validation: <pre><code># Verify __init__.py files exist\nls -la Helpers/__init__.py\nls -la Decorators/__init__.py\n\n# Test imports work\npython -c \"import Helpers; import Decorators; print('\u2705 Imports successful')\"\n\n# Run tests that depend on these imports\npytest Test/Tests/TestPassByRef.py -v\n</code></pre></p> <p>Why This Matters:</p> <p>In Python 3.3+, namespace packages can work without <code>__init__.py</code>, but the test suite and some imports expect these directories to be regular packages. Adding <code>__init__.py</code> ensures consistent behavior across different Python versions and import scenarios.</p>"},{"location":"ci_cd_fixes.html#task-21-fix-dockerignore","title":"Task 2.1: Fix <code>.dockerignore</code>","text":"<p>Issue: The <code>.dockerignore</code> file had a pattern <code>validate_*.py</code> that excluded ALL validation scripts, including <code>validate_dask_setup.py</code> which is needed by the Dockerfile.</p> <p>Impact: Docker build failed with \"file not found\" error when trying to run <code>validate_dask_setup.py</code></p> <p>Root Cause: The Dockerfile CMD (line 60) runs <code>validate_dask_setup.py</code>, but <code>.dockerignore</code> (line 82) excludes all <code>validate_*.py</code> files from the build context.</p> <p>Fix: Added an exception pattern <code>!validate_dask_setup.py</code> before the exclusion pattern</p> <p>Before (.dockerignore:82): <pre><code>validate_*.py\n</code></pre></p> <p>After (.dockerignore:81-82): <pre><code>!validate_dask_setup.py\nvalidate_*.py\n</code></pre></p> <p>Files Changed: - <code>.dockerignore:81-82</code></p> <p>Commit: <code>b88dd7b</code></p> <p>Validation: <pre><code># Test Docker build\ndocker build -t connected-driving-pipeline:test .\n\n# Expected: Build succeeds and includes validate_dask_setup.py\n\n# Test Docker run\ndocker run --rm connected-driving-pipeline:test python validate_dask_setup.py\n\n# Expected: Validation script runs successfully\n</code></pre></p> <p>Technical Note:</p> <p>Docker processes <code>.dockerignore</code> patterns sequentially. Exception patterns (starting with <code>!</code>) must come BEFORE the exclusion pattern they're exempting from. This is why <code>!validate_dask_setup.py</code> is on line 81 and <code>validate_*.py</code> is on line 82.</p>"},{"location":"ci_cd_fixes.html#task-22-fix-validate_dask_clean_with_timestampspy-indentation","title":"Task 2.2: Fix <code>validate_dask_clean_with_timestamps.py</code> Indentation","text":"<p>Issue: Lines 163-252 in <code>validate_dask_clean_with_timestamps.py</code> had extra 4-space indentation (8 spaces instead of 4), causing syntax errors.</p> <p>Impact: Script failed to parse with <code>IndentationError</code></p> <p>Fix: Dedented lines 163-252 by 4 spaces to restore proper indentation</p> <p>Before (validate_dask_clean_with_timestamps.py:163-252): <pre><code>def validate_timestamp_consistency():\n        # Extra 4 spaces of indentation\n        \"\"\"Validate timestamp consistency.\"\"\"\n        df = dd.read_csv(\"data.csv\")\n        # ... rest of function with 8-space indent\n</code></pre></p> <p>After (validate_dask_clean_with_timestamps.py:163-252): <pre><code>def validate_timestamp_consistency():\n    # Correct 4-space indentation\n    \"\"\"Validate timestamp consistency.\"\"\"\n    df = dd.read_csv(\"data.csv\")\n    # ... rest of function with 4-space indent\n</code></pre></p> <p>Files Changed: - <code>validate_dask_clean_with_timestamps.py:163-252</code> (dedented 90 lines)</p> <p>Commit: <code>fddfa5e</code></p> <p>Validation: <pre><code># Verify Python syntax\npython -m py_compile validate_dask_clean_with_timestamps.py\n\n# Expected: No syntax errors\n\n# Run the validation script\npython validate_dask_clean_with_timestamps.py\n\n# Expected: Script runs without IndentationError\n</code></pre></p>"},{"location":"ci_cd_fixes.html#task-31-add-missing-test-fixture-somedict","title":"Task 3.1: Add Missing Test Fixture (<code>someDict</code>)","text":"<p>Issue: Test file <code>Test/Tests/TestPassByRef.py</code> required a <code>someDict</code> fixture that was missing from <code>conftest.py</code>.</p> <p>Impact: 1 test failure with error: <code>fixture 'someDict' not found</code></p> <p>Fix: Added <code>someDict</code> fixture to <code>conftest.py</code></p> <p>Status: \u2705 Already existed in codebase (conftest.py:116-123)</p> <p>Implementation (conftest.py:116-123): <pre><code>@pytest.fixture\ndef someDict():\n    \"\"\"Fixture providing a simple dictionary for pass-by-reference tests.\"\"\"\n    return {\n        'key1': 'value1',\n        'key2': 'value2'\n    }\n</code></pre></p> <p>Files Changed: - <code>conftest.py:116-123</code> (already present)</p> <p>Validation: <pre><code># Test the fixture works\npytest Test/Tests/TestPassByRef.py -v\n\n# Expected: Test passes successfully\npytest Test/Tests/TestPassByRef.py::TestPassByRef::test_func_to_add_to_dict -v\n</code></pre></p>"},{"location":"ci_cd_fixes.html#validation-commands","title":"Validation Commands","text":""},{"location":"ci_cd_fixes.html#complete-test-suite-validation","title":"Complete Test Suite Validation","text":"<p>Run these commands to verify all fixes:</p> <pre><code># 1. Verify Python syntax for all files\nfind . -name \"*.py\" -not -path \"./.venv/*\" -exec python -m py_compile {} \\;\n\n# 2. Run full test suite\npytest -v --cov=. --cov-report=term-missing\n\n# Expected: All tests pass with 70%+ coverage\n\n# 3. Run Dask setup validation\npython validate_dask_setup.py\n\n# Expected: All 8 validation tests pass\n\n# 4. Build and test Docker image\ndocker build -t connected-driving-pipeline:test .\ndocker run --rm connected-driving-pipeline:test python validate_dask_setup.py\n\n# Expected: Docker build succeeds and validation passes\n\n# 5. Check CI workflow configuration\ngrep -A 3 \"python-version:\" .github/workflows/test.yml\n\n# Expected: Only Python 3.10, 3.11, 3.12\n\n# 6. Verify __init__.py files exist\ntest -f Helpers/__init__.py &amp;&amp; test -f Decorators/__init__.py &amp;&amp; echo \"\u2705 __init__.py files present\"\n\n# 7. Test imports\npython -c \"import Helpers; import Decorators; from Decorators.CacheManager import CacheManager; print('\u2705 All imports successful')\"\n</code></pre>"},{"location":"ci_cd_fixes.html#cicd-pipeline-validation","title":"CI/CD Pipeline Validation","text":"<p>To test locally what CI runs:</p> <pre><code># Run tests for each Python version (if you have pyenv)\nfor version in 3.10 3.11 3.12; do\n    echo \"Testing Python $version...\"\n    python$version -m pytest -v\ndone\n\n# Run linting (code quality checks)\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\nblack --check --diff .\nisort --check-only --diff .\n\n# Run integration tests\npytest -v -m integration --tb=short\n\n# Run slow tests\npytest -v -m slow --tb=short\n</code></pre>"},{"location":"ci_cd_fixes.html#results","title":"Results","text":""},{"location":"ci_cd_fixes.html#before-fixes","title":"Before Fixes","text":"<ul> <li>\u274c 25/27 tests failing (92.6% failure rate)</li> <li>\u274c Python 3.8/3.9 incompatibility</li> <li>\u274c Docker build failures</li> <li>\u274c Import errors</li> <li>\u274c Infinite recursion in CacheManager</li> </ul>"},{"location":"ci_cd_fixes.html#after-fixes","title":"After Fixes","text":"<ul> <li>\u2705 27/27 tests passing (100% pass rate)</li> <li>\u2705 Python 3.10+ compatibility enforced</li> <li>\u2705 Docker build and validation successful</li> <li>\u2705 All imports working correctly</li> <li>\u2705 CacheManager recursion bug fixed</li> <li>\u2705 Code coverage \u226570%</li> </ul>"},{"location":"ci_cd_fixes.html#lessons-learned","title":"Lessons Learned","text":"<ol> <li> <p>Infinite Recursion in <code>__getattribute__</code>: Be extremely careful when overriding <code>__getattribute__</code>. Any <code>self.x</code> access inside it must use <code>object.__getattribute__(self, 'x')</code> or direct references to avoid recursion.</p> </li> <li> <p>Docker <code>.dockerignore</code> Pattern Order: Exception patterns (<code>!pattern</code>) must come BEFORE exclusion patterns (<code>pattern</code>) in <code>.dockerignore</code> for them to take effect.</p> </li> <li> <p>Python Package Structure: Even in Python 3.3+, it's best practice to include <code>__init__.py</code> files in all packages to ensure compatibility and clear intent.</p> </li> <li> <p>CI Python Version Matrix: Always match CI test matrix to actual project requirements. Testing against unsupported Python versions wastes CI time and creates false failures.</p> </li> <li> <p>Indentation Consistency: Use automated tools (black, autopep8) to prevent indentation bugs. A simple <code>black .</code> would have prevented the validation script syntax error.</p> </li> </ol>"},{"location":"ci_cd_fixes.html#future-maintenance","title":"Future Maintenance","text":"<p>To prevent similar issues in the future:</p> <ol> <li>Pre-commit Hooks: Set up pre-commit hooks to run:</li> <li><code>black</code> for code formatting</li> <li><code>flake8</code> for linting</li> <li> <p><code>pytest</code> for tests</p> </li> <li> <p>CI Pipeline Monitoring: Regularly review CI pipeline results and investigate any new failures immediately.</p> </li> <li> <p>Docker Build Testing: Include Docker build in CI pipeline (already implemented in <code>.github/workflows/test.yml</code>).</p> </li> <li> <p>Documentation: Keep this document updated when making CI/CD changes.</p> </li> <li> <p>Validation Scripts: Regularly run <code>validate_dask_setup.py</code> and other validation scripts locally before pushing.</p> </li> </ol>"},{"location":"ci_cd_fixes.html#references","title":"References","text":"<ul> <li>GitHub Actions Workflow</li> <li>README.md CI/CD Troubleshooting Section</li> <li>Troubleshooting Guide</li> <li>CacheManager Implementation</li> </ul> <p>Document Version: 1.0 Last Updated: 2026-01-18 Author: Ralph (AI Development Agent)</p>"},{"location":"results.html","title":"Results","text":"Loading Results... In Progress Jobs Our Current Results"},{"location":"Getting%20Started/Configuration%20Parameters.html","title":"Configuration Parameters","text":"<p>There's a lot of configuration parameters that can be used to customize the behavior of the application. The following tables lists all the available parameters and their default values for each respective provider.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#pathprovider","title":"PathProvider","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#loggerlogpath","title":"Logger.logpath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value","title":"Recommended Value","text":"<p><code>DEFAULT_LOG_PATH</code></p> <p>Get the default log path by importing it from the Logger module</p> <pre><code>from Logger.Logger import DEFAULT_LOG_PATH\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description","title":"Description","text":"<p>The path to the log folder. The log folder will contain all the log files generated by the pipeline user file.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#initialgathererpathprovider","title":"InitialGathererPathProvider","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#datagathererfilepath","title":"DataGatherer.filepath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_1","title":"Recommended Value","text":"<pre><code>lambda model: \"data/data.csv\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_1","title":"Description","text":"<p>The path function to the file containing the data to be used for the initial gathering of data. The file should be a csv file.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#datagatherersubsectionpath","title":"DataGatherer.subsectionpath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_2","title":"Recommended Value","text":"<pre><code>lambda model: f\"data/classifierdata/subsection/{model}/subsection{numSubsectionRows}.csv\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_2","title":"Description","text":"<p>The path function to the file containing the subsection of the data to be used for the initial gathering of data. The file should be a csv file. This file is used to get a subsection of the massive data file to be used for the initial gathering of data. This parameter is used in both the regular (non-large) pipeline and the large pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#datagatherersplitfilespath","title":"DataGatherer.splitfilespath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_3","title":"Recommended Value","text":"<pre><code>lambda model: f\"data/classifierdata/splitfiles/{model}/\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_3","title":"Description","text":"<p>The path function to the folder containing the split files of the data. The split files will be individually cleaned and then merged later. This folder is only used in the large pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#generatorpathprovider","title":"GeneratorPathProvider","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanercleanedfilespath","title":"ConnectedDrivingLargeDataCleaner.cleanedfilespath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_4","title":"Recommended Value","text":"<pre><code>lambda model:  f\"data/classifierdata/splitfiles/cleaned/{model}/\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_4","title":"Description","text":"<p>The path function to the folder containing the cleaned split files of the data. The split files will be individually cleaned and then stored in this folder. This folder is only used in the large pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanercombinedcleandatapath","title":"ConnectedDrivingLargeDataCleaner.combinedcleandatapath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_5","title":"Recommended Value","text":"<pre><code>lambda model: f\"data/classifierdata/splitfiles/combinedcleaned/{model}/combinedcleaned\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_5","title":"Description","text":"<p>The path function to the file containing the combined cleaned data. This file is only used in the large pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#mlpathprovider","title":"MLPathProvider","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#mconnecteddrivingdatacleanercleandatapath","title":"MConnectedDrivingDataCleaner.cleandatapath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_6","title":"Recommended Value","text":"<pre><code>lambda model: f\"data/mclassifierdata/cleaned/{model}/clean.csv\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_6","title":"Description","text":"<p>The path function to the file containing the attacked and cleaned data. This file is used in every pipeline to get the data to be used for training and testing.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#mdataclassifierplot_confusion_matrix_path","title":"MDataClassifier.plot_confusion_matrix_path","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_7","title":"Recommended Value","text":"<pre><code>lambda model: f\"data/mclassifierdata/results/{model}/\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_7","title":"Description","text":"<p>The path function to the folder containing the confusion matrix plots. This folder is used to store the confusion matrix plots generated by  each classifier.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#generatorcontextprovider","title":"GeneratorContextProvider","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#datagatherernumrows","title":"DataGatherer.numrows","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_8","title":"Recommended Value","text":"<pre><code>100000\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_8","title":"Description","text":"<p>The number of rows to gather and store as a subsection from the original dataset. This parameter is used in both the regular (non-large) pipeline and the large pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#datagathererlines_per_file","title":"DataGatherer.lines_per_file","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_9","title":"Recommended Value","text":"<pre><code>1000000\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_9","title":"Description","text":"<p>The number of rows to store in each split file. This parameter is only used in the large pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingcleanerx_pos","title":"ConnectedDrivingCleaner.x_pos","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_10","title":"Recommended Value","text":"<pre><code>-105.1159611\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_10","title":"Description","text":"<p>The x coordinate of the center of the circle to be used for filtering the data. This parameter is only used in the large pipeline when filtering the data to be within a certain range.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingcleanery_pos","title":"ConnectedDrivingCleaner.y_pos","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_11","title":"Recommended Value","text":"<pre><code>41.0982327\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_11","title":"Description","text":"<p>The y coordinate of the center of the circle to be used for filtering the data. This parameter is only used in the large pipeline when filtering the data to be within a certain range.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingcleanercolumns","title":"ConnectedDrivingCleaner.columns","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_12","title":"Recommended Value","text":"<pre><code>[\"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n\"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n\"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n#  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n#  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n\"coreData_id\", \"coreData_secMark\", \"coreData_position_lat\", \"coreData_position_long\",\n\"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n\"coreData_elevation\", \"coreData_accelset_accelYaw\",\"coreData_speed\", \"coreData_heading\", \"coreData_position\"]\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_12","title":"Description","text":"<p>The columns to be used for filtering the initial data. Most of the columns are useless so these columns are the ones we are choosing to use in the pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanermax_dist","title":"ConnectedDrivingLargeDataCleaner.max_dist","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_13","title":"Recommended Value","text":"<pre><code>500\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_13","title":"Description","text":"<p>The maximum distance from the center of the circle to be used for filtering the data. This parameter is only used in the large pipeline when filtering the data to be within a certain range.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingcleanershouldgatherautomatically","title":"ConnectedDrivingCleaner.shouldGatherAutomatically","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_14","title":"Recommended Value","text":"<pre><code>False\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_14","title":"Description","text":"<p>Whether or not to automatically gather the data if the cleaner isn't given any data. The reason we set it off by default is to prevent data from being collected without the user's knowledge.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanercleanerclass","title":"ConnectedDrivingLargeDataCleaner.cleanerClass","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_15","title":"Recommended Value","text":"<pre><code>CleanWithTimestamps\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_15","title":"Description","text":"<p>The class to be used for cleaning the data. This parameter should match the class that the cleanFunc comes from.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanercleanfunc","title":"ConnectedDrivingLargeDataCleaner.cleanFunc","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_16","title":"Recommended Value","text":"<pre><code>CleanWithTimestamps.clean_data_with_timestamps\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_16","title":"Description","text":"<p>The function to be used for cleaning the data. This parameter should match the class that the cleanerClass comes from.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanercleanerwithfilterclass","title":"ConnectedDrivingLargeDataCleaner.cleanerWithFilterClass","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_17","title":"Recommended Value","text":"<pre><code>CleanerWithFilterWithinRangeXY\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_17","title":"Description","text":"<p>The class to be used for filtering the data. This parameter should match the class that the filterFunc comes from.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanerfilterfunc","title":"ConnectedDrivingLargeDataCleaner.filterFunc","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_18","title":"Recommended Value","text":"<pre><code>CleanerWithFilterWithinRangeXY.within_rangeXY\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_18","title":"Description","text":"<p>The function to be used for filtering the data. This parameter should match the class that the cleanerWithFilterClass comes from.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingattackerseed","title":"ConnectedDrivingAttacker.SEED","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_19","title":"Recommended Value","text":"<pre><code>42\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_19","title":"Description","text":"<p>The seed to be used for the random number generator. This parameter is only used in the large pipeline when generating the attack data.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingcleanerisxycoords","title":"ConnectedDrivingCleaner.isXYCoords","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_20","title":"Recommended Value","text":"<pre><code>True\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_20","title":"Description","text":"<p>Whether or not the data is in (x,y) coordinates (as distance from the point on the longitude, latitude axis respectively in meters).</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingattackerattack_ratio","title":"ConnectedDrivingAttacker.attack_ratio","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_21","title":"Recommended Value","text":"<pre><code>0.3\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_21","title":"Description","text":"<p>The ratio of the data to be attacked. For example, 0.3 = 30% of the data will be attacked. The attack ratio is used differently depending on the attack distribution method. For example, <code>add_attackers</code> will specify a ratio of the cars to be attackers and 100% of their BSMs will be attacked. On the other hand, <code>add_rand_attackers</code> will specify a ratio of the BSMs to be attacked randomly.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingcleanercleanparams","title":"ConnectedDrivingCleaner.cleanParams","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_22","title":"Recommended Value","text":"<pre><code>f\"clean_data_with_timestamps-within_rangeXY-WithXYCoords-1000mdist-x{x_pos_str}y{y_pos_str}dd02mm04yyyy2021\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_22","title":"Description","text":"<p>The name of the parameters to be used for cleaning the data. This parameter is used in the caching and should match cleanFunc + any other parameters used such as the filter, etc.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#mlcontextprovider","title":"MLContextProvider","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#mconnecteddrivingdatacleanercolumns","title":"MConnectedDrivingDataCleaner.columns","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_23","title":"Recommended Value","text":"<pre><code> [\n# \"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n#  \"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n#  \"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n#  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n#  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n\"coreData_id\",  # \"coreData_position_lat\", \"coreData_position_long\",\n\"coreData_secMark\", \"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n\"month\", \"day\", \"year\", \"hour\", \"minute\", \"second\", \"pm\",\n\"coreData_elevation\", \"coreData_accelset_accelYaw\", \"coreData_speed\", \"coreData_heading\", \"x_pos\", \"y_pos\", \"isAttacker\"],\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_23","title":"Description","text":"<p>The columns to be used for training the model (and also the final columns after the attacker finishes).</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#mclassifierpipelineclassifier_instances","title":"MClassifierPipeline.classifier_instances","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value-auto_filled","title":"Recommended Value (AUTO_FILLED)","text":"<pre><code>[RandomForestClassifier(\n), DecisionTreeClassifier(), KNeighborsClassifier()]\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_24","title":"Description","text":"<p>The classifiers to be used for training the model. These are autofilled but we can change them if we want to use different classifiers. At the top of the example class on the development page, we specify the CLASSIFIER_INSTANCES variable to be used for the pipeline but we didn't include it in the config because it was autofilled. However, it would be easy to modify the array and pass it in. Make sure to include the modified parameters in your LOG_NAME and file name to avoid caching errors though!</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#cleanerwithfilterwithinrangexyanddayday","title":"CleanerWithFilterWithinRangeXYAndDay.day","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_24","title":"Recommended Value","text":"<pre><code>2\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_25","title":"Description","text":"<p>The day to be used for filtering the data. This parameter is only used in the large pipeline when filtering the data to be within a certain range.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#cleanerwithfilterwithinrangexyanddaymonth","title":"CleanerWithFilterWithinRangeXYAndDay.month","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_25","title":"Recommended Value","text":"<pre><code>4\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_26","title":"Description","text":"<p>The month to be used for filtering the data. This parameter is only used in the large pipeline when filtering the data to be within a certain range.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#cleanerwithfilterwithinrangexyanddayyear","title":"CleanerWithFilterWithinRangeXYAndDay.year","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_26","title":"Recommended Value","text":"<pre><code>2021\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_27","title":"Description","text":"<p>The year to be used for filtering the data. This parameter is only used in the large pipeline when filtering the data to be within a certain range.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#mdataclassifierplot_distribution_path","title":"MDataClassifier.plot_distribution_path","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_27","title":"Recommended Value","text":"<pre><code>lambda model: f\"data/mclassifierdata/results/{model}/\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_28","title":"Description","text":"<p>The path to be used for plotting the distribution of the data. This parameter is only used in the large pipeline when plotting the distribution of the data during feature analysis.</p>"},{"location":"Getting%20Started/development.html","title":"Development","text":"<p>Head to Setup if you haven't already setup the project.</p>"},{"location":"Getting%20Started/development.html#creating-a-pipeline-user-file","title":"Creating A Pipeline User File","text":""},{"location":"Getting%20Started/development.html#file-creation","title":"File Creation","text":"<p>Pipeline user files should always have a unique name that specifies what the file will execute and the parameters that make it unique.</p> <p>For example, look at the pipeline user file called <code>MClassifierLargePipelineUserWithXYOffsetPos500mDist100kRowsEXTTimestampsCols30attackersRandOffset100To200</code>. It runs the large pipeline with the following settings:</p> <ul> <li>WithXYOffsetPos: Tells us that it will use the XY coordinate system</li> <li>500mDist: This reminds us that we are using the 500m distance filter</li> <li>100kRows: This specifies that we are only using 100k rows to train our data</li> <li>EXTTimestampscols: This reminds us that we are cleaning our data to use the extended timestamp columns (which I've included in the file)</li> <li>30attackers: Specifies that we are using 30% attackers (and by default, it is splitting the cars such that 30% are attackers and then making 100% of the BSMs malicious)</li> <li>RandOffset100To200: Specifies that we are using the random offset attack with a min distance of 100m and a max distance of 200m.</li> </ul> <p>Pipeline user files should be placed in the base directory of the project. For example, the <code>MClassifierLargePipelineUserWithXYOffsetPos500mDist100kRowsEXTTimestampsCols30attackersRandOffset100To200</code> file is placed in the base directory of the project.</p>"},{"location":"Getting%20Started/development.html#providers","title":"Providers","text":"<p>Providers act as a way to configure the pipeline and set properties that can be accessed within any file. They are implemented as singletons so that they can be instantiated anywhere to access the properties.</p> <p>The most basic provider is the <code>DictProvider</code> and it provides a dictionary of key value pairs that can be accessed by the pipeline. For example, the following provider provides a dictionary with the key <code>test</code> and the value <code>testvalue</code>:</p> <pre><code>from ServiceProviders.DictProvider import DictProvider\n\nprovider = DictProvider(contexts={\"test\": \"testvalue\"})\n</code></pre> <p>which can be accessed by the pipeline like so:</p> <pre><code>from ServiceProviders.DictProvider import DictProvider\n\nprovider = DictProvider()\ntestVal = provider.get(\"test\")\nprint(testVal) # prints \"testvalue\"\n</code></pre> <p>However, since the DictProvider is a singleton, we run into trouble if we want to have multiple providers with different values for the same key. Thus, we created context providers for different parts of the pipeline. The ones we used are <code>GeneratorContextProvider</code> and <code>MLContextProvider</code> which are used for the generator and machine learning parts of the pipeline respectively. These providers are instantiated with a dictionary of key value pairs (similar to DictProvider). For example, the following provider provides a dictionary with the key <code>test</code> and the value <code>testvalue</code>:</p> <pre><code>from ServiceProviders.GeneratorContextProvider import GeneratorContextProvider\nfrom ServiceProviders.MLContextProvider import MLContextProvider\n\ngeneratorProvider = GeneratorContextProvider(contexts={\"test\": \"testvalue\"})\nmlProvider = MLContextProvider(contexts={\"test\": \"testvalue2\"})\n</code></pre> <p>which can be accessed by the pipeline like so:</p> <pre><code>from ServiceProviders.GeneratorContextProvider import GeneratorContextProvider\nfrom ServiceProviders.MLContextProvider import MLContextProvider\n\ngeneratorProvider = GeneratorContextProvider()\nmlProvider = MLContextProvider()\ntestVal = generatorProvider.get(\"test\")\ntestVal2 = mlProvider.get(\"test\")\nprint(testVal) # prints \"testvalue\"\nprint(testVal2) # prints \"testvalue2\"\n</code></pre> <p>ContextProviders are useful but we also ran into another issue with paths. We needed the paths for some parts of the pipeline to be unique and other parts to be shared. Thus, we created a variant of the <code>DictProvider</code> called the <code>PathProvider</code> that uses a model attribute to create a unique element of each value. The user provides a key and value with the key being a string and the value as a function. The function takes in a model name and returns the value. For example, the following <code>PathProvider</code> provides a dictionary with the key <code>test</code> and the value <code>{model}/testvalue</code> where model=<code>somemodel</code>:</p> <pre><code>from ServiceProviders.PathProvider import PathProvider\n\nprovider = PathProvider(model=\"somemodel\" contexts={\"test\": lambda model: f\"{model}/testvalue\"})\n</code></pre> <p>which can be accessed by the pipeline like so:</p> <pre><code>from ServiceProviders.PathProvider import PathProvider\n\nprovider = PathProvider(model=\"somemodel\")\ntestVal = provider.get(\"test\")\nprint(testVal) # prints \"somemodel/testvalue\"\n</code></pre> <p>However, we ran into the same issue with PathProvider, needing a unique path for the initial gatherer, generator, and machine learning parts of the pipeline. Thus, we created the <code>InitialGathererPathProvider</code>, <code>GeneratorPathProvider</code>, and <code>MLPathProvider</code> which are used for the initial gatherer, generator, and machine learning parts of the pipeline respectively. These providers are instantiated with a dictionary of key value pairs (similar to PathProvider). The model name is used to create a unique key for each key value pair. For example, the following provider provides a dictionary with the key <code>test</code> and the value <code>{model}/testvalue</code> where model=<code>initialGatherer</code>, <code>generator</code>, or <code>ml</code> depending on the provider:</p> <pre><code>from ServiceProviders.InitialGathererPathProvider import InitialGathererPathProvider\nfrom ServiceProviders.GeneratorPathProvider import GeneratorPathProvider\nfrom ServiceProviders.MLPathProvider import MLPathProvider\n\ninitialGathererProvider = InitialGathererPathProvider(model=\"initialGatherer\", contexts={\"test\": lambda model: f\"{model}/testvalue\"})\ngeneratorProvider = GeneratorPathProvider(model=\"generator\", contexts={\"test\": lambda model: f\"{model}/testvalue\"})\nmlProvider = MLPathProvider(model=\"ml\", contexts={\"test\": lambda model: f\"{model}/testvalue\"})\n</code></pre> <p>which can be accessed by the pipeline like so:</p> <pre><code>from ServiceProviders.InitialGathererPathProvider import InitialGathererPathProvider\nfrom ServiceProviders.GeneratorPathProvider import GeneratorPathProvider\nfrom ServiceProviders.MLPathProvider import MLPathProvider\n\ninitialGathererProvider = InitialGathererPathProvider(model=\"initialGatherer\")\ngeneratorProvider = GeneratorPathProvider(model=\"generator\")\nmlProvider = MLPathProvider(model=\"ml\")\n\n# prints \"initialGatherer/testvalue\"\ninitialGathererTestPath = initialGathererProvider.get(\"test\")\n\n# prints \"generator/testvalue\"\ngeneratorTestPath = generatorProvider.get(\"test\")\n\n# prints \"ml/testvalue\"\nmlTestPath = mlProvider.get(\"test\")\n</code></pre>"},{"location":"Getting%20Started/development.html#configuration","title":"Configuration","text":""},{"location":"Getting%20Started/development.html#setting-up-the-configuration","title":"Setting Up The Configuration","text":"<p>All pipeline user files should use a unique class name and log name along with configuration for the providers. The providers can provide a value when requested from a given key. See the Providers section for more information on providers. You can also reference the Configuration Parameters page to see all the possible configuration parameters.</p> <p>The following is the config for <code>MClassifierLargePipelineUserWithXYOffsetPos500mDist100kRowsEXTTimestampsCols30attackersRandOffset100To200</code>:</p> <pre><code># MClassifierPipeline-Const-50-offset\n\nimport os\nfrom pandas import DataFrame\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom EasyMLLib.CSVWriter import CSVWriter\nfrom Generator.Attackers.Attacks.StandardPositionalOffsetAttacker import StandardPositionalOffsetAttacker\nfrom Generator.Attackers.ConnectedDrivingAttacker import ConnectedDrivingAttacker\nfrom Generator.Cleaners.CleanersWithFilters.CleanerWithFilterWithinRangeXY import CleanerWithFilterWithinRangeXY\nfrom Generator.Cleaners.ConnectedDrivingCleaner import ConnectedDrivingCleaner\nfrom Generator.Cleaners.ConnectedDrivingLargeDataCleaner import ConnectedDrivingLargeDataCleaner\nfrom Generator.Cleaners.ExtraCleaningFunctions.CleanWithTimestamps import CleanWithTimestamps\n\nfrom Logger.Logger import DEFAULT_LOG_PATH, Logger\nfrom Generator.Cleaners.ConnectedDrivingLargeDataPipelineGathererAndCleaner import ConnectedDrivingLargeDataPipelineGathererAndCleaner\nfrom MachineLearning.MClassifierPipeline import MClassifierPipeline\nfrom MachineLearning.MConnectedDrivingDataCleaner import MConnectedDrivingDataCleaner\nfrom ServiceProviders.GeneratorContextProvider import GeneratorContextProvider\nfrom ServiceProviders.GeneratorPathProvider import GeneratorPathProvider\nfrom ServiceProviders.InitialGathererPathProvider import InitialGathererPathProvider\nfrom ServiceProviders.MLContextProvider import MLContextProvider\nfrom ServiceProviders.MLPathProvider import MLPathProvider\nfrom ServiceProviders.PathProvider import PathProvider\n\n\nCLASSIFIER_INSTANCES = [RandomForestClassifier(\n), DecisionTreeClassifier(), KNeighborsClassifier()]\n\nLOG_NAME = \"MClassifierLargePipelineUserWithXYOffsetPos500mDist100kRowsEXTTimestampsCols30attackersRandOffset100To200\"\n\nCSV_COLUMNS = [\"Model\", \"Total_Train_Time\",\n               \"Total_Train_Sample_Size\", \"Total_Test_Sample_Size\", \"Train_Time_Per_Sample\", \"Prediction_Train_Set_Time_Per_Sample\", \"Prediction_Test_Set_Time_Per_Sample\",\n               \"train_accuracy\", \"train_precision\", \"train_recall\", \"train_f1\",\n               \"test_accuracy\", \"test_precision\", \"test_recall\", \"test_f1\"]\n\nCSV_FORMAT = {CSV_COLUMNS[i]: i for i in range(len(CSV_COLUMNS))}\n\n\nclass MClassifierLargePipelineUserWithXYOffsetPos500mDist100kRowsEXTTimestampsCols30attackersRandOffset100To200:\n\n    def __init__(self):\n\n        #################  CONFIG FOR ALL PROPERTIES IN THE PIPELINE #################################################\n\n        # used for the logger's path\n        self._pathprovider = PathProvider(model=LOG_NAME, contexts={\n                \"Logger.logpath\": DEFAULT_LOG_PATH,\n        })\n\n        initialGathererModelName = \"CreatingConnectedDrivingDataset\"\n        numSubsectionRows = 100000\n\n        # Properties:\n        # DataGatherer.filepath\n        # DataGatherer.subsectionpath\n        # DataGatherer.splitfilespath\n        # DataGatherer.lines_per_file\n        self._initialGathererPathProvider = InitialGathererPathProvider(model=initialGathererModelName, contexts={\n            \"DataGatherer.filepath\": lambda model: \"data/data.csv\",\n            \"DataGatherer.subsectionpath\": lambda model: f\"data/classifierdata/subsection/{model}/subsection{numSubsectionRows}.csv\",\n            \"DataGatherer.splitfilespath\": lambda model: f\"data/classifierdata/splitfiles/{model}/\",\n        }\n        )\n\n        # Properties:\n        #\n        # ConnectedDrivingLargeDataCleaner.cleanedfilespath\n        # ConnectedDrivingLargeDataCleaner.combinedcleandatapath\n        # MConnectedDrivingLargeDataCleaner.dtypes # AUTO_FILLED\n        #\n        # MAKE SURE TO CHANGE THE MODEL NAME TO THE PROPER NAME (IE A NAME THAT MATCHES IF\n        # IT HAS TIMESTAMPS OR NOT, AND IF IT HAS XY COORDS OR NOT, ETC)\n        self._generatorPathProvider = GeneratorPathProvider(model=f\"{initialGathererModelName}-CCDDWithTimestampsAndWithXYCoords-500mdist\", contexts={\n            \"ConnectedDrivingLargeDataCleaner.cleanedfilespath\": lambda model:  f\"data/classifierdata/splitfiles/cleaned/{model}/\",\n            \"ConnectedDrivingLargeDataCleaner.combinedcleandatapath\": lambda model: f\"data/classifierdata/splitfiles/combinedcleaned/{model}/combinedcleaned\",\n        }\n        )\n\n        # Properties:\n        #\n        # MConnectedDrivingDataCleaner.cleandatapath\n        # MDataClassifier.plot_confusion_matrix_path\n        #\n        self._mlPathProvider = MLPathProvider(model=LOG_NAME, contexts={\n            \"MConnectedDrivingDataCleaner.cleandatapath\": lambda model: f\"data/mclassifierdata/cleaned/{model}/clean.csv\",\n            \"MDataClassifier.plot_confusion_matrix_path\": lambda model: f\"data/mclassifierdata/results/{model}/\",\n        }\n        )\n\n        # Properties:\n        #\n        # DataGatherer.numrows\n        # ConnectedDrivingCleaner.x_pos\n        # ConnectedDrivingCleaner.y_pos\n        # ConnectedDrivingLargeDataCleaner.max_dist\n        # ConnectedDrivingLargeDataCleaner.cleanFunc\n        # ConnectedDrivingLargeDataCleaner.filterFunc\n        # ConnectedDrivingAttacker.SEED\n        # ConnectedDrivingCleaner.isXYCoords\n        # ConnectedDrivingAttacker.attack_ratio\n        # ConnectedDrivingCleaner.cleanParams\n        #\n\n        # Cleaned columns are added/modified after these columns are used for filtering\n        COLUMNS=[\"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n            \"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n            \"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n            #  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n            #  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n            \"coreData_id\", \"coreData_secMark\", \"coreData_position_lat\", \"coreData_position_long\",\n            \"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n            \"coreData_elevation\", \"coreData_accelset_accelYaw\",\"coreData_speed\", \"coreData_heading\", \"coreData_position\"]\n\n\n        x_pos = -105.1159611\n        y_pos = 41.0982327\n        x_pos_str = MathHelper.convertNumToTitleStr(x_pos)\n        y_pos_str = MathHelper.convertNumToTitleStr(y_pos)\n        self.generatorContextProvider = GeneratorContextProvider(contexts={\n            \"DataGatherer.numrows\": numSubsectionRows,\n            \"DataGatherer.lines_per_file\": 1000000,\n            \"ConnectedDrivingCleaner.x_pos\": -105.1159611,\n            \"ConnectedDrivingCleaner.y_pos\": 41.0982327,\n            \"ConnectedDrivingCleaner.columns\": COLUMNS,\n            \"ConnectedDrivingLargeDataCleaner.max_dist\": 500,\n            \"ConnectedDrivingCleaner.shouldGatherAutomatically\": False,\n            \"ConnectedDrivingLargeDataCleaner.cleanerClass\": CleanWithTimestamps,\n            \"ConnectedDrivingLargeDataCleaner.cleanFunc\": CleanWithTimestamps.clean_data_with_timestamps,\n            \"ConnectedDrivingLargeDataCleaner.cleanerWithFilterClass\": CleanerWithFilterWithinRangeXY,\n            \"ConnectedDrivingLargeDataCleaner.filterFunc\": CleanerWithFilterWithinRangeXY.within_rangeXY,\n            \"ConnectedDrivingAttacker.SEED\": 42,\n            \"ConnectedDrivingCleaner.isXYCoords\": True,\n            \"ConnectedDrivingAttacker.attack_ratio\": 0.3,\n            \"ConnectedDrivingCleaner.cleanParams\": f\"clean_data_with_timestamps-within_rangeXY-WithXYCoords-1000mdist-x{x_pos_str}y{y_pos_str}dd02mm04yyyy2021\", # makes cached data have info on if/if not we use timestamps for uniqueness\n\n        }\n        )\n\n        # Properties:\n        #\n        # MConnectedDrivingDataCleaner.columns\n        # MClassifierPipeline.classifier_instances # AUTO_FILLED\n        # MClassifierPipeline.csvWriter\n        #\n        self.MLContextProvider = MLContextProvider(contexts={\n            \"MConnectedDrivingDataCleaner.columns\": [\n            # \"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n            #  \"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n            #  \"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n            #  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n            #  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n            \"coreData_id\",  # \"coreData_position_lat\", \"coreData_position_long\",\n            \"coreData_secMark\", \"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n            \"month\", \"day\", \"year\", \"hour\", \"minute\", \"second\", \"pm\",\n            \"coreData_elevation\", \"coreData_accelset_accelYaw\", \"coreData_speed\", \"coreData_heading\", \"x_pos\", \"y_pos\", \"isAttacker\"],\n\n            # \"MClassifierPipeline.classifier_instances\": [...] # AUTO_FILLED\n            \"MClassifierPipeline.csvWriter\": CSVWriter(f\"{LOG_NAME}.csv\", CSV_COLUMNS),\n\n        }\n        )\n\n        ######### END OF CONFIG FOR ALL PROPERTIES IN THE PIPELINE ##################################################\n\n        self.logger = Logger(LOG_NAME)\n        self.csvWriter = CSVWriter(f\"{LOG_NAME}.csv\", CSV_COLUMNS)\n\n    def write_entire_row(self, dict):\n        row = [\" \"]*len(CSV_COLUMNS)\n        # Writing each variable to the row\n        for d in dict:\n            row[CSV_FORMAT[d]] = dict[d]\n\n        self.csvWriter.addRow(row)\n</code></pre>"},{"location":"Getting%20Started/development.html#classifier-models","title":"Classifier Models","text":"<p>You'll notice that we default to using the RandomForestClassifier, DecisionTreeClassifier and KNeighborsClassifier models for the MClassifierPipeline. This is because these models are the most accurate for our data. However, you can change this to use any model you want. You can also change the parameters for each model. For example, if you want to use a different number of estimators for the RandomForestClassifier, you can change the parameters to be:</p> <pre><code>CLASSIFIER_INSTANCES = [\n    RandomForestClassifier(n_estimators=200), # 100 is default\n    DecisionTreeClassifier(),\n    KNeighborsClassifier()\n]\n</code></pre> <p>However, make sure to include the <code>classifier_instances</code> property in the MLContextProvider and set it to <code>CLASSIFIER_INSTANCES</code></p>"},{"location":"Getting%20Started/development.html#log-name","title":"Log Name","text":"<p>The log name is very important because it ensures that the caching used in the machine learning part of the pipeline works correctly. It also acts as the log folder name for our logs.</p>"},{"location":"Getting%20Started/development.html#csv-columns","title":"CSV Columns","text":"<p>These columns act as our CSV headers for our results file. You won't need to change these unless you plan to modify the way we write our results to the CSV file.</p>"},{"location":"Getting%20Started/development.html#csv-format","title":"CSV Format","text":"<p>This is a dictionary that maps the CSV column names to their index in the CSV file. You won't need to change these unless you plan to modify the way we write our results to the CSV file.</p>"},{"location":"Getting%20Started/development.html#pathprovider","title":"PathProvider","text":"<p>You'll notice that the first part we configure within the init method is the PathProvider. We use this to ensure that our logs are written to the correct folder. You won't need to change this unless you plan to modify the way we write our logs. Make sure that the LOG_NAME is unique to each pipeline user file.</p> <pre><code># used for the logger's path\nself._pathprovider = PathProvider(model=LOG_NAME, contexts={\n        \"Logger.logpath\": DEFAULT_LOG_PATH,\n})\n</code></pre>"},{"location":"Getting%20Started/development.html#initialgathererpathprovider","title":"InitialGathererPathProvider","text":"<p>The paths in the InitialGathererPathProvider are used to gather the data from the initial data source. You'll need to change these to match the paths to your data. The model name should be shared among all similar initial datasets (i.e. all datasets that are gathered from the same source).</p> <pre><code>initialGathererModelName = \"CreatingConnectedDrivingDataset\"\n# Properties:\n# DataGatherer.filepath\n# DataGatherer.subsectionpath\n# DataGatherer.splitfilespath\n# DataGatherer.lines_per_file\nself._initialGathererPathProvider = InitialGathererPathProvider(model=initialGathererModelName, contexts={\n    \"DataGatherer.filepath\": lambda model: \"data/data.csv\",\n    \"DataGatherer.subsectionpath\": lambda model: f\"data/classifierdata/subsection/{model}/subsection{numSubsectionRows}.csv\",\n    \"DataGatherer.splitfilespath\": lambda model: f\"data/classifierdata/splitfiles/{model}/\",\n}\n)\n</code></pre>"},{"location":"Getting%20Started/development.html#generatorpathprovider","title":"GeneratorPathProvider","text":"<p>The paths in the GeneratorPathProvider are used to generate the data for the machine learning part of the pipeline. You'll need to change these to match the paths to your generated data. Make sure that the model name is unique to the cleaning/filtering options. For example, it should include whether or not we included timestamps, XY COORDS, etc. and also the distance of the filter (default 500m)</p> <pre><code># Properties:\n#\n# ConnectedDrivingLargeDataCleaner.cleanedfilespath\n# ConnectedDrivingLargeDataCleaner.combinedcleandatapath\n# MConnectedDrivingLargeDataCleaner.dtypes # AUTO_FILLED\n#\n# MAKE SURE TO CHANGE THE MODEL NAME TO THE PROPER NAME (IE A NAME THAT MATCHES IF\n# IT HAS TIMESTAMPS OR NOT, AND IF IT HAS XY COORDS OR NOT, ETC)\nself._generatorPathProvider = GeneratorPathProvider(model=f\"{initialGathererModelName}-CCDDWithTimestampsAndWithXYCoords-500mdist\", contexts={\n    \"ConnectedDrivingLargeDataCleaner.cleanedfilespath\": lambda model:  f\"data/classifierdata/splitfiles/cleaned/{model}/\",\n    \"ConnectedDrivingLargeDataCleaner.combinedcleandatapath\": lambda model: f\"data/classifierdata/splitfiles/combinedcleaned/{model}/combinedcleaned\",\n}\n)\n</code></pre>"},{"location":"Getting%20Started/development.html#mlpathprovider","title":"MLPathProvider","text":"<p>The paths in the MLPathProvider are used to run the machine learning part of the pipeline. You'll need to change these to match the paths to your ML data. Make sure that the model name is unique to the ML options and cleaning options. The best way to do this is to make your file name unique and set the model as the file name (A.K.A. the LOG_NAME)</p> <pre><code># Properties:\n#\n# MConnectedDrivingDataCleaner.cleandatapath\n# MDataClassifier.plot_confusion_matrix_path\n#\nself._mlPathProvider = MLPathProvider(model=LOG_NAME, contexts={\n    \"MConnectedDrivingDataCleaner.cleandatapath\": lambda model: f\"data/mclassifierdata/cleaned/{model}/clean.csv\",\n    \"MDataClassifier.plot_confusion_matrix_path\": lambda model: f\"data/mclassifierdata/results/{model}/\",\n}\n)\n</code></pre>"},{"location":"Getting%20Started/development.html#columns","title":"COLUMNS","text":"<p>The COLUMNS variable is used to specify the columns that we want to use for our initial cleaning and gathering of data. You'll need to change these to match the columns in your data if they are different.</p> <pre><code># Cleaned columns are added/modified after these columns are used for filtering\nCOLUMNS=[\"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n    \"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n    \"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n    #  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n    #  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n    \"coreData_id\", \"coreData_secMark\", \"coreData_position_lat\", \"coreData_position_long\",\n    \"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n    \"coreData_elevation\", \"coreData_accelset_accelYaw\",\"coreData_speed\", \"coreData_heading\", \"coreData_position\"]\n</code></pre>"},{"location":"Getting%20Started/development.html#generatorcontextprovider","title":"GeneratorContextProvider","text":"<p>The GeneratorContextProvider is used to provide the contexts for the generation part of the pipeline (including the initial gathering part). You'll need to change these to match configurations for your data. Make sure to change the cleanParams to match the cleaning options you want to use. Make sure that the cleanParams is unique to the cleaning/filtering options. For example, it should include whether or not we included timestamps, XY COORDS, etc. and also the distance of the filter.</p> <pre><code># Properties:\n#\n# DataGatherer.numrows\n# ConnectedDrivingCleaner.x_pos\n# ConnectedDrivingCleaner.y_pos\n# ConnectedDrivingLargeDataCleaner.max_dist\n# ConnectedDrivingLargeDataCleaner.cleanFunc\n# ConnectedDrivingLargeDataCleaner.filterFunc\n# ConnectedDrivingAttacker.SEED\n# ConnectedDrivingCleaner.isXYCoords\n# ConnectedDrivingAttacker.attack_ratio\n# ConnectedDrivingCleaner.cleanParams\n#\n\n# XY columns are added after these columns are used for filtering\nCOLUMNS=[\"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n    \"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n    \"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n    #  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n    #  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n    \"coreData_id\", \"coreData_secMark\", \"coreData_position_lat\", \"coreData_position_long\",\n    \"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n    \"coreData_elevation\", \"coreData_accelset_accelYaw\",\"coreData_speed\", \"coreData_heading\", \"coreData_position\"]\n\n\nx_pos = -105.1159611\ny_pos = 41.0982327\nx_pos_str = MathHelper.convertNumToTitleStr(x_pos)\ny_pos_str = MathHelper.convertNumToTitleStr(y_pos)\nself.generatorContextProvider = GeneratorContextProvider(contexts={\n    \"DataGatherer.numrows\": numSubsectionRows,\n    \"DataGatherer.lines_per_file\": 1000000,\n    \"ConnectedDrivingCleaner.x_pos\": -105.1159611,\n    \"ConnectedDrivingCleaner.y_pos\": 41.0982327,\n    \"ConnectedDrivingCleaner.columns\": COLUMNS,\n    \"ConnectedDrivingLargeDataCleaner.max_dist\": 500,\n    \"ConnectedDrivingCleaner.shouldGatherAutomatically\": False,\n    \"ConnectedDrivingLargeDataCleaner.cleanerClass\": CleanWithTimestamps,\n    \"ConnectedDrivingLargeDataCleaner.cleanFunc\": CleanWithTimestamps.clean_data_with_timestamps,\n    \"ConnectedDrivingLargeDataCleaner.cleanerWithFilterClass\": CleanerWithFilterWithinRangeXY,\n    \"ConnectedDrivingLargeDataCleaner.filterFunc\": CleanerWithFilterWithinRangeXY.within_rangeXY,\n    \"ConnectedDrivingAttacker.SEED\": 42,\n    \"ConnectedDrivingCleaner.isXYCoords\": True,\n    \"ConnectedDrivingAttacker.attack_ratio\": 0.3,\n    \"ConnectedDrivingCleaner.cleanParams\": f\"clean_data_with_timestamps-within_rangeXY-WithXYCoords-1000mdist-x{x_pos_str}y{y_pos_str}dd02mm04yyyy2021\", # makes cached data have info on if/if not we use timestamps for uniqueness\n\n}\n)\n</code></pre>"},{"location":"Getting%20Started/development.html#mlcontextprovider","title":"MLContextProvider","text":"<p>The MLContextProvider is used to provide the contexts for the machine learning part of the pipeline. You'll need to change these to match configurations for your ML data.</p> <pre><code># Properties:\n#\n# MConnectedDrivingDataCleaner.columns\n# MClassifierPipeline.classifier_instances # AUTO_FILLED\n# MClassifierPipeline.csvWriter\n#\nself.MLContextProvider = MLContextProvider(contexts={\n    \"MConnectedDrivingDataCleaner.columns\": [\n    # \"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n    #  \"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n    #  \"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n    #  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n    #  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n    \"coreData_id\",  # \"coreData_position_lat\", \"coreData_position_long\",\n    \"coreData_secMark\", \"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n    \"month\", \"day\", \"year\", \"hour\", \"minute\", \"second\", \"pm\",\n    \"coreData_elevation\", \"coreData_accelset_accelYaw\", \"coreData_speed\", \"coreData_heading\", \"x_pos\", \"y_pos\", \"isAttacker\"],\n\n    # \"MClassifierPipeline.classifier_instances\": [...] # AUTO_FILLED\n    \"MClassifierPipeline.csvWriter\": CSVWriter(f\"{LOG_NAME}.csv\", CSV_COLUMNS),\n\n}\n)\n</code></pre>"},{"location":"Getting%20Started/development.html#logger-and-csv-writer","title":"Logger and CSV Writer","text":"<p>The logger and CSV writer are used to log the results of the pipeline. You won't need to change these.</p> <pre><code># Goes after the config ...\n\nself.logger = Logger(LOG_NAME)\nself.csvWriter = self.MLContextProvider.get(\"MClassifierPipeline.csvWriter\")\n</code></pre>"},{"location":"Getting%20Started/development.html#write_entire_row-function","title":"write_entire_row function","text":"<p>The write_entire_row function is used to write the results of the pipeline to a CSV file. You won't need to change this.</p> <pre><code>def write_entire_row(self, dict):\n    row = [\" \"]*len(CSV_COLUMNS)\n    # Writing each variable to the row\n    for d in dict:\n        row[CSV_FORMAT[d]] = dict[d]\n\n    self.csvWriter.addRow(row)\n</code></pre>"},{"location":"Getting%20Started/development.html#creating-the-run-function","title":"Creating The <code>run</code> Function","text":"<p>The <code>run</code> function is the main function of the pipeline. It is called when the pipeline is run. You'll need to change this to match your pipeline.</p>"},{"location":"Getting%20Started/development.html#an-example-run-function","title":"An Example <code>run</code> Function","text":"<p>The following is the run function for <code>MClassifierLargePipelineUserWithXYOffsetPos500mDist100kRowsEXTTimestampsCols30attackersRandOffset100To200</code></p> <pre><code>def run(self):\n\n    mcdldpgac = ConnectedDrivingLargeDataPipelineGathererAndCleaner().run()\n\n    data: DataFrame = mcdldpgac.getNRows(200000)\n\n    # splitting into train and test sets\n    train = data.iloc[:100000].copy()\n    test = data.iloc[100000:200000].copy()\n\n    # Note you can randomize the sampling as follows:\n    # # splitting into train and test sets\n    # seed = self.generatorContextProvider.get(\"ConnectedDrivingAttacker.SEED\")\n    # train, test = train_test_split(data, test_size=0.2, random_state=seed)\n\n    # cleaning/adding attackers to the data\n    train = StandardPositionalOffsetAttacker(train, \"train\").add_attackers().add_attacks_positional_offset_rand(min_dist=100, max_dist=200).get_data()\n    test = StandardPositionalOffsetAttacker(test, \"test\").add_attackers().add_attacks_positional_offset_rand(min_dist=100, max_dist=200).get_data()\n\n\n\n    # Cleaning it for the malicious data detection\n    mdcleaner_train = MConnectedDrivingDataCleaner(train, \"train\")\n    mdcleaner_test = MConnectedDrivingDataCleaner(test, \"test\")\n    m_train = mdcleaner_train.clean_data().get_cleaned_data()\n    m_test = mdcleaner_test.clean_data().get_cleaned_data()\n\n    # splitting into features and the labels\n    attacker_col_name = \"isAttacker\"\n    train_X = m_train.drop(columns=[attacker_col_name], axis=1)\n    train_Y = m_train[attacker_col_name]\n    test_X = m_test.drop(columns=[attacker_col_name], axis=1)\n    test_Y = m_test[attacker_col_name]\n\n    # training the classifiers\n    mcp = MClassifierPipeline(train_X, train_Y, test_X, test_Y)\n\n    mcp.train()\n    mcp.test()\n\n    # getting the results\n    results = mcp.calc_classifier_results().get_classifier_results()\n\n    # printing the results\n    for mclassifier, train_result, result in results:\n        mcp.logger.log(mclassifier)\n        mcp.logger.log(\"Train Set Results:\")\n        mcp.logger.log(\"Accuracy: \", train_result[0])\n        mcp.logger.log(\"Precision: \", train_result[1])\n        mcp.logger.log(\"Recall: \", train_result[2])\n        mcp.logger.log(\"F1: \", train_result[3])\n        mcp.logger.log(\"Test Set Results:\")\n        mcp.logger.log(\"Accuracy: \", result[0])\n        mcp.logger.log(\"Precision: \", result[1])\n        mcp.logger.log(\"Recall: \", result[2])\n        mcp.logger.log(\"F1: \", result[3])\n        # printing the elapsed training and prediction time\n        mcp.logger.log(\"Elapsed Training Time: \",\n                        mclassifier.elapsed_train_time)\n        mcp.logger.log(\"Elapsed Prediction Time: \",\n                        mclassifier.elapsed_prediction_time)\n\n        mcp.logger.log(\"Writing to CSV...\")\n\n        # writing entire row to csv\n        # columns: \"Model\", \"Total_Train_Time\",\n        #    \"Total_Train_Sample_Size\", \"Total_Test_Sample_Size\", \"Train_Time_Per_Sample\", \"Prediction_Train_Set_Time_Per_Sample\", \"Prediction_Test_Set_Time_Per_Sample\",\n        #    \"train_accuracy\", \"train_precision\", \"train_recall\", \"train_f1\",\n        #    \"test_accuracy\", \"test_precision\", \"test_recall\", \"test_f1\"\n\n        csvrowdata = {\n            \"Model\": mclassifier.classifier.__class__.__name__,\n            \"Total_Train_Time\": mclassifier.elapsed_train_time,\n            # train and test have the same number of samples\n            \"Total_Train_Sample_Size\": len(train_X),\n            # train and test have the same number of samples\n            \"Total_Test_Sample_Size\": len(test_X),\n            \"Train_Time_Per_Sample\": mclassifier.elapsed_train_time/len(train_X),\n            \"Prediction_Train_Set_Time_Per_Sample\": mclassifier.elapsed_prediction_train_time/len(train_X),\n            \"Prediction_Test_Set_Time_Per_Sample\": mclassifier.elapsed_prediction_time/len(test_X),\n            \"train_accuracy\": train_result[0],\n            \"train_precision\": train_result[1],\n            \"train_recall\": train_result[2],\n            \"train_f1\": train_result[3],\n            \"test_accuracy\": result[0],\n            \"test_precision\": result[1],\n            \"test_recall\": result[2],\n            \"test_f1\": result[3]}\n        self.write_entire_row(csvrowdata)\n\n    # calculating confusion matrices and storing them\n    mcp.logger.log(\"Calculating confusion matrices and storing...\")\n    mcp.calculate_classifiers_and_confusion_matrices().plot_confusion_matrices()\n</code></pre>"},{"location":"Getting%20Started/development.html#explaining-how-the-run-function-works","title":"Explaining How The <code>run</code> Function Works","text":"<p>The <code>run</code> function acts as your main method for your pipeline.</p> <ol> <li>You'll notice that this pipeline uses the <code>ConnectedDrivingLargeDataPipelineGathererAndCleaner</code> class. It acts as a gatherer and cleaner for the data. It is used to split the data from the CSV files, clean them, combine them into one DataFrame. We can call the <code>run</code> function of the <code>ConnectedDrivingLargeDataPipelineGathererAndCleaner</code> class to get the data. We can then use the <code>getNRows</code> function to get the first 200 000 rows of the data.</li> </ol> <pre><code>mcdldpgac = ConnectedDrivingLargeDataPipelineGathererAndCleaner().run()\n\ndata: DataFrame = mcdldpgac.getNRows(200000)\n</code></pre> <ol> <li>We then split the data into a train and test set.</li> </ol> <pre><code># splitting into train and test sets\ntrain = data.iloc[:100000].copy()\ntest = data.iloc[100000:200000].copy()\n# Note you could also randomize the sampling as follows:\nseed = self.generatorContextProvider.get(\"ConnectedDrivingAttacker.SEED\")\ntrain, test = train_test_split(data, test_size=0.2, random_state=seed)\n</code></pre> <ol> <li>Adding attackers to the data.</li> </ol> <pre><code># cleaning/adding attackers to the data\ntrain = StandardPositionalOffsetAttacker(train, \"train\").add_attackers().add_attacks_positional_offset_rand(min_dist=100, max_dist=200).get_data()\ntest = StandardPositionalOffsetAttacker(test, \"test\").add_attackers().add_attacks_positional_offset_rand(min_dist=100, max_dist=200).get_data()\n</code></pre> <ol> <li>Cleaning the data using the <code>MConnectedDrivingDataCleaner</code> class.</li> </ol> <pre><code> # Cleaning it for the malicious data detection\nmdcleaner_train = MConnectedDrivingDataCleaner(train, \"train\")\nmdcleaner_test = MConnectedDrivingDataCleaner(test, \"test\")\nm_train = mdcleaner_train.clean_data().get_cleaned_data()\nm_test = mdcleaner_test.clean_data().get_cleaned_data()\n</code></pre> <ol> <li>Splitting the data into features and labels.</li> </ol> <pre><code>attacker_col_name = \"isAttacker\"\ntrain_X = m_train.drop(columns=[attacker_col_name], axis=1)\ntrain_Y = m_train[attacker_col_name]\ntest_X = m_test.drop(columns=[attacker_col_name], axis=1)\ntest_Y = m_test[attacker_col_name]\n</code></pre> <ol> <li>Training the classifiers using the <code>MClassifierPipeline</code> class.</li> </ol> <pre><code>mcp = MClassifierPipeline(train_X, train_Y, test_X, test_Y)\n\nmcp.train()\nmcp.test()\n</code></pre> <ol> <li>Getting the results of the classifiers.</li> </ol> <pre><code>results = mcp.calc_classifier_results().get_classifier_results()\n</code></pre> <ol> <li>Printing the results of the classifiers.</li> </ol> <pre><code># printing the results\nfor mclassifier, train_result, result in results:\n    mcp.logger.log(mclassifier)\n    mcp.logger.log(\"Train Set Results:\")\n    mcp.logger.log(\"Accuracy: \", train_result[0])\n    mcp.logger.log(\"Precision: \", train_result[1])\n    mcp.logger.log(\"Recall: \", train_result[2])\n    mcp.logger.log(\"F1: \", train_result[3])\n    mcp.logger.log(\"Test Set Results:\")\n    mcp.logger.log(\"Accuracy: \", result[0])\n    mcp.logger.log(\"Precision: \", result[1])\n    mcp.logger.log(\"Recall: \", result[2])\n    mcp.logger.log(\"F1: \", result[3])\n    # printing the elapsed training and prediction time\n    mcp.logger.log(\"Elapsed Training Time: \",\n                    mclassifier.elapsed_train_time)\n    mcp.logger.log(\"Elapsed Prediction Time: \",\n                    mclassifier.elapsed_prediction_time)\n\n    mcp.logger.log(\"Writing to CSV...\")\n\n    # writing entire row to csv\n    # columns: \"Model\", \"Total_Train_Time\",\n    #    \"Total_Train_Sample_Size\", \"Total_Test_Sample_Size\", \"Train_Time_Per_Sample\", \"Prediction_Train_Set_Time_Per_Sample\", \"Prediction_Test_Set_Time_Per_Sample\",\n    #    \"train_accuracy\", \"train_precision\", \"train_recall\", \"train_f1\",\n    #    \"test_accuracy\", \"test_precision\", \"test_recall\", \"test_f1\"\n\n    csvrowdata = {\n        \"Model\": mclassifier.classifier.__class__.__name__,\n        \"Total_Train_Time\": mclassifier.elapsed_train_time,\n        # train and test have the same number of samples\n        \"Total_Train_Sample_Size\": len(train_X),\n        # train and test have the same number of samples\n        \"Total_Test_Sample_Size\": len(test_X),\n        \"Train_Time_Per_Sample\": mclassifier.elapsed_train_time/len(train_X),\n        \"Prediction_Train_Set_Time_Per_Sample\": mclassifier.elapsed_prediction_train_time/len(train_X),\n        \"Prediction_Test_Set_Time_Per_Sample\": mclassifier.elapsed_prediction_time/len(test_X),\n        \"train_accuracy\": train_result[0],\n        \"train_precision\": train_result[1],\n        \"train_recall\": train_result[2],\n        \"train_f1\": train_result[3],\n        \"test_accuracy\": result[0],\n        \"test_precision\": result[1],\n        \"test_recall\": result[2],\n        \"test_f1\": result[3]}\n    self.write_entire_row(csvrowdata)\n</code></pre> <ol> <li>Calculating the confusion matrices and storing them. Note that this step also saved the matrices in your results file as base64 images.</li> </ol> <pre><code># calculating the confusion matrices\nmcp.calculate_classifiers_and_confusion_matrices().plot_confusion_matrices()\n</code></pre>"},{"location":"Getting%20Started/development.html#running-the-pipeline","title":"Running The Pipeline","text":"<p>To run the pipeline, you can run the following command in the terminal:</p> <p><pre><code>python &lt;your python file&gt;.py\n</code></pre> If you haven't already, make sure to activate your virtual environment before running the command. See the Setup page for more details.</p>"},{"location":"Getting%20Started/development.html#running-on-the-super-computer","title":"Running On The Super Computer","text":"<p>To run on the super computer, register with compute canada. Next, follow the instructions on the Setup page to set up your virtual environment. You'll likely want to use the beluga super computer (i.e. @beluga.computecanada.ca). You may want to use --no-index when installing requirements to avoid downloading packages from the internet. <p>Finally, you can run the following command to submit a job to the super computer:</p> <pre><code>./runUserPipeline.sh &lt;USERNAME&gt; &lt;PATH_TO_REPO (not ending in slash)&gt; &lt;FILE&gt; &lt;DAYS&gt; &lt;HOURS&gt; &lt;MINUTES&gt; &lt;CPUS&gt; &lt;MEM&gt; [OPTIONAL: DEPENDENCY]\n</code></pre> <p>Optionally, you can use the defaultrunnerconfig.sh file to use default values:</p> <pre><code>./defaultrunnerconfig.sh &lt;FILE&gt; &lt;USERNAME&gt; [OPTIONAL: DEPENDENCY]\n</code></pre> <p>If you decide to run the commands directly from the super computer, you can generate a slurm file using the following command:</p> <pre><code>python3 generateSlurm.py &lt;fileName&gt; &lt;days&gt; &lt;hours&gt; &lt;minutes&gt; &lt;cpus&gt; &lt;memory(G)&gt; [dependency]\n</code></pre> <p>Or you can create the slurm file yourself and submit it as a job using the following command:</p> <pre><code>sbatch &lt;fileName&gt;\n</code></pre>"},{"location":"Getting%20Started/setup.html","title":"Setup","text":""},{"location":"Getting%20Started/setup.html#requirements","title":"Requirements","text":"<ul> <li>A Windows, Linux or Mac operating system (note that you may need to adjust python to python3, etc. when installing on Mac or Linux)</li> <li>Python 3.10 since Python 3.11 isn't fully supported by all ML libraries</li> <li>Docker (If you intend to run the docs locally)</li> <li>Pip (Installed with Python3)</li> <li>Git</li> </ul>"},{"location":"Getting%20Started/setup.html#install","title":"Install","text":"<ol> <li>Install the requirements listed above.</li> <li>clone the github repo by running the following in your git bash terminal (note that on the super computer you should do this within the projects directory): <pre><code>git clone https://github.com/aaron777collins/ConnectedDrivingPipelineV4.git\n</code></pre></li> <li>cd into the folder by typing: <pre><code>cd ConnectedDrivingPipelineV4\n</code></pre> Note that all your future commands will be expected to start from this directory.</li> <li>Create a folder called <code>data</code> <pre><code>mkdir data\n</code></pre></li> <li> <p>Download the data from here and name it <code>data.csv</code> within the <code>data</code> folder.</p> <p>Optionally: You can download wget.exe from https://eternallybored.org/misc/wget/ and put it in your git bash directory (The default Windows install directory is <code>C:\\Program Files\\Git\\mingw64\\bin</code>). Next, run the following in git bash (within the project directory) to download the file: <pre><code>while [ 1 ]; do\n    wget --retry-connrefused --retry-on-http-error=500 --waitretry=1 --read-timeout=20 --timeout=15 -t 0 --continue -O data/data.csv https://data.transportation.gov/api/views/9k4m-a3jc/rows.csv?accessType=DOWNLOAD\n    if [ $? = 0 ]; then break; fi; # check return value, break if successful (0)\n    echo `error downloading. Trying again!`\n    sleep 1s;\ndone;\n</code></pre></p> </li> <li> <p>Create a virtual environment to install the required modules     <pre><code>python -m venv venv\n</code></pre>     and then activate your instance by running     <pre><code>./venv/Scripts/activate\n</code></pre>     on Windows. For other OS' consult the python docs here</p> <p>Make sure to always activate your venv before running anything in this pipeline</p> </li> <li> <p>Install the required modules (note that on the super computer you can use --no-index to install from the local cache)</p> <pre><code>pip install -r requirements.txt\n</code></pre> </li> </ol> <p>You're Finished Installing! Head over to the Development page to learn how to make your first pipeline user.</p>"}]}