var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#connecteddrivingpipelinev4","title":"ConnectedDrivingPipelineV4","text":"<p>This pipeline was created to end the continuous cycle of making maching learning (ML) pipelines.</p> <p>This version of the pipeline was created for connected driving research. The goal is to create a better, more realistic dataset for training malicious bsm detection systems in the connected driving space. The current datasets, such as Veremi and Veremi Extension are too easy to train detection models for and don't model the world realistically.</p> <p>We took the Wyoming CV Pilot Basic Safety Message One Day Sample dataset from OpenDataNetwork as our original, real data.</p> <p>This pipeline creates malicious datasets based on customizable attacks and then tests them using machine learning models such as Random Forest, Decision Tree and K-Nearest-Neighbours.</p> <p>Head to Setup to get started.</p>"},{"location":"index.html#links","title":"Links","text":"<ul> <li>Setup to get started.</li> <li>Development for developing a pipeline user file.</li> <li>Repo for the project repository.</li> <li>Other Projects that I have created.</li> </ul>"},{"location":"about.html","title":"Author","text":"<p>This pipeline was created  me, Aaron Collins. You can learn more about me at https://aaroncollins.info.</p>"},{"location":"results.html","title":"Results","text":"Loading Results... In Progress Jobs Our Current Results"},{"location":"Getting%20Started/Configuration%20Parameters.html","title":"Configuration Parameters","text":"<p>There's a lot of configuration parameters that can be used to customize the behavior of the application. The following tables lists all the available parameters and their default values for each respective provider.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#pathprovider","title":"PathProvider","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#loggerlogpath","title":"Logger.logpath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value","title":"Recommended Value","text":"<p><code>DEFAULT_LOG_PATH</code></p> <p>Get the default log path by importing it from the Logger module</p> <pre><code>from Logger.Logger import DEFAULT_LOG_PATH\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description","title":"Description","text":"<p>The path to the log folder. The log folder will contain all the log files generated by the pipeline user file.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#initialgathererpathprovider","title":"InitialGathererPathProvider","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#datagathererfilepath","title":"DataGatherer.filepath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_1","title":"Recommended Value","text":"<pre><code>lambda model: \"data/data.csv\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_1","title":"Description","text":"<p>The path function to the file containing the data to be used for the initial gathering of data. The file should be a csv file.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#datagatherersubsectionpath","title":"DataGatherer.subsectionpath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_2","title":"Recommended Value","text":"<pre><code>lambda model: f\"data/classifierdata/subsection/{model}/subsection{numSubsectionRows}.csv\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_2","title":"Description","text":"<p>The path function to the file containing the subsection of the data to be used for the initial gathering of data. The file should be a csv file. This file is used to get a subsection of the massive data file to be used for the initial gathering of data. This parameter is used in both the regular (non-large) pipeline and the large pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#datagatherersplitfilespath","title":"DataGatherer.splitfilespath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_3","title":"Recommended Value","text":"<pre><code>lambda model: f\"data/classifierdata/splitfiles/{model}/\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_3","title":"Description","text":"<p>The path function to the folder containing the split files of the data. The split files will be individually cleaned and then merged later. This folder is only used in the large pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#generatorpathprovider","title":"GeneratorPathProvider","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanercleanedfilespath","title":"ConnectedDrivingLargeDataCleaner.cleanedfilespath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_4","title":"Recommended Value","text":"<pre><code>lambda model:  f\"data/classifierdata/splitfiles/cleaned/{model}/\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_4","title":"Description","text":"<p>The path function to the folder containing the cleaned split files of the data. The split files will be individually cleaned and then stored in this folder. This folder is only used in the large pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanercombinedcleandatapath","title":"ConnectedDrivingLargeDataCleaner.combinedcleandatapath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_5","title":"Recommended Value","text":"<pre><code>lambda model: f\"data/classifierdata/splitfiles/combinedcleaned/{model}/combinedcleaned\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_5","title":"Description","text":"<p>The path function to the file containing the combined cleaned data. This file is only used in the large pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#mlpathprovider","title":"MLPathProvider","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#mconnecteddrivingdatacleanercleandatapath","title":"MConnectedDrivingDataCleaner.cleandatapath","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_6","title":"Recommended Value","text":"<pre><code>lambda model: f\"data/mclassifierdata/cleaned/{model}/clean.csv\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_6","title":"Description","text":"<p>The path function to the file containing the attacked and cleaned data. This file is used in every pipeline to get the data to be used for training and testing.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#mdataclassifierplot_confusion_matrix_path","title":"MDataClassifier.plot_confusion_matrix_path","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_7","title":"Recommended Value","text":"<pre><code>lambda model: f\"data/mclassifierdata/results/{model}/\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_7","title":"Description","text":"<p>The path function to the folder containing the confusion matrix plots. This folder is used to store the confusion matrix plots generated by  each classifier.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#generatorcontextprovider","title":"GeneratorContextProvider","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#datagatherernumrows","title":"DataGatherer.numrows","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_8","title":"Recommended Value","text":"<pre><code>100000\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_8","title":"Description","text":"<p>The number of rows to gather and store as a subsection from the original dataset. This parameter is used in both the regular (non-large) pipeline and the large pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#datagathererlines_per_file","title":"DataGatherer.lines_per_file","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_9","title":"Recommended Value","text":"<pre><code>1000000\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_9","title":"Description","text":"<p>The number of rows to store in each split file. This parameter is only used in the large pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingcleanerx_pos","title":"ConnectedDrivingCleaner.x_pos","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_10","title":"Recommended Value","text":"<pre><code>-105.1159611\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_10","title":"Description","text":"<p>The x coordinate of the center of the circle to be used for filtering the data. This parameter is only used in the large pipeline when filtering the data to be within a certain range.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingcleanery_pos","title":"ConnectedDrivingCleaner.y_pos","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_11","title":"Recommended Value","text":"<pre><code>41.0982327\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_11","title":"Description","text":"<p>The y coordinate of the center of the circle to be used for filtering the data. This parameter is only used in the large pipeline when filtering the data to be within a certain range.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingcleanercolumns","title":"ConnectedDrivingCleaner.columns","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_12","title":"Recommended Value","text":"<pre><code>[\"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n\"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n\"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n#  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n#  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n\"coreData_id\", \"coreData_secMark\", \"coreData_position_lat\", \"coreData_position_long\",\n\"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n\"coreData_elevation\", \"coreData_accelset_accelYaw\",\"coreData_speed\", \"coreData_heading\", \"coreData_position\"]\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_12","title":"Description","text":"<p>The columns to be used for filtering the initial data. Most of the columns are useless so these columns are the ones we are choosing to use in the pipeline.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanermax_dist","title":"ConnectedDrivingLargeDataCleaner.max_dist","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_13","title":"Recommended Value","text":"<pre><code>500\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_13","title":"Description","text":"<p>The maximum distance from the center of the circle to be used for filtering the data. This parameter is only used in the large pipeline when filtering the data to be within a certain range.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingcleanershouldgatherautomatically","title":"ConnectedDrivingCleaner.shouldGatherAutomatically","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_14","title":"Recommended Value","text":"<pre><code>False\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_14","title":"Description","text":"<p>Whether or not to automatically gather the data if the cleaner isn't given any data. The reason we set it off by default is to prevent data from being collected without the user's knowledge.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanercleanerclass","title":"ConnectedDrivingLargeDataCleaner.cleanerClass","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_15","title":"Recommended Value","text":"<pre><code>CleanWithTimestamps\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_15","title":"Description","text":"<p>The class to be used for cleaning the data. This parameter should match the class that the cleanFunc comes from.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanercleanfunc","title":"ConnectedDrivingLargeDataCleaner.cleanFunc","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_16","title":"Recommended Value","text":"<pre><code>CleanWithTimestamps.clean_data_with_timestamps\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_16","title":"Description","text":"<p>The function to be used for cleaning the data. This parameter should match the class that the cleanerClass comes from.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanercleanerwithfilterclass","title":"ConnectedDrivingLargeDataCleaner.cleanerWithFilterClass","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_17","title":"Recommended Value","text":"<pre><code>CleanerWithFilterWithinRangeXY\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_17","title":"Description","text":"<p>The class to be used for filtering the data. This parameter should match the class that the filterFunc comes from.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivinglargedatacleanerfilterfunc","title":"ConnectedDrivingLargeDataCleaner.filterFunc","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_18","title":"Recommended Value","text":"<pre><code>CleanerWithFilterWithinRangeXY.within_rangeXY\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_18","title":"Description","text":"<p>The function to be used for filtering the data. This parameter should match the class that the cleanerWithFilterClass comes from.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingattackerseed","title":"ConnectedDrivingAttacker.SEED","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_19","title":"Recommended Value","text":"<pre><code>42\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_19","title":"Description","text":"<p>The seed to be used for the random number generator. This parameter is only used in the large pipeline when generating the attack data.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingcleanerisxycoords","title":"ConnectedDrivingCleaner.isXYCoords","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_20","title":"Recommended Value","text":"<pre><code>True\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_20","title":"Description","text":"<p>Whether or not the data is in (x,y) coordinates (as distance from the point on the longitude, latitude axis respectively in meters).</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingattackerattack_ratio","title":"ConnectedDrivingAttacker.attack_ratio","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_21","title":"Recommended Value","text":"<pre><code>0.3\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_21","title":"Description","text":"<p>The ratio of the data to be attacked. For example, 0.3 = 30% of the data will be attacked. The attack ratio is used differently depending on the attack distribution method. For example, <code>add_attackers</code> will specify a ratio of the cars to be attackers and 100% of their BSMs will be attacked. On the other hand, <code>add_rand_attackers</code> will specify a ratio of the BSMs to be attacked randomly.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#connecteddrivingcleanercleanparams","title":"ConnectedDrivingCleaner.cleanParams","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_22","title":"Recommended Value","text":"<pre><code>f\"clean_data_with_timestamps-within_rangeXY-WithXYCoords-1000mdist-x{x_pos_str}y{y_pos_str}dd02mm04yyyy2021\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_22","title":"Description","text":"<p>The name of the parameters to be used for cleaning the data. This parameter is used in the caching and should match cleanFunc + any other parameters used such as the filter, etc.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#mlcontextprovider","title":"MLContextProvider","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#mconnecteddrivingdatacleanercolumns","title":"MConnectedDrivingDataCleaner.columns","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_23","title":"Recommended Value","text":"<pre><code> [\n# \"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n#  \"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n#  \"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n#  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n#  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n\"coreData_id\",  # \"coreData_position_lat\", \"coreData_position_long\",\n\"coreData_secMark\", \"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n\"month\", \"day\", \"year\", \"hour\", \"minute\", \"second\", \"pm\",\n\"coreData_elevation\", \"coreData_accelset_accelYaw\", \"coreData_speed\", \"coreData_heading\", \"x_pos\", \"y_pos\", \"isAttacker\"],\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_23","title":"Description","text":"<p>The columns to be used for training the model (and also the final columns after the attacker finishes).</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#mclassifierpipelineclassifier_instances","title":"MClassifierPipeline.classifier_instances","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value-auto_filled","title":"Recommended Value (AUTO_FILLED)","text":"<pre><code>[RandomForestClassifier(\n), DecisionTreeClassifier(), KNeighborsClassifier()]\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_24","title":"Description","text":"<p>The classifiers to be used for training the model. These are autofilled but we can change them if we want to use different classifiers. At the top of the example class on the development page, we specify the CLASSIFIER_INSTANCES variable to be used for the pipeline but we didn't include it in the config because it was autofilled. However, it would be easy to modify the array and pass it in. Make sure to include the modified parameters in your LOG_NAME and file name to avoid caching errors though!</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#cleanerwithfilterwithinrangexyanddayday","title":"CleanerWithFilterWithinRangeXYAndDay.day","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_24","title":"Recommended Value","text":"<pre><code>2\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_25","title":"Description","text":"<p>The day to be used for filtering the data. This parameter is only used in the large pipeline when filtering the data to be within a certain range.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#cleanerwithfilterwithinrangexyanddaymonth","title":"CleanerWithFilterWithinRangeXYAndDay.month","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_25","title":"Recommended Value","text":"<pre><code>4\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_26","title":"Description","text":"<p>The month to be used for filtering the data. This parameter is only used in the large pipeline when filtering the data to be within a certain range.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#cleanerwithfilterwithinrangexyanddayyear","title":"CleanerWithFilterWithinRangeXYAndDay.year","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_26","title":"Recommended Value","text":"<pre><code>2021\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_27","title":"Description","text":"<p>The year to be used for filtering the data. This parameter is only used in the large pipeline when filtering the data to be within a certain range.</p>"},{"location":"Getting%20Started/Configuration%20Parameters.html#mdataclassifierplot_distribution_path","title":"MDataClassifier.plot_distribution_path","text":""},{"location":"Getting%20Started/Configuration%20Parameters.html#recommended-value_27","title":"Recommended Value","text":"<pre><code>lambda model: f\"data/mclassifierdata/results/{model}/\"\n</code></pre>"},{"location":"Getting%20Started/Configuration%20Parameters.html#description_28","title":"Description","text":"<p>The path to be used for plotting the distribution of the data. This parameter is only used in the large pipeline when plotting the distribution of the data during feature analysis.</p>"},{"location":"Getting%20Started/development.html","title":"Development","text":"<p>Head to Setup if you haven't already setup the project.</p>"},{"location":"Getting%20Started/development.html#creating-a-pipeline-user-file","title":"Creating A Pipeline User File","text":""},{"location":"Getting%20Started/development.html#file-creation","title":"File Creation","text":"<p>Pipeline user files should always have a unique name that specifies what the file will execute and the parameters that make it unique.</p> <p>For example, look at the pipeline user file called <code>MClassifierLargePipelineUserWithXYOffsetPos500mDist100kRowsEXTTimestampsCols30attackersRandOffset100To200</code>. It runs the large pipeline with the following settings:</p> <ul> <li>WithXYOffsetPos: Tells us that it will use the XY coordinate system</li> <li>500mDist: This reminds us that we are using the 500m distance filter</li> <li>100kRows: This specifies that we are only using 100k rows to train our data</li> <li>EXTTimestampscols: This reminds us that we are cleaning our data to use the extended timestamp columns (which I've included in the file)</li> <li>30attackers: Specifies that we are using 30% attackers (and by default, it is splitting the cars such that 30% are attackers and then making 100% of the BSMs malicious)</li> <li>RandOffset100To200: Specifies that we are using the random offset attack with a min distance of 100m and a max distance of 200m.</li> </ul> <p>Pipeline user files should be placed in the base directory of the project. For example, the <code>MClassifierLargePipelineUserWithXYOffsetPos500mDist100kRowsEXTTimestampsCols30attackersRandOffset100To200</code> file is placed in the base directory of the project.</p>"},{"location":"Getting%20Started/development.html#providers","title":"Providers","text":"<p>Providers act as a way to configure the pipeline and set properties that can be accessed within any file. They are implemented as singletons so that they can be instantiated anywhere to access the properties.</p> <p>The most basic provider is the <code>DictProvider</code> and it provides a dictionary of key value pairs that can be accessed by the pipeline. For example, the following provider provides a dictionary with the key <code>test</code> and the value <code>testvalue</code>:</p> <pre><code>from ServiceProviders.DictProvider import DictProvider\nprovider = DictProvider(contexts={\"test\": \"testvalue\"})\n</code></pre> <p>which can be accessed by the pipeline like so:</p> <pre><code>from ServiceProviders.DictProvider import DictProvider\nprovider = DictProvider()\ntestVal = provider.get(\"test\")\nprint(testVal) # prints \"testvalue\"\n</code></pre> <p>However, since the DictProvider is a singleton, we run into trouble if we want to have multiple providers with different values for the same key. Thus, we created context providers for different parts of the pipeline. The ones we used are <code>GeneratorContextProvider</code> and <code>MLContextProvider</code> which are used for the generator and machine learning parts of the pipeline respectively. These providers are instantiated with a dictionary of key value pairs (similar to DictProvider). For example, the following provider provides a dictionary with the key <code>test</code> and the value <code>testvalue</code>:</p> <pre><code>from ServiceProviders.GeneratorContextProvider import GeneratorContextProvider\nfrom ServiceProviders.MLContextProvider import MLContextProvider\ngeneratorProvider = GeneratorContextProvider(contexts={\"test\": \"testvalue\"})\nmlProvider = MLContextProvider(contexts={\"test\": \"testvalue2\"})\n</code></pre> <p>which can be accessed by the pipeline like so:</p> <pre><code>from ServiceProviders.GeneratorContextProvider import GeneratorContextProvider\nfrom ServiceProviders.MLContextProvider import MLContextProvider\ngeneratorProvider = GeneratorContextProvider()\nmlProvider = MLContextProvider()\ntestVal = generatorProvider.get(\"test\")\ntestVal2 = mlProvider.get(\"test\")\nprint(testVal) # prints \"testvalue\"\nprint(testVal2) # prints \"testvalue2\"\n</code></pre> <p>ContextProviders are useful but we also ran into another issue with paths. We needed the paths for some parts of the pipeline to be unique and other parts to be shared. Thus, we created a variant of the <code>DictProvider</code> called the <code>PathProvider</code> that uses a model attribute to create a unique element of each value. The user provides a key and value with the key being a string and the value as a function. The function takes in a model name and returns the value. For example, the following <code>PathProvider</code> provides a dictionary with the key <code>test</code> and the value <code>{model}/testvalue</code> where model=<code>somemodel</code>:</p> <pre><code>from ServiceProviders.PathProvider import PathProvider\nprovider = PathProvider(model=\"somemodel\" contexts={\"test\": lambda model: f\"{model}/testvalue\"})\n</code></pre> <p>which can be accessed by the pipeline like so:</p> <pre><code>from ServiceProviders.PathProvider import PathProvider\nprovider = PathProvider(model=\"somemodel\")\ntestVal = provider.get(\"test\")\nprint(testVal) # prints \"somemodel/testvalue\"\n</code></pre> <p>However, we ran into the same issue with PathProvider, needing a unique path for the initial gatherer, generator, and machine learning parts of the pipeline. Thus, we created the <code>InitialGathererPathProvider</code>, <code>GeneratorPathProvider</code>, and <code>MLPathProvider</code> which are used for the initial gatherer, generator, and machine learning parts of the pipeline respectively. These providers are instantiated with a dictionary of key value pairs (similar to PathProvider). The model name is used to create a unique key for each key value pair. For example, the following provider provides a dictionary with the key <code>test</code> and the value <code>{model}/testvalue</code> where model=<code>initialGatherer</code>, <code>generator</code>, or <code>ml</code> depending on the provider:</p> <pre><code>from ServiceProviders.InitialGathererPathProvider import InitialGathererPathProvider\nfrom ServiceProviders.GeneratorPathProvider import GeneratorPathProvider\nfrom ServiceProviders.MLPathProvider import MLPathProvider\ninitialGathererProvider = InitialGathererPathProvider(model=\"initialGatherer\", contexts={\"test\": lambda model: f\"{model}/testvalue\"})\ngeneratorProvider = GeneratorPathProvider(model=\"generator\", contexts={\"test\": lambda model: f\"{model}/testvalue\"})\nmlProvider = MLPathProvider(model=\"ml\", contexts={\"test\": lambda model: f\"{model}/testvalue\"})\n</code></pre> <p>which can be accessed by the pipeline like so:</p> <pre><code>from ServiceProviders.InitialGathererPathProvider import InitialGathererPathProvider\nfrom ServiceProviders.GeneratorPathProvider import GeneratorPathProvider\nfrom ServiceProviders.MLPathProvider import MLPathProvider\ninitialGathererProvider = InitialGathererPathProvider(model=\"initialGatherer\")\ngeneratorProvider = GeneratorPathProvider(model=\"generator\")\nmlProvider = MLPathProvider(model=\"ml\")\n# prints \"initialGatherer/testvalue\"\ninitialGathererTestPath = initialGathererProvider.get(\"test\")\n# prints \"generator/testvalue\"\ngeneratorTestPath = generatorProvider.get(\"test\")\n# prints \"ml/testvalue\"\nmlTestPath = mlProvider.get(\"test\")\n</code></pre>"},{"location":"Getting%20Started/development.html#configuration","title":"Configuration","text":""},{"location":"Getting%20Started/development.html#setting-up-the-configuration","title":"Setting Up The Configuration","text":"<p>All pipeline user files should use a unique class name and log name along with configuration for the providers. The providers can provide a value when requested from a given key. See the Providers section for more information on providers. You can also reference the Configuration Parameters page to see all the possible configuration parameters.</p> <p>The following is the config for <code>MClassifierLargePipelineUserWithXYOffsetPos500mDist100kRowsEXTTimestampsCols30attackersRandOffset100To200</code>:</p> <pre><code># MClassifierPipeline-Const-50-offset\nimport os\nfrom pandas import DataFrame\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom EasyMLLib.CSVWriter import CSVWriter\nfrom Generator.Attackers.Attacks.StandardPositionalOffsetAttacker import StandardPositionalOffsetAttacker\nfrom Generator.Attackers.ConnectedDrivingAttacker import ConnectedDrivingAttacker\nfrom Generator.Cleaners.CleanersWithFilters.CleanerWithFilterWithinRangeXY import CleanerWithFilterWithinRangeXY\nfrom Generator.Cleaners.ConnectedDrivingCleaner import ConnectedDrivingCleaner\nfrom Generator.Cleaners.ConnectedDrivingLargeDataCleaner import ConnectedDrivingLargeDataCleaner\nfrom Generator.Cleaners.ExtraCleaningFunctions.CleanWithTimestamps import CleanWithTimestamps\nfrom Logger.Logger import DEFAULT_LOG_PATH, Logger\nfrom Generator.Cleaners.ConnectedDrivingLargeDataPipelineGathererAndCleaner import ConnectedDrivingLargeDataPipelineGathererAndCleaner\nfrom MachineLearning.MClassifierPipeline import MClassifierPipeline\nfrom MachineLearning.MConnectedDrivingDataCleaner import MConnectedDrivingDataCleaner\nfrom ServiceProviders.GeneratorContextProvider import GeneratorContextProvider\nfrom ServiceProviders.GeneratorPathProvider import GeneratorPathProvider\nfrom ServiceProviders.InitialGathererPathProvider import InitialGathererPathProvider\nfrom ServiceProviders.MLContextProvider import MLContextProvider\nfrom ServiceProviders.MLPathProvider import MLPathProvider\nfrom ServiceProviders.PathProvider import PathProvider\nCLASSIFIER_INSTANCES = [RandomForestClassifier(\n), DecisionTreeClassifier(), KNeighborsClassifier()]\nLOG_NAME = \"MClassifierLargePipelineUserWithXYOffsetPos500mDist100kRowsEXTTimestampsCols30attackersRandOffset100To200\"\nCSV_COLUMNS = [\"Model\", \"Total_Train_Time\",\n\"Total_Train_Sample_Size\", \"Total_Test_Sample_Size\", \"Train_Time_Per_Sample\", \"Prediction_Train_Set_Time_Per_Sample\", \"Prediction_Test_Set_Time_Per_Sample\",\n\"train_accuracy\", \"train_precision\", \"train_recall\", \"train_f1\",\n\"test_accuracy\", \"test_precision\", \"test_recall\", \"test_f1\"]\nCSV_FORMAT = {CSV_COLUMNS[i]: i for i in range(len(CSV_COLUMNS))}\nclass MClassifierLargePipelineUserWithXYOffsetPos500mDist100kRowsEXTTimestampsCols30attackersRandOffset100To200:\ndef __init__(self):\n#################  CONFIG FOR ALL PROPERTIES IN THE PIPELINE #################################################\n# used for the logger's path\nself._pathprovider = PathProvider(model=LOG_NAME, contexts={\n\"Logger.logpath\": DEFAULT_LOG_PATH,\n})\ninitialGathererModelName = \"CreatingConnectedDrivingDataset\"\nnumSubsectionRows = 100000\n# Properties:\n# DataGatherer.filepath\n# DataGatherer.subsectionpath\n# DataGatherer.splitfilespath\n# DataGatherer.lines_per_file\nself._initialGathererPathProvider = InitialGathererPathProvider(model=initialGathererModelName, contexts={\n\"DataGatherer.filepath\": lambda model: \"data/data.csv\",\n\"DataGatherer.subsectionpath\": lambda model: f\"data/classifierdata/subsection/{model}/subsection{numSubsectionRows}.csv\",\n\"DataGatherer.splitfilespath\": lambda model: f\"data/classifierdata/splitfiles/{model}/\",\n}\n)\n# Properties:\n#\n# ConnectedDrivingLargeDataCleaner.cleanedfilespath\n# ConnectedDrivingLargeDataCleaner.combinedcleandatapath\n# MConnectedDrivingLargeDataCleaner.dtypes # AUTO_FILLED\n#\n# MAKE SURE TO CHANGE THE MODEL NAME TO THE PROPER NAME (IE A NAME THAT MATCHES IF\n# IT HAS TIMESTAMPS OR NOT, AND IF IT HAS XY COORDS OR NOT, ETC)\nself._generatorPathProvider = GeneratorPathProvider(model=f\"{initialGathererModelName}-CCDDWithTimestampsAndWithXYCoords-500mdist\", contexts={\n\"ConnectedDrivingLargeDataCleaner.cleanedfilespath\": lambda model:  f\"data/classifierdata/splitfiles/cleaned/{model}/\",\n\"ConnectedDrivingLargeDataCleaner.combinedcleandatapath\": lambda model: f\"data/classifierdata/splitfiles/combinedcleaned/{model}/combinedcleaned\",\n}\n)\n# Properties:\n#\n# MConnectedDrivingDataCleaner.cleandatapath\n# MDataClassifier.plot_confusion_matrix_path\n#\nself._mlPathProvider = MLPathProvider(model=LOG_NAME, contexts={\n\"MConnectedDrivingDataCleaner.cleandatapath\": lambda model: f\"data/mclassifierdata/cleaned/{model}/clean.csv\",\n\"MDataClassifier.plot_confusion_matrix_path\": lambda model: f\"data/mclassifierdata/results/{model}/\",\n}\n)\n# Properties:\n#\n# DataGatherer.numrows\n# ConnectedDrivingCleaner.x_pos\n# ConnectedDrivingCleaner.y_pos\n# ConnectedDrivingLargeDataCleaner.max_dist\n# ConnectedDrivingLargeDataCleaner.cleanFunc\n# ConnectedDrivingLargeDataCleaner.filterFunc\n# ConnectedDrivingAttacker.SEED\n# ConnectedDrivingCleaner.isXYCoords\n# ConnectedDrivingAttacker.attack_ratio\n# ConnectedDrivingCleaner.cleanParams\n#\n# Cleaned columns are added/modified after these columns are used for filtering\nCOLUMNS=[\"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n\"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n\"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n#  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n#  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n\"coreData_id\", \"coreData_secMark\", \"coreData_position_lat\", \"coreData_position_long\",\n\"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n\"coreData_elevation\", \"coreData_accelset_accelYaw\",\"coreData_speed\", \"coreData_heading\", \"coreData_position\"]\nx_pos = -105.1159611\ny_pos = 41.0982327\nx_pos_str = MathHelper.convertNumToTitleStr(x_pos)\ny_pos_str = MathHelper.convertNumToTitleStr(y_pos)\nself.generatorContextProvider = GeneratorContextProvider(contexts={\n\"DataGatherer.numrows\": numSubsectionRows,\n\"DataGatherer.lines_per_file\": 1000000,\n\"ConnectedDrivingCleaner.x_pos\": -105.1159611,\n\"ConnectedDrivingCleaner.y_pos\": 41.0982327,\n\"ConnectedDrivingCleaner.columns\": COLUMNS,\n\"ConnectedDrivingLargeDataCleaner.max_dist\": 500,\n\"ConnectedDrivingCleaner.shouldGatherAutomatically\": False,\n\"ConnectedDrivingLargeDataCleaner.cleanerClass\": CleanWithTimestamps,\n\"ConnectedDrivingLargeDataCleaner.cleanFunc\": CleanWithTimestamps.clean_data_with_timestamps,\n\"ConnectedDrivingLargeDataCleaner.cleanerWithFilterClass\": CleanerWithFilterWithinRangeXY,\n\"ConnectedDrivingLargeDataCleaner.filterFunc\": CleanerWithFilterWithinRangeXY.within_rangeXY,\n\"ConnectedDrivingAttacker.SEED\": 42,\n\"ConnectedDrivingCleaner.isXYCoords\": True,\n\"ConnectedDrivingAttacker.attack_ratio\": 0.3,\n\"ConnectedDrivingCleaner.cleanParams\": f\"clean_data_with_timestamps-within_rangeXY-WithXYCoords-1000mdist-x{x_pos_str}y{y_pos_str}dd02mm04yyyy2021\", # makes cached data have info on if/if not we use timestamps for uniqueness\n}\n)\n# Properties:\n#\n# MConnectedDrivingDataCleaner.columns\n# MClassifierPipeline.classifier_instances # AUTO_FILLED\n# MClassifierPipeline.csvWriter\n#\nself.MLContextProvider = MLContextProvider(contexts={\n\"MConnectedDrivingDataCleaner.columns\": [\n# \"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n#  \"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n#  \"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n#  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n#  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n\"coreData_id\",  # \"coreData_position_lat\", \"coreData_position_long\",\n\"coreData_secMark\", \"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n\"month\", \"day\", \"year\", \"hour\", \"minute\", \"second\", \"pm\",\n\"coreData_elevation\", \"coreData_accelset_accelYaw\", \"coreData_speed\", \"coreData_heading\", \"x_pos\", \"y_pos\", \"isAttacker\"],\n# \"MClassifierPipeline.classifier_instances\": [...] # AUTO_FILLED\n\"MClassifierPipeline.csvWriter\": CSVWriter(f\"{LOG_NAME}.csv\", CSV_COLUMNS),\n}\n)\n######### END OF CONFIG FOR ALL PROPERTIES IN THE PIPELINE ##################################################\nself.logger = Logger(LOG_NAME)\nself.csvWriter = CSVWriter(f\"{LOG_NAME}.csv\", CSV_COLUMNS)\ndef write_entire_row(self, dict):\nrow = [\" \"]*len(CSV_COLUMNS)\n# Writing each variable to the row\nfor d in dict:\nrow[CSV_FORMAT[d]] = dict[d]\nself.csvWriter.addRow(row)\n</code></pre>"},{"location":"Getting%20Started/development.html#classifier-models","title":"Classifier Models","text":"<p>You'll notice that we default to using the RandomForestClassifier, DecisionTreeClassifier and KNeighborsClassifier models for the MClassifierPipeline. This is because these models are the most accurate for our data. However, you can change this to use any model you want. You can also change the parameters for each model. For example, if you want to use a different number of estimators for the RandomForestClassifier, you can change the parameters to be:</p> <pre><code>CLASSIFIER_INSTANCES = [\nRandomForestClassifier(n_estimators=200), # 100 is default\nDecisionTreeClassifier(),\nKNeighborsClassifier()\n]\n</code></pre> <p>However, make sure to include the <code>classifier_instances</code> property in the MLContextProvider and set it to <code>CLASSIFIER_INSTANCES</code></p>"},{"location":"Getting%20Started/development.html#log-name","title":"Log Name","text":"<p>The log name is very important because it ensures that the caching used in the machine learning part of the pipeline works correctly. It also acts as the log folder name for our logs.</p>"},{"location":"Getting%20Started/development.html#csv-columns","title":"CSV Columns","text":"<p>These columns act as our CSV headers for our results file. You won't need to change these unless you plan to modify the way we write our results to the CSV file.</p>"},{"location":"Getting%20Started/development.html#csv-format","title":"CSV Format","text":"<p>This is a dictionary that maps the CSV column names to their index in the CSV file. You won't need to change these unless you plan to modify the way we write our results to the CSV file.</p>"},{"location":"Getting%20Started/development.html#pathprovider","title":"PathProvider","text":"<p>You'll notice that the first part we configure within the init method is the PathProvider. We use this to ensure that our logs are written to the correct folder. You won't need to change this unless you plan to modify the way we write our logs. Make sure that the LOG_NAME is unique to each pipeline user file.</p> <pre><code># used for the logger's path\nself._pathprovider = PathProvider(model=LOG_NAME, contexts={\n\"Logger.logpath\": DEFAULT_LOG_PATH,\n})\n</code></pre>"},{"location":"Getting%20Started/development.html#initialgathererpathprovider","title":"InitialGathererPathProvider","text":"<p>The paths in the InitialGathererPathProvider are used to gather the data from the initial data source. You'll need to change these to match the paths to your data. The model name should be shared among all similar initial datasets (i.e. all datasets that are gathered from the same source).</p> <pre><code>initialGathererModelName = \"CreatingConnectedDrivingDataset\"\n# Properties:\n# DataGatherer.filepath\n# DataGatherer.subsectionpath\n# DataGatherer.splitfilespath\n# DataGatherer.lines_per_file\nself._initialGathererPathProvider = InitialGathererPathProvider(model=initialGathererModelName, contexts={\n\"DataGatherer.filepath\": lambda model: \"data/data.csv\",\n\"DataGatherer.subsectionpath\": lambda model: f\"data/classifierdata/subsection/{model}/subsection{numSubsectionRows}.csv\",\n\"DataGatherer.splitfilespath\": lambda model: f\"data/classifierdata/splitfiles/{model}/\",\n}\n)\n</code></pre>"},{"location":"Getting%20Started/development.html#generatorpathprovider","title":"GeneratorPathProvider","text":"<p>The paths in the GeneratorPathProvider are used to generate the data for the machine learning part of the pipeline. You'll need to change these to match the paths to your generated data. Make sure that the model name is unique to the cleaning/filtering options. For example, it should include whether or not we included timestamps, XY COORDS, etc. and also the distance of the filter (default 500m)</p> <pre><code># Properties:\n#\n# ConnectedDrivingLargeDataCleaner.cleanedfilespath\n# ConnectedDrivingLargeDataCleaner.combinedcleandatapath\n# MConnectedDrivingLargeDataCleaner.dtypes # AUTO_FILLED\n#\n# MAKE SURE TO CHANGE THE MODEL NAME TO THE PROPER NAME (IE A NAME THAT MATCHES IF\n# IT HAS TIMESTAMPS OR NOT, AND IF IT HAS XY COORDS OR NOT, ETC)\nself._generatorPathProvider = GeneratorPathProvider(model=f\"{initialGathererModelName}-CCDDWithTimestampsAndWithXYCoords-500mdist\", contexts={\n\"ConnectedDrivingLargeDataCleaner.cleanedfilespath\": lambda model:  f\"data/classifierdata/splitfiles/cleaned/{model}/\",\n\"ConnectedDrivingLargeDataCleaner.combinedcleandatapath\": lambda model: f\"data/classifierdata/splitfiles/combinedcleaned/{model}/combinedcleaned\",\n}\n)\n</code></pre>"},{"location":"Getting%20Started/development.html#mlpathprovider","title":"MLPathProvider","text":"<p>The paths in the MLPathProvider are used to run the machine learning part of the pipeline. You'll need to change these to match the paths to your ML data. Make sure that the model name is unique to the ML options and cleaning options. The best way to do this is to make your file name unique and set the model as the file name (A.K.A. the LOG_NAME)</p> <pre><code># Properties:\n#\n# MConnectedDrivingDataCleaner.cleandatapath\n# MDataClassifier.plot_confusion_matrix_path\n#\nself._mlPathProvider = MLPathProvider(model=LOG_NAME, contexts={\n\"MConnectedDrivingDataCleaner.cleandatapath\": lambda model: f\"data/mclassifierdata/cleaned/{model}/clean.csv\",\n\"MDataClassifier.plot_confusion_matrix_path\": lambda model: f\"data/mclassifierdata/results/{model}/\",\n}\n)\n</code></pre>"},{"location":"Getting%20Started/development.html#columns","title":"COLUMNS","text":"<p>The COLUMNS variable is used to specify the columns that we want to use for our initial cleaning and gathering of data. You'll need to change these to match the columns in your data if they are different.</p> <pre><code># Cleaned columns are added/modified after these columns are used for filtering\nCOLUMNS=[\"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n\"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n\"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n#  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n#  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n\"coreData_id\", \"coreData_secMark\", \"coreData_position_lat\", \"coreData_position_long\",\n\"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n\"coreData_elevation\", \"coreData_accelset_accelYaw\",\"coreData_speed\", \"coreData_heading\", \"coreData_position\"]\n</code></pre>"},{"location":"Getting%20Started/development.html#generatorcontextprovider","title":"GeneratorContextProvider","text":"<p>The GeneratorContextProvider is used to provide the contexts for the generation part of the pipeline (including the initial gathering part). You'll need to change these to match configurations for your data. Make sure to change the cleanParams to match the cleaning options you want to use. Make sure that the cleanParams is unique to the cleaning/filtering options. For example, it should include whether or not we included timestamps, XY COORDS, etc. and also the distance of the filter.</p> <pre><code># Properties:\n#\n# DataGatherer.numrows\n# ConnectedDrivingCleaner.x_pos\n# ConnectedDrivingCleaner.y_pos\n# ConnectedDrivingLargeDataCleaner.max_dist\n# ConnectedDrivingLargeDataCleaner.cleanFunc\n# ConnectedDrivingLargeDataCleaner.filterFunc\n# ConnectedDrivingAttacker.SEED\n# ConnectedDrivingCleaner.isXYCoords\n# ConnectedDrivingAttacker.attack_ratio\n# ConnectedDrivingCleaner.cleanParams\n#\n# XY columns are added after these columns are used for filtering\nCOLUMNS=[\"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n\"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n\"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n#  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n#  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n\"coreData_id\", \"coreData_secMark\", \"coreData_position_lat\", \"coreData_position_long\",\n\"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n\"coreData_elevation\", \"coreData_accelset_accelYaw\",\"coreData_speed\", \"coreData_heading\", \"coreData_position\"]\nx_pos = -105.1159611\ny_pos = 41.0982327\nx_pos_str = MathHelper.convertNumToTitleStr(x_pos)\ny_pos_str = MathHelper.convertNumToTitleStr(y_pos)\nself.generatorContextProvider = GeneratorContextProvider(contexts={\n\"DataGatherer.numrows\": numSubsectionRows,\n\"DataGatherer.lines_per_file\": 1000000,\n\"ConnectedDrivingCleaner.x_pos\": -105.1159611,\n\"ConnectedDrivingCleaner.y_pos\": 41.0982327,\n\"ConnectedDrivingCleaner.columns\": COLUMNS,\n\"ConnectedDrivingLargeDataCleaner.max_dist\": 500,\n\"ConnectedDrivingCleaner.shouldGatherAutomatically\": False,\n\"ConnectedDrivingLargeDataCleaner.cleanerClass\": CleanWithTimestamps,\n\"ConnectedDrivingLargeDataCleaner.cleanFunc\": CleanWithTimestamps.clean_data_with_timestamps,\n\"ConnectedDrivingLargeDataCleaner.cleanerWithFilterClass\": CleanerWithFilterWithinRangeXY,\n\"ConnectedDrivingLargeDataCleaner.filterFunc\": CleanerWithFilterWithinRangeXY.within_rangeXY,\n\"ConnectedDrivingAttacker.SEED\": 42,\n\"ConnectedDrivingCleaner.isXYCoords\": True,\n\"ConnectedDrivingAttacker.attack_ratio\": 0.3,\n\"ConnectedDrivingCleaner.cleanParams\": f\"clean_data_with_timestamps-within_rangeXY-WithXYCoords-1000mdist-x{x_pos_str}y{y_pos_str}dd02mm04yyyy2021\", # makes cached data have info on if/if not we use timestamps for uniqueness\n}\n)\n</code></pre>"},{"location":"Getting%20Started/development.html#mlcontextprovider","title":"MLContextProvider","text":"<p>The MLContextProvider is used to provide the contexts for the machine learning part of the pipeline. You'll need to change these to match configurations for your ML data.</p> <pre><code># Properties:\n#\n# MConnectedDrivingDataCleaner.columns\n# MClassifierPipeline.classifier_instances # AUTO_FILLED\n# MClassifierPipeline.csvWriter\n#\nself.MLContextProvider = MLContextProvider(contexts={\n\"MConnectedDrivingDataCleaner.columns\": [\n# \"metadata_generatedAt\", \"metadata_recordType\", \"metadata_serialId_streamId\",\n#  \"metadata_serialId_bundleSize\", \"metadata_serialId_bundleId\", \"metadata_serialId_recordId\",\n#  \"metadata_serialId_serialNumber\", \"metadata_receivedAt\",\n#  \"metadata_rmd_elevation\", \"metadata_rmd_heading\",\"metadata_rmd_latitude\", \"metadata_rmd_longitude\", \"metadata_rmd_speed\",\n#  \"metadata_rmd_rxSource\",\"metadata_bsmSource\",\n\"coreData_id\",  # \"coreData_position_lat\", \"coreData_position_long\",\n\"coreData_secMark\", \"coreData_accuracy_semiMajor\", \"coreData_accuracy_semiMinor\",\n\"month\", \"day\", \"year\", \"hour\", \"minute\", \"second\", \"pm\",\n\"coreData_elevation\", \"coreData_accelset_accelYaw\", \"coreData_speed\", \"coreData_heading\", \"x_pos\", \"y_pos\", \"isAttacker\"],\n# \"MClassifierPipeline.classifier_instances\": [...] # AUTO_FILLED\n\"MClassifierPipeline.csvWriter\": CSVWriter(f\"{LOG_NAME}.csv\", CSV_COLUMNS),\n}\n)\n</code></pre>"},{"location":"Getting%20Started/development.html#logger-and-csv-writer","title":"Logger and CSV Writer","text":"<p>The logger and CSV writer are used to log the results of the pipeline. You won't need to change these.</p> <pre><code># Goes after the config ...\nself.logger = Logger(LOG_NAME)\nself.csvWriter = self.MLContextProvider.get(\"MClassifierPipeline.csvWriter\")\n</code></pre>"},{"location":"Getting%20Started/development.html#write_entire_row-function","title":"write_entire_row function","text":"<p>The write_entire_row function is used to write the results of the pipeline to a CSV file. You won't need to change this.</p> <pre><code>def write_entire_row(self, dict):\nrow = [\" \"]*len(CSV_COLUMNS)\n# Writing each variable to the row\nfor d in dict:\nrow[CSV_FORMAT[d]] = dict[d]\nself.csvWriter.addRow(row)\n</code></pre>"},{"location":"Getting%20Started/development.html#creating-the-run-function","title":"Creating The <code>run</code> Function","text":"<p>The <code>run</code> function is the main function of the pipeline. It is called when the pipeline is run. You'll need to change this to match your pipeline.</p>"},{"location":"Getting%20Started/development.html#an-example-run-function","title":"An Example <code>run</code> Function","text":"<p>The following is the run function for <code>MClassifierLargePipelineUserWithXYOffsetPos500mDist100kRowsEXTTimestampsCols30attackersRandOffset100To200</code></p> <pre><code>def run(self):\nmcdldpgac = ConnectedDrivingLargeDataPipelineGathererAndCleaner().run()\ndata: DataFrame = mcdldpgac.getNRows(200000)\n# splitting into train and test sets\ntrain = data.iloc[:100000].copy()\ntest = data.iloc[100000:200000].copy()\n# Note you can randomize the sampling as follows:\n# # splitting into train and test sets\n# seed = self.generatorContextProvider.get(\"ConnectedDrivingAttacker.SEED\")\n# train, test = train_test_split(data, test_size=0.2, random_state=seed)\n# cleaning/adding attackers to the data\ntrain = StandardPositionalOffsetAttacker(train, \"train\").add_attackers().add_attacks_positional_offset_rand(min_dist=100, max_dist=200).get_data()\ntest = StandardPositionalOffsetAttacker(test, \"test\").add_attackers().add_attacks_positional_offset_rand(min_dist=100, max_dist=200).get_data()\n# Cleaning it for the malicious data detection\nmdcleaner_train = MConnectedDrivingDataCleaner(train, \"train\")\nmdcleaner_test = MConnectedDrivingDataCleaner(test, \"test\")\nm_train = mdcleaner_train.clean_data().get_cleaned_data()\nm_test = mdcleaner_test.clean_data().get_cleaned_data()\n# splitting into features and the labels\nattacker_col_name = \"isAttacker\"\ntrain_X = m_train.drop(columns=[attacker_col_name], axis=1)\ntrain_Y = m_train[attacker_col_name]\ntest_X = m_test.drop(columns=[attacker_col_name], axis=1)\ntest_Y = m_test[attacker_col_name]\n# training the classifiers\nmcp = MClassifierPipeline(train_X, train_Y, test_X, test_Y)\nmcp.train()\nmcp.test()\n# getting the results\nresults = mcp.calc_classifier_results().get_classifier_results()\n# printing the results\nfor mclassifier, train_result, result in results:\nmcp.logger.log(mclassifier)\nmcp.logger.log(\"Train Set Results:\")\nmcp.logger.log(\"Accuracy: \", train_result[0])\nmcp.logger.log(\"Precision: \", train_result[1])\nmcp.logger.log(\"Recall: \", train_result[2])\nmcp.logger.log(\"F1: \", train_result[3])\nmcp.logger.log(\"Test Set Results:\")\nmcp.logger.log(\"Accuracy: \", result[0])\nmcp.logger.log(\"Precision: \", result[1])\nmcp.logger.log(\"Recall: \", result[2])\nmcp.logger.log(\"F1: \", result[3])\n# printing the elapsed training and prediction time\nmcp.logger.log(\"Elapsed Training Time: \",\nmclassifier.elapsed_train_time)\nmcp.logger.log(\"Elapsed Prediction Time: \",\nmclassifier.elapsed_prediction_time)\nmcp.logger.log(\"Writing to CSV...\")\n# writing entire row to csv\n# columns: \"Model\", \"Total_Train_Time\",\n#    \"Total_Train_Sample_Size\", \"Total_Test_Sample_Size\", \"Train_Time_Per_Sample\", \"Prediction_Train_Set_Time_Per_Sample\", \"Prediction_Test_Set_Time_Per_Sample\",\n#    \"train_accuracy\", \"train_precision\", \"train_recall\", \"train_f1\",\n#    \"test_accuracy\", \"test_precision\", \"test_recall\", \"test_f1\"\ncsvrowdata = {\n\"Model\": mclassifier.classifier.__class__.__name__,\n\"Total_Train_Time\": mclassifier.elapsed_train_time,\n# train and test have the same number of samples\n\"Total_Train_Sample_Size\": len(train_X),\n# train and test have the same number of samples\n\"Total_Test_Sample_Size\": len(test_X),\n\"Train_Time_Per_Sample\": mclassifier.elapsed_train_time/len(train_X),\n\"Prediction_Train_Set_Time_Per_Sample\": mclassifier.elapsed_prediction_train_time/len(train_X),\n\"Prediction_Test_Set_Time_Per_Sample\": mclassifier.elapsed_prediction_time/len(test_X),\n\"train_accuracy\": train_result[0],\n\"train_precision\": train_result[1],\n\"train_recall\": train_result[2],\n\"train_f1\": train_result[3],\n\"test_accuracy\": result[0],\n\"test_precision\": result[1],\n\"test_recall\": result[2],\n\"test_f1\": result[3]}\nself.write_entire_row(csvrowdata)\n# calculating confusion matrices and storing them\nmcp.logger.log(\"Calculating confusion matrices and storing...\")\nmcp.calculate_classifiers_and_confusion_matrices().plot_confusion_matrices()\n</code></pre>"},{"location":"Getting%20Started/development.html#explaining-how-the-run-function-works","title":"Explaining How The <code>run</code> Function Works","text":"<p>The <code>run</code> function acts as your main method for your pipeline.</p> <ol> <li>You'll notice that this pipeline uses the <code>ConnectedDrivingLargeDataPipelineGathererAndCleaner</code> class. It acts as a gatherer and cleaner for the data. It is used to split the data from the CSV files, clean them, combine them into one DataFrame. We can call the <code>run</code> function of the <code>ConnectedDrivingLargeDataPipelineGathererAndCleaner</code> class to get the data. We can then use the <code>getNRows</code> function to get the first 200 000 rows of the data.</li> </ol> <pre><code>mcdldpgac = ConnectedDrivingLargeDataPipelineGathererAndCleaner().run()\ndata: DataFrame = mcdldpgac.getNRows(200000)\n</code></pre> <ol> <li>We then split the data into a train and test set.</li> </ol> <pre><code># splitting into train and test sets\ntrain = data.iloc[:100000].copy()\ntest = data.iloc[100000:200000].copy()\n# Note you could also randomize the sampling as follows:\nseed = self.generatorContextProvider.get(\"ConnectedDrivingAttacker.SEED\")\ntrain, test = train_test_split(data, test_size=0.2, random_state=seed)\n</code></pre> <ol> <li>Adding attackers to the data.</li> </ol> <pre><code># cleaning/adding attackers to the data\ntrain = StandardPositionalOffsetAttacker(train, \"train\").add_attackers().add_attacks_positional_offset_rand(min_dist=100, max_dist=200).get_data()\ntest = StandardPositionalOffsetAttacker(test, \"test\").add_attackers().add_attacks_positional_offset_rand(min_dist=100, max_dist=200).get_data()\n</code></pre> <ol> <li>Cleaning the data using the <code>MConnectedDrivingDataCleaner</code> class.</li> </ol> <pre><code> # Cleaning it for the malicious data detection\nmdcleaner_train = MConnectedDrivingDataCleaner(train, \"train\")\nmdcleaner_test = MConnectedDrivingDataCleaner(test, \"test\")\nm_train = mdcleaner_train.clean_data().get_cleaned_data()\nm_test = mdcleaner_test.clean_data().get_cleaned_data()\n</code></pre> <ol> <li>Splitting the data into features and labels.</li> </ol> <pre><code>attacker_col_name = \"isAttacker\"\ntrain_X = m_train.drop(columns=[attacker_col_name], axis=1)\ntrain_Y = m_train[attacker_col_name]\ntest_X = m_test.drop(columns=[attacker_col_name], axis=1)\ntest_Y = m_test[attacker_col_name]\n</code></pre> <ol> <li>Training the classifiers using the <code>MClassifierPipeline</code> class.</li> </ol> <pre><code>mcp = MClassifierPipeline(train_X, train_Y, test_X, test_Y)\nmcp.train()\nmcp.test()\n</code></pre> <ol> <li>Getting the results of the classifiers.</li> </ol> <pre><code>results = mcp.calc_classifier_results().get_classifier_results()\n</code></pre> <ol> <li>Printing the results of the classifiers.</li> </ol> <pre><code># printing the results\nfor mclassifier, train_result, result in results:\nmcp.logger.log(mclassifier)\nmcp.logger.log(\"Train Set Results:\")\nmcp.logger.log(\"Accuracy: \", train_result[0])\nmcp.logger.log(\"Precision: \", train_result[1])\nmcp.logger.log(\"Recall: \", train_result[2])\nmcp.logger.log(\"F1: \", train_result[3])\nmcp.logger.log(\"Test Set Results:\")\nmcp.logger.log(\"Accuracy: \", result[0])\nmcp.logger.log(\"Precision: \", result[1])\nmcp.logger.log(\"Recall: \", result[2])\nmcp.logger.log(\"F1: \", result[3])\n# printing the elapsed training and prediction time\nmcp.logger.log(\"Elapsed Training Time: \",\nmclassifier.elapsed_train_time)\nmcp.logger.log(\"Elapsed Prediction Time: \",\nmclassifier.elapsed_prediction_time)\nmcp.logger.log(\"Writing to CSV...\")\n# writing entire row to csv\n# columns: \"Model\", \"Total_Train_Time\",\n#    \"Total_Train_Sample_Size\", \"Total_Test_Sample_Size\", \"Train_Time_Per_Sample\", \"Prediction_Train_Set_Time_Per_Sample\", \"Prediction_Test_Set_Time_Per_Sample\",\n#    \"train_accuracy\", \"train_precision\", \"train_recall\", \"train_f1\",\n#    \"test_accuracy\", \"test_precision\", \"test_recall\", \"test_f1\"\ncsvrowdata = {\n\"Model\": mclassifier.classifier.__class__.__name__,\n\"Total_Train_Time\": mclassifier.elapsed_train_time,\n# train and test have the same number of samples\n\"Total_Train_Sample_Size\": len(train_X),\n# train and test have the same number of samples\n\"Total_Test_Sample_Size\": len(test_X),\n\"Train_Time_Per_Sample\": mclassifier.elapsed_train_time/len(train_X),\n\"Prediction_Train_Set_Time_Per_Sample\": mclassifier.elapsed_prediction_train_time/len(train_X),\n\"Prediction_Test_Set_Time_Per_Sample\": mclassifier.elapsed_prediction_time/len(test_X),\n\"train_accuracy\": train_result[0],\n\"train_precision\": train_result[1],\n\"train_recall\": train_result[2],\n\"train_f1\": train_result[3],\n\"test_accuracy\": result[0],\n\"test_precision\": result[1],\n\"test_recall\": result[2],\n\"test_f1\": result[3]}\nself.write_entire_row(csvrowdata)\n</code></pre> <ol> <li>Calculating the confusion matrices and storing them. Note that this step also saved the matrices in your results file as base64 images.</li> </ol> <pre><code># calculating the confusion matrices\nmcp.calculate_classifiers_and_confusion_matrices().plot_confusion_matrices()\n</code></pre>"},{"location":"Getting%20Started/development.html#running-the-pipeline","title":"Running The Pipeline","text":"<p>To run the pipeline, you can run the following command in the terminal:</p> <p><pre><code>python &lt;your python file&gt;.py\n</code></pre> If you haven't already, make sure to activate your virtual environment before running the command. See the Setup page for more details.</p>"},{"location":"Getting%20Started/development.html#running-on-the-super-computer","title":"Running On The Super Computer","text":"<p>To run on the super computer, register with compute canada. Next, follow the instructions on the Setup page to set up your virtual environment. You'll likely want to use the beluga super computer (i.e. @beluga.computecanada.ca). You may want to use --no-index when installing requirements to avoid downloading packages from the internet. <p>Finally, you can run the following command to submit a job to the super computer:</p> <pre><code>./runUserPipeline.sh &lt;USERNAME&gt; &lt;PATH_TO_REPO (not ending in slash)&gt; &lt;FILE&gt; &lt;DAYS&gt; &lt;HOURS&gt; &lt;MINUTES&gt; &lt;CPUS&gt; &lt;MEM&gt; [OPTIONAL: DEPENDENCY]\n</code></pre> <p>Optionally, you can use the defaultrunnerconfig.sh file to use default values:</p> <pre><code>./defaultrunnerconfig.sh &lt;FILE&gt; &lt;USERNAME&gt; [OPTIONAL: DEPENDENCY]\n</code></pre> <p>If you decide to run the commands directly from the super computer, you can generate a slurm file using the following command:</p> <pre><code>python3 generateSlurm.py &lt;fileName&gt; &lt;days&gt; &lt;hours&gt; &lt;minutes&gt; &lt;cpus&gt; &lt;memory(G)&gt; [dependency]\n</code></pre> <p>Or you can create the slurm file yourself and submit it as a job using the following command:</p> <pre><code>sbatch &lt;fileName&gt;\n</code></pre>"},{"location":"Getting%20Started/setup.html","title":"Setup","text":""},{"location":"Getting%20Started/setup.html#requirements","title":"Requirements","text":"<ul> <li>A Windows, Linux or Mac operating system (note that you may need to adjust python to python3, etc. when installing on Mac or Linux)</li> <li>Python 3.10 since Python 3.11 isn't fully supported by all ML libraries</li> <li>Docker (If you intend to run the docs locally)</li> <li>Pip (Installed with Python3)</li> <li>Git</li> </ul>"},{"location":"Getting%20Started/setup.html#install","title":"Install","text":"<ol> <li>Install the requirements listed above.</li> <li>clone the github repo by running the following in your git bash terminal (note that on the super computer you should do this within the projects directory): <pre><code>git clone https://github.com/aaron777collins/ConnectedDrivingPipelineV4.git\n</code></pre></li> <li>cd into the folder by typing: <pre><code>cd ConnectedDrivingPipelineV4\n</code></pre> Note that all your future commands will be expected to start from this directory.</li> <li>Create a folder called <code>data</code> <pre><code>mkdir data\n</code></pre></li> <li> <p>Download the data from here and name it <code>data.csv</code> within the <code>data</code> folder.</p> <p>Optionally: You can download wget.exe from https://eternallybored.org/misc/wget/ and put it in your git bash directory (The default Windows install directory is <code>C:\\Program Files\\Git\\mingw64\\bin</code>). Next, run the following in git bash (within the project directory) to download the file: <pre><code>while [ 1 ]; do\nwget --retry-connrefused --retry-on-http-error=500 --waitretry=1 --read-timeout=20 --timeout=15 -t 0 --continue -O data/data.csv https://data.transportation.gov/api/views/9k4m-a3jc/rows.csv?accessType=DOWNLOAD\n    if [ $? = 0 ]; then break; fi; # check return value, break if successful (0)\necho `error downloading. Trying again!`\nsleep 1s;\ndone;\n</code></pre></p> </li> <li> <p>Create a virtual environment to install the required modules     <pre><code>python -m venv venv\n</code></pre>     and then activate your instance by running     <pre><code>./venv/Scripts/activate\n</code></pre>     on Windows. For other OS' consult the python docs here</p> <p>Make sure to always activate your venv before running anything in this pipeline</p> </li> <li> <p>Install the required modules (note that on the super computer you can use --no-index to install from the local cache)</p> <pre><code>pip install -r requirements.txt\n</code></pre> </li> </ol> <p>You're Finished Installing! Head over to the Development page to learn how to make your first pipeline user.</p>"}]}